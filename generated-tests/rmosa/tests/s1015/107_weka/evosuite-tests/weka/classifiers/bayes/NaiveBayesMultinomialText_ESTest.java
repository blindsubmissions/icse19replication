/*
 * This file was automatically generated by EvoSuite
 * Thu Aug 23 12:17:01 GMT 2018
 */

package weka.classifiers.bayes;

import org.junit.Test;
import static org.junit.Assert.*;
import static org.evosuite.runtime.EvoAssertions.*;
import java.io.File;
import java.net.URI;
import java.util.HashMap;
import java.util.LinkedHashMap;
import java.util.Locale;
import java.util.Map;
import org.evosuite.runtime.EvoRunner;
import org.evosuite.runtime.EvoRunnerParameters;
import org.evosuite.runtime.Random;
import org.evosuite.runtime.System;
import org.evosuite.runtime.mock.java.io.MockFile;
import org.evosuite.runtime.mock.java.net.MockURI;
import org.evosuite.runtime.testdata.EvoSuiteFile;
import org.evosuite.runtime.testdata.FileSystemHandling;
import org.junit.runner.RunWith;
import weka.attributeSelection.PrincipalComponents;
import weka.classifiers.AbstractClassifier;
import weka.classifiers.bayes.NaiveBayesMultinomialText;
import weka.classifiers.functions.SGDText;
import weka.classifiers.misc.InputMappedClassifier;
import weka.classifiers.misc.SerializedClassifier;
import weka.classifiers.trees.RandomForest;
import weka.core.BinarySparseInstance;
import weka.core.Capabilities;
import weka.core.DenseInstance;
import weka.core.Instance;
import weka.core.Instances;
import weka.core.SparseInstance;
import weka.core.TestInstances;
import weka.core.neighboursearch.balltrees.BallNode;
import weka.core.stemmers.SnowballStemmer;
import weka.core.stemmers.Stemmer;
import weka.core.tokenizers.AlphabeticTokenizer;
import weka.core.tokenizers.Tokenizer;

@RunWith(EvoRunner.class) @EvoRunnerParameters(mockJVMNonDeterminism = true, useVFS = true, useVNET = true, resetStaticState = true, separateClassLoader = true, useJEE = true) 
public class NaiveBayesMultinomialText_ESTest extends NaiveBayesMultinomialText_ESTest_scaffolding {

  /**
  //Test case number: 0
  /*Coverage entropy=0.6931471805599453
  */
  @Test(timeout = 4000)
  public void test00()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      String string0 = naiveBayesMultinomialText0.tokenizerTipText();
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The tokenizing algorithm to use on the strings.", string0);
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
  }

  /**
  //Test case number: 1
  /*Coverage entropy=0.5623351446188083
  */
  @Test(timeout = 4000)
  public void test01()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      MockFile mockFile0 = (MockFile)naiveBayesMultinomialText0.m_stopwordsFile;
      mockFile0.mkdirs();
      naiveBayesMultinomialText0.setStopwords(mockFile0);
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
  }

  /**
  //Test case number: 2
  /*Coverage entropy=0.6931471805599453
  */
  @Test(timeout = 4000)
  public void test02()  throws Throwable  {
      String[] stringArray0 = new String[4];
      NaiveBayesMultinomialText.main(stringArray0);
      assertEquals(4, stringArray0.length);
  }

  /**
  //Test case number: 3
  /*Coverage entropy=0.6931471805599453
  */
  @Test(timeout = 4000)
  public void test03()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      String string0 = naiveBayesMultinomialText0.getRevision();
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("9122", string0);
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
  }

  /**
  //Test case number: 4
  /*Coverage entropy=0.6931471805599453
  */
  @Test(timeout = 4000)
  public void test04()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      InputMappedClassifier inputMappedClassifier0 = new InputMappedClassifier();
      InputMappedClassifier inputMappedClassifier1 = new InputMappedClassifier();
      naiveBayesMultinomialText0.useWordFrequenciesTipText();
      String[] stringArray0 = Locale.getISOCountries();
      assertEquals(250, stringArray0.length);
  }

  /**
  //Test case number: 5
  /*Coverage entropy=0.6931471805599453
  */
  @Test(timeout = 4000)
  public void test05()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.normTipText();
      Random.setNextRandom((-1581));
  }

  /**
  //Test case number: 6
  /*Coverage entropy=0.6931471805599453
  */
  @Test(timeout = 4000)
  public void test06()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.getDebug();
      naiveBayesMultinomialText0.setDebug(false);
      String string0 = naiveBayesMultinomialText0.useStopListTipText();
      assertEquals("If true, ignores all words that are on the stoplist.", string0);
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
  }

  /**
  //Test case number: 7
  /*Coverage entropy=1.715263227902811
  */
  @Test(timeout = 4000)
  public void test07()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      RandomForest randomForest0 = new RandomForest();
      TestInstances testInstances0 = new TestInstances();
      AlphabeticTokenizer alphabeticTokenizer0 = new AlphabeticTokenizer();
      String[] stringArray0 = new String[2];
      stringArray0[0] = "-tokenizer";
      testInstances0.setNoClass(true);
      stringArray0[1] = "-tokenizer";
      try { 
        naiveBayesMultinomialText0.setOptions(stringArray0);
        fail("Expecting exception: ClassNotFoundException");
      
      } catch(ClassNotFoundException e) {
      }
  }

  /**
  //Test case number: 8
  /*Coverage entropy=0.6931471805599453
  */
  @Test(timeout = 4000)
  public void test08()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      String string0 = naiveBayesMultinomialText0.stemmerTipText();
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("The stemming algorithm to use on the words.", string0);
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
  }

  /**
  //Test case number: 9
  /*Coverage entropy=1.3862943611198906
  */
  @Test(timeout = 4000)
  public void test09()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      SGDText sGDText0 = new SGDText();
      Tokenizer tokenizer0 = sGDText0.getTokenizer();
      naiveBayesMultinomialText0.setTokenizer(tokenizer0);
      naiveBayesMultinomialText0.globalInfo();
      naiveBayesMultinomialText0.getMinWordFrequency();
      NaiveBayesMultinomialText naiveBayesMultinomialText1 = new NaiveBayesMultinomialText();
      Instances instances0 = naiveBayesMultinomialText1.m_data;
      URI uRI0 = MockURI.aFileURI;
      MockFile mockFile0 = new MockFile(uRI0);
      File file0 = MockFile.createTempFile("\tMinimum word frequency. Words with less than this frequence are ignored.\n\tIf periodic pruning is turned on then this is also used to determine which\n\twords to remove from the dictionary (default = 3).", "@relation", (File) mockFile0);
      naiveBayesMultinomialText0.m_stopwordsFile = file0;
      PrincipalComponents principalComponents0 = new PrincipalComponents();
      double[] doubleArray0 = new double[1];
      doubleArray0[0] = (double) 0;
      BinarySparseInstance binarySparseInstance0 = new BinarySparseInstance(2762.387550736643, doubleArray0);
      try { 
        principalComponents0.convertInstance(binarySparseInstance0);
        fail("Expecting exception: Exception");
      
      } catch(Exception e) {
         //
         // convertInstance: Principal components not built yet
         //
         verifyException("weka.attributeSelection.PrincipalComponents", e);
      }
  }

  /**
  //Test case number: 10
  /*Coverage entropy=2.1639021922480013
  */
  @Test(timeout = 4000)
  public void test10()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      InputMappedClassifier inputMappedClassifier0 = new InputMappedClassifier();
      InputMappedClassifier inputMappedClassifier1 = new InputMappedClassifier();
      Capabilities capabilities0 = inputMappedClassifier1.getCapabilities();
      TestInstances testInstances0 = TestInstances.forCapabilities(capabilities0);
      Instances instances0 = testInstances0.generate("elily");
      naiveBayesMultinomialText0.buildClassifier(instances0);
      capabilities0.dependencies();
      naiveBayesMultinomialText0.toString();
      Random.setNextRandom((-2));
      String string0 = naiveBayesMultinomialText0.toString();
      assertEquals("The independent probability of a class\n--------------------------------------\nclass1\t9.0\nclass2\t4.0\nclass3\t7.0\nclass4\t4.0\n\nThe probability of a word given the class\n-----------------------------------------\n\tclass1\tclass2\tclass3\tclass4\t\nover\t2.718281828459045\t2.718281828459045\t7.38905609893065\t7.38905609893065\t\nthe\t7.38905609893065\t7.38905609893065\t2.718281828459045\t2.718281828459045\t\nThe\t7.38905609893065\t2.718281828459045\t7.38905609893065\t2.718281828459045\t\nquick\t7.38905609893065\t2.718281828459045\t7.38905609893065\t7.38905609893065\t\nlazy\t7.38905609893065\t2.718281828459045\t7.38905609893065\t2.718281828459045\t\njumps\t20.085536923187668\t7.38905609893065\t2.718281828459045\t2.718281828459045\t\nbrown\t7.38905609893065\t7.38905609893065\t2.718281828459045\t2.718281828459045\t\ndog\t7.38905609893065\t2.718281828459045\t7.38905609893065\t2.718281828459045\t\nfox\t2.718281828459045\t2.718281828459045\t7.38905609893065\t7.38905609893065\t\n", string0);
      
      String[] stringArray0 = naiveBayesMultinomialText0.getOptions();
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals(12, stringArray0.length);
  }

  /**
  //Test case number: 11
  /*Coverage entropy=0.6931471805599453
  */
  @Test(timeout = 4000)
  public void test11()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      String string0 = naiveBayesMultinomialText0.minWordFrequencyTipText();
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", string0);
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
  }

  /**
  //Test case number: 12
  /*Coverage entropy=0.6931471805599453
  */
  @Test(timeout = 4000)
  public void test12()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      String string0 = naiveBayesMultinomialText0.LNormTipText();
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", string0);
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
  }

  /**
  //Test case number: 13
  /*Coverage entropy=0.6931471805599453
  */
  @Test(timeout = 4000)
  public void test13()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      String string0 = naiveBayesMultinomialText0.lowercaseTokensTipText();
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Whether to convert all tokens to lowercase", string0);
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
  }

  /**
  //Test case number: 14
  /*Coverage entropy=2.6571479316791113
  */
  @Test(timeout = 4000)
  public void test14()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      InputMappedClassifier inputMappedClassifier0 = new InputMappedClassifier();
      InputMappedClassifier inputMappedClassifier1 = new InputMappedClassifier();
      Capabilities capabilities0 = inputMappedClassifier1.getCapabilities();
      TestInstances testInstances0 = TestInstances.forCapabilities(capabilities0);
      Instances instances0 = testInstances0.generate("elily");
      inputMappedClassifier1.setModelHeader(instances0);
      naiveBayesMultinomialText0.setNormalizeDocLength(true);
      naiveBayesMultinomialText0.buildClassifier(instances0);
      capabilities0.dependencies();
      naiveBayesMultinomialText0.toString();
      Random.setNextRandom((-2));
      naiveBayesMultinomialText0.toString();
      Random.setNextRandom(3810);
      System.setCurrentTimeMillis(0L);
      double[] doubleArray0 = new double[8];
      naiveBayesMultinomialText0.setUseStopList(true);
      int[] intArray0 = new int[1];
      intArray0[0] = 3810;
      SparseInstance sparseInstance0 = new SparseInstance(0L, doubleArray0, intArray0, 0);
      FileSystemHandling.appendLineToFile((EvoSuiteFile) null, " ");
      naiveBayesMultinomialText0.distributionForInstance(sparseInstance0);
      assertTrue(naiveBayesMultinomialText0.getUseStopList());
      
      Random.setNextRandom(30);
      NaiveBayesMultinomialText naiveBayesMultinomialText1 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.setOptions(testInstances0.DEFAULT_WORDS);
      AbstractClassifier.makeCopy(inputMappedClassifier0);
      String[] stringArray0 = naiveBayesMultinomialText0.getOptions();
      naiveBayesMultinomialText1.setOptions(stringArray0);
      naiveBayesMultinomialText1.setOptions(stringArray0);
      naiveBayesMultinomialText1.toString();
      naiveBayesMultinomialText1.setOptions(testInstances0.DEFAULT_WORDS);
      assertEquals(2.0, naiveBayesMultinomialText1.getLNorm(), 0.01);
  }

  /**
  //Test case number: 15
  /*Coverage entropy=0.6931471805599453
  */
  @Test(timeout = 4000)
  public void test15()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      String string0 = naiveBayesMultinomialText0.normalizeDocLengthTipText();
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", string0);
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
  }

  /**
  //Test case number: 16
  /*Coverage entropy=0.6931471805599453
  */
  @Test(timeout = 4000)
  public void test16()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      String string0 = naiveBayesMultinomialText0.periodicPruningTipText();
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", string0);
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
  }

  /**
  //Test case number: 17
  /*Coverage entropy=0.6931471805599453
  */
  @Test(timeout = 4000)
  public void test17()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      String string0 = naiveBayesMultinomialText0.stopwordsTipText();
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", string0);
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
  }

  /**
  //Test case number: 18
  /*Coverage entropy=2.2523542313691998
  */
  @Test(timeout = 4000)
  public void test18()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      
      naiveBayesMultinomialText0.m_wordFrequencies = true;
      naiveBayesMultinomialText0.listOptions();
      naiveBayesMultinomialText0.getOptions();
      assertTrue(naiveBayesMultinomialText0.getUseWordFrequencies());
  }

  /**
  //Test case number: 19
  /*Coverage entropy=2.2717760071931643
  */
  @Test(timeout = 4000)
  public void test19()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.setLowercaseTokens(true);
      InputMappedClassifier inputMappedClassifier0 = new InputMappedClassifier();
      InputMappedClassifier inputMappedClassifier1 = new InputMappedClassifier();
      Capabilities capabilities0 = inputMappedClassifier1.getCapabilities();
      TestInstances testInstances0 = TestInstances.forCapabilities(capabilities0);
      Instances instances0 = testInstances0.generate("elily");
      naiveBayesMultinomialText0.buildClassifier(instances0);
      capabilities0.dependencies();
      naiveBayesMultinomialText0.toString();
      Random.setNextRandom((-2));
      naiveBayesMultinomialText0.toString();
      naiveBayesMultinomialText0.getOptions();
      assertTrue(naiveBayesMultinomialText0.getLowercaseTokens());
  }

  /**
  //Test case number: 20
  /*Coverage entropy=2.5649493574615376
  */
  @Test(timeout = 4000)
  public void test20()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.m_useStopList = true;
      naiveBayesMultinomialText0.getNorm();
      naiveBayesMultinomialText0.getOptions();
      assertTrue(naiveBayesMultinomialText0.getUseStopList());
  }

  /**
  //Test case number: 21
  /*Coverage entropy=2.5649493574615376
  */
  @Test(timeout = 4000)
  public void test21()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.m_stemmer = null;
      String[] stringArray0 = naiveBayesMultinomialText0.getOptions();
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals(10, stringArray0.length);
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
  }

  /**
  //Test case number: 22
  /*Coverage entropy=2.5649493574615376
  */
  @Test(timeout = 4000)
  public void test22()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      SerializedClassifier serializedClassifier0 = new SerializedClassifier();
      File file0 = serializedClassifier0.getModelFile();
      MockFile mockFile0 = new MockFile(file0, "?.ya6r{e");
      naiveBayesMultinomialText0.m_stopwordsFile = (File) mockFile0;
      naiveBayesMultinomialText0.getOptions();
      BinarySparseInstance binarySparseInstance0 = null;
      try {
        binarySparseInstance0 = new BinarySparseInstance((-13));
        fail("Expecting exception: NegativeArraySizeException");
      
      } catch(NegativeArraySizeException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("weka.core.BinarySparseInstance", e);
      }
  }

  /**
  //Test case number: 23
  /*Coverage entropy=1.7950641206096323
  */
  @Test(timeout = 4000)
  public void test23()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      File file0 = MockFile.createTempFile("No=j}0k-", "No=j}0k-");
      naiveBayesMultinomialText0.setStopwords(file0);
      InputMappedClassifier inputMappedClassifier0 = new InputMappedClassifier();
      byte[] byteArray0 = new byte[3];
      byteArray0[0] = (byte) (-74);
      byteArray0[1] = (byte) (-90);
      byteArray0[2] = (byte)57;
      FileSystemHandling.appendDataToFile((EvoSuiteFile) null, byteArray0);
      Capabilities capabilities0 = inputMappedClassifier0.getCapabilities();
      TestInstances testInstances0 = TestInstances.forCapabilities(capabilities0);
      FileSystemHandling.setPermissions((EvoSuiteFile) null, true, true, true);
      Instances instances0 = testInstances0.generate("weka/core/Capabilities.props");
      naiveBayesMultinomialText0.buildClassifier(instances0);
      double[] doubleArray0 = new double[1];
      doubleArray0[0] = (double) (-2);
      int[] intArray0 = new int[6];
      intArray0[0] = (int) (byte) (-74);
      intArray0[1] = (int) (byte)57;
      intArray0[2] = (int) (byte)57;
      intArray0[3] = (int) (byte) (-74);
      intArray0[4] = (int) (byte) (-74);
      intArray0[5] = (int) (byte) (-90);
      SparseInstance sparseInstance0 = new SparseInstance((-2), doubleArray0, intArray0, (-2884));
      naiveBayesMultinomialText0.distributionForInstance(sparseInstance0);
      naiveBayesMultinomialText0.toString();
      naiveBayesMultinomialText0.toString();
      naiveBayesMultinomialText0.pruneDictionary();
      naiveBayesMultinomialText0.toString();
      naiveBayesMultinomialText0.tokenizeInstance(sparseInstance0, false);
      assertTrue(naiveBayesMultinomialText0.getUseStopList());
  }

  /**
  //Test case number: 24
  /*Coverage entropy=1.8393603972335884
  */
  @Test(timeout = 4000)
  public void test24()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      RandomForest randomForest0 = new RandomForest();
      TestInstances testInstances0 = new TestInstances();
      AlphabeticTokenizer alphabeticTokenizer0 = new AlphabeticTokenizer();
      testInstances0.setHandler(randomForest0);
      Instances instances0 = testInstances0.generate("m|)");
      naiveBayesMultinomialText0.m_normalize = true;
      naiveBayesMultinomialText0.debugTipText();
      testInstances0.toString();
      naiveBayesMultinomialText0.buildClassifier(instances0);
      naiveBayesMultinomialText0.toString();
      double[] doubleArray0 = new double[1];
      int[] intArray0 = new int[0];
      SparseInstance sparseInstance0 = new SparseInstance((-1), doubleArray0, intArray0, (-1));
      naiveBayesMultinomialText0.distributionForInstance(sparseInstance0);
      naiveBayesMultinomialText0.toString();
      naiveBayesMultinomialText0.toString();
      naiveBayesMultinomialText0.pruneDictionary();
  }

  /**
  //Test case number: 25
  /*Coverage entropy=2.5649493574615376
  */
  @Test(timeout = 4000)
  public void test25()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.m_normalize = true;
      naiveBayesMultinomialText0.getOptions();
  }

  /**
  //Test case number: 26
  /*Coverage entropy=1.0986122886681096
  */
  @Test(timeout = 4000)
  public void test26()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.m_t = 3.0;
      naiveBayesMultinomialText0.setPeriodicPruning(2751);
      naiveBayesMultinomialText0.pruneDictionary();
  }

  /**
  //Test case number: 27
  /*Coverage entropy=2.3025850929940455
  */
  @Test(timeout = 4000)
  public void test27()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      InputMappedClassifier inputMappedClassifier0 = new InputMappedClassifier();
      InputMappedClassifier inputMappedClassifier1 = new InputMappedClassifier();
      Capabilities capabilities0 = inputMappedClassifier1.getCapabilities();
      TestInstances testInstances0 = TestInstances.forCapabilities(capabilities0);
      Instances instances0 = testInstances0.generate("end");
      DenseInstance denseInstance0 = new DenseInstance(2372);
      instances0.add((Instance) denseInstance0);
      naiveBayesMultinomialText0.buildClassifier(instances0);
      capabilities0.dependencies();
      naiveBayesMultinomialText0.toString();
      Random.setNextRandom(1610612736);
      NaiveBayesMultinomialText naiveBayesMultinomialText1 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText1.toString();
  }

  /**
  //Test case number: 28
  /*Coverage entropy=2.3025850929940455
  */
  @Test(timeout = 4000)
  public void test28()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.setOptions((String[]) null);
      InputMappedClassifier inputMappedClassifier0 = new InputMappedClassifier();
      Capabilities capabilities0 = inputMappedClassifier0.getCapabilities();
      TestInstances testInstances0 = TestInstances.forCapabilities(capabilities0);
      String[] stringArray0 = new String[6];
      stringArray0[0] = "";
      stringArray0[1] = "IQskbTZ?CAe";
      stringArray0[2] = "weka/core/Capabilities.props";
      stringArray0[3] = "-stemmer";
      stringArray0[4] = " ";
      testInstances0.getOptions();
      stringArray0[5] = "oYB.#";
      try { 
        naiveBayesMultinomialText0.setOptions(stringArray0);
        fail("Expecting exception: Exception");
      
      } catch(Exception e) {
         //
         // Invalid stemmer specification string
         //
         verifyException("weka.classifiers.bayes.NaiveBayesMultinomialText", e);
      }
  }

  /**
  //Test case number: 29
  /*Coverage entropy=2.3025850929940455
  */
  @Test(timeout = 4000)
  public void test29()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.getPeriodicPruning();
      String[] stringArray0 = new String[9];
      stringArray0[0] = "+1~iWF*![8{e\"-QlkF";
      stringArray0[1] = "+1~iWF*![8{e\"-QlkF";
      stringArray0[2] = "-stemmer";
      stringArray0[3] = "pZa&$1tf";
      stringArray0[4] = "-stopwords";
      SerializedClassifier serializedClassifier0 = new SerializedClassifier();
      File file0 = serializedClassifier0.getModelFile();
      naiveBayesMultinomialText0.m_stopwordsFile = file0;
      stringArray0[5] = "kP)DKg,1=<6_O)B";
      stringArray0[6] = "";
      stringArray0[7] = "NaiveBayesMultinomialText: No model built yet.\n";
      stringArray0[8] = "";
      try { 
        naiveBayesMultinomialText0.setOptions(stringArray0);
        fail("Expecting exception: ClassNotFoundException");
      
      } catch(ClassNotFoundException e) {
      }
  }

  /**
  //Test case number: 30
  /*Coverage entropy=3.2580965380214835
  */
  @Test(timeout = 4000)
  public void test30()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      SnowballStemmer snowballStemmer0 = new SnowballStemmer();
      naiveBayesMultinomialText0.m_stemmer = (Stemmer) snowballStemmer0;
      String[] stringArray0 = naiveBayesMultinomialText0.getOptions();
      naiveBayesMultinomialText0.setOptions(stringArray0);
      Random.setNextRandom(1400);
  }

  /**
  //Test case number: 31
  /*Coverage entropy=2.3978952727983707
  */
  @Test(timeout = 4000)
  public void test31()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      InputMappedClassifier inputMappedClassifier0 = new InputMappedClassifier();
      naiveBayesMultinomialText0.setUseWordFrequencies(true);
      InputMappedClassifier inputMappedClassifier1 = new InputMappedClassifier();
      Capabilities capabilities0 = inputMappedClassifier1.getCapabilities();
      TestInstances testInstances0 = TestInstances.forCapabilities(capabilities0);
      Instances instances0 = testInstances0.generate("weka/core/Capabilities.props");
      naiveBayesMultinomialText0.setNormalizeDocLength(true);
      naiveBayesMultinomialText0.buildClassifier(instances0);
      int[] intArray0 = new int[0];
      SparseInstance sparseInstance0 = null;
      try {
        sparseInstance0 = new SparseInstance((-2869), (double[]) null, intArray0, (-1));
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("weka.core.SparseInstance", e);
      }
  }

  /**
  //Test case number: 32
  /*Coverage entropy=2.3025850929940455
  */
  @Test(timeout = 4000)
  public void test32()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      InputMappedClassifier inputMappedClassifier0 = new InputMappedClassifier();
      naiveBayesMultinomialText0.setUseWordFrequencies(true);
      InputMappedClassifier inputMappedClassifier1 = new InputMappedClassifier();
      Capabilities capabilities0 = inputMappedClassifier1.getCapabilities();
      TestInstances testInstances0 = TestInstances.forCapabilities(capabilities0);
      Instances instances0 = testInstances0.generate("elily");
      naiveBayesMultinomialText0.buildClassifier(instances0);
      capabilities0.dependencies();
      Random.setNextRandom((-2));
  }

  /**
  //Test case number: 33
  /*Coverage entropy=2.0794415416798357
  */
  @Test(timeout = 4000)
  public void test33()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      RandomForest randomForest0 = new RandomForest();
      TestInstances testInstances0 = new TestInstances();
      AlphabeticTokenizer alphabeticTokenizer0 = new AlphabeticTokenizer();
      testInstances0.setNoClass(true);
      String[] stringArray0 = new String[6];
      stringArray0[0] = "-tokenizer";
      stringArray0[1] = " ";
      stringArray0[2] = "-tokenizer";
      stringArray0[3] = " ";
      stringArray0[4] = "-tokenizer";
      stringArray0[5] = "-tokenizer";
      try { 
        naiveBayesMultinomialText0.setOptions(stringArray0);
        fail("Expecting exception: Exception");
      
      } catch(Exception e) {
         //
         // Invalid tokenizer specification string
         //
         verifyException("weka.classifiers.bayes.NaiveBayesMultinomialText", e);
      }
  }

  /**
  //Test case number: 34
  /*Coverage entropy=2.3025850929940455
  */
  @Test(timeout = 4000)
  public void test34()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      InputMappedClassifier inputMappedClassifier0 = new InputMappedClassifier();
      naiveBayesMultinomialText0.m_useStopList = true;
      Capabilities capabilities0 = inputMappedClassifier0.getCapabilities();
      TestInstances testInstances0 = TestInstances.forCapabilities(capabilities0);
      String string0 = "elily";
      Instances instances0 = testInstances0.generate("elily");
      naiveBayesMultinomialText0.buildClassifier(instances0);
      FileSystemHandling fileSystemHandling0 = new FileSystemHandling();
      capabilities0.dependencies();
      try { 
        AbstractClassifier.makeCopies(inputMappedClassifier0, (-2779));
        fail("Expecting exception: NegativeArraySizeException");
      
      } catch(NegativeArraySizeException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("weka.classifiers.AbstractClassifier", e);
      }
  }

  /**
  //Test case number: 35
  /*Coverage entropy=1.6094379124341005
  */
  @Test(timeout = 4000)
  public void test35()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      double[] doubleArray0 = new double[7];
      doubleArray0[0] = 23.0;
      MockFile mockFile0 = new MockFile(".v/", ".v/");
      naiveBayesMultinomialText0.setStopwords(mockFile0);
      doubleArray0[1] = (-191.5824);
      doubleArray0[2] = 210.7;
      doubleArray0[3] = 9.007858875772442;
      doubleArray0[4] = (-1395.1905);
      doubleArray0[5] = 2636.9309704;
      doubleArray0[6] = 7250.24081908352;
      naiveBayesMultinomialText0.m_wordsPerClass = doubleArray0;
      naiveBayesMultinomialText0.m_useStopList = true;
      try { 
        naiveBayesMultinomialText0.distributionForInstance((Instance) null);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("weka.classifiers.bayes.NaiveBayesMultinomialText", e);
      }
  }

  /**
  //Test case number: 36
  /*Coverage entropy=2.0347756587756045
  */
  @Test(timeout = 4000)
  public void test36()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      HashMap<Integer, LinkedHashMap<String, NaiveBayesMultinomialText.Count>> hashMap0 = new HashMap<Integer, LinkedHashMap<String, NaiveBayesMultinomialText.Count>>();
      naiveBayesMultinomialText0.m_probOfWordGivenClass = (Map<Integer, LinkedHashMap<String, NaiveBayesMultinomialText.Count>>) hashMap0;
      hashMap0.putAll(naiveBayesMultinomialText0.m_probOfWordGivenClass);
      naiveBayesMultinomialText0.m_periodicP = 802;
      Integer integer0 = new Integer(802);
      LinkedHashMap<String, NaiveBayesMultinomialText.Count> linkedHashMap0 = new LinkedHashMap<String, NaiveBayesMultinomialText.Count>();
      naiveBayesMultinomialText0.pruneDictionary();
      Capabilities capabilities0 = naiveBayesMultinomialText0.getCapabilities();
      TestInstances testInstances0 = TestInstances.forCapabilities(capabilities0);
      Instances instances0 = testInstances0.generate(" ");
      NaiveBayesMultinomialText naiveBayesMultinomialText1 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText1.buildClassifier(instances0);
      naiveBayesMultinomialText1.toString();
      LinkedHashMap<String, NaiveBayesMultinomialText.Count> linkedHashMap1 = naiveBayesMultinomialText1.m_inputVector;
      hashMap0.put(integer0, linkedHashMap1);
      naiveBayesMultinomialText0.toString();
      naiveBayesMultinomialText0.pruneDictionary();
      System.setCurrentTimeMillis((-2L));
      naiveBayesMultinomialText0.setOptions(testInstances0.DEFAULT_WORDS);
  }

  /**
  //Test case number: 37
  /*Coverage entropy=2.3978952727983707
  */
  @Test(timeout = 4000)
  public void test37()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      HashMap<Integer, LinkedHashMap<String, NaiveBayesMultinomialText.Count>> hashMap0 = new HashMap<Integer, LinkedHashMap<String, NaiveBayesMultinomialText.Count>>();
      naiveBayesMultinomialText0.m_probOfWordGivenClass = (Map<Integer, LinkedHashMap<String, NaiveBayesMultinomialText.Count>>) hashMap0;
      int int0 = 802;
      naiveBayesMultinomialText0.setMinWordFrequency(802);
      hashMap0.putAll(naiveBayesMultinomialText0.m_probOfWordGivenClass);
      naiveBayesMultinomialText0.m_periodicP = 802;
      Integer integer0 = new Integer(802);
      LinkedHashMap<String, NaiveBayesMultinomialText.Count> linkedHashMap0 = new LinkedHashMap<String, NaiveBayesMultinomialText.Count>();
      naiveBayesMultinomialText0.pruneDictionary();
      Capabilities capabilities0 = naiveBayesMultinomialText0.getCapabilities();
      TestInstances testInstances0 = TestInstances.forCapabilities(capabilities0);
      Instances instances0 = testInstances0.generate(" ");
      NaiveBayesMultinomialText naiveBayesMultinomialText1 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText1.buildClassifier(instances0);
      naiveBayesMultinomialText1.toString();
      LinkedHashMap<String, NaiveBayesMultinomialText.Count> linkedHashMap1 = naiveBayesMultinomialText1.m_inputVector;
      hashMap0.put(integer0, linkedHashMap1);
      naiveBayesMultinomialText0.toString();
      // Undeclared exception!
      try { 
        naiveBayesMultinomialText0.pruneDictionary();
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("weka.classifiers.bayes.NaiveBayesMultinomialText", e);
      }
  }
}
