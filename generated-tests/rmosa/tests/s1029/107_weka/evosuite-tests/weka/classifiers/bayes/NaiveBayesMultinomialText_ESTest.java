/*
 * This file was automatically generated by EvoSuite
 * Thu Aug 23 18:46:46 GMT 2018
 */

package weka.classifiers.bayes;

import org.junit.Test;
import static org.junit.Assert.*;
import static org.evosuite.runtime.EvoAssertions.*;
import java.io.File;
import java.util.LinkedHashMap;
import java.util.Locale;
import org.evosuite.runtime.EvoRunner;
import org.evosuite.runtime.EvoRunnerParameters;
import org.evosuite.runtime.Random;
import org.evosuite.runtime.System;
import org.evosuite.runtime.mock.java.io.MockFile;
import org.evosuite.runtime.testdata.EvoSuiteFile;
import org.evosuite.runtime.testdata.FileSystemHandling;
import org.evosuite.runtime.util.SystemInUtil;
import org.junit.runner.RunWith;
import weka.classifiers.AbstractClassifier;
import weka.classifiers.Classifier;
import weka.classifiers.bayes.NaiveBayesMultinomialText;
import weka.classifiers.functions.GaussianProcesses;
import weka.classifiers.functions.supportVector.PrecomputedKernelMatrixKernel;
import weka.classifiers.lazy.IBk;
import weka.classifiers.misc.InputMappedClassifier;
import weka.classifiers.misc.SerializedClassifier;
import weka.core.AbstractInstance;
import weka.core.Attribute;
import weka.core.BinarySparseInstance;
import weka.core.Capabilities;
import weka.core.DenseInstance;
import weka.core.Instance;
import weka.core.Instances;
import weka.core.TestInstances;
import weka.core.neighboursearch.BallTree;
import weka.core.neighboursearch.CoverTree;
import weka.core.neighboursearch.LinearNNSearch;
import weka.core.stemmers.IteratedLovinsStemmer;
import weka.core.stemmers.LovinsStemmer;
import weka.core.stemmers.SnowballStemmer;
import weka.core.stemmers.Stemmer;
import weka.core.tokenizers.AlphabeticTokenizer;
import weka.core.tokenizers.WordTokenizer;
import weka.filters.MultiFilter;

@RunWith(EvoRunner.class) @EvoRunnerParameters(mockJVMNonDeterminism = true, useVFS = true, useVNET = true, resetStaticState = true, separateClassLoader = true, useJEE = true) 
public class NaiveBayesMultinomialText_ESTest extends NaiveBayesMultinomialText_ESTest_scaffolding {

  /**
  //Test case number: 0
  /*Coverage entropy=0.6931471805599453
  */
  @Test(timeout = 4000)
  public void test00()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.listOptions();
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
  }

  /**
  //Test case number: 1
  /*Coverage entropy=0.6931471805599453
  */
  @Test(timeout = 4000)
  public void test01()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      String string0 = naiveBayesMultinomialText0.toString();
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("NaiveBayesMultinomialText: No model built yet.\n", string0);
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
  }

  /**
  //Test case number: 2
  /*Coverage entropy=2.1816004996238716
  */
  @Test(timeout = 4000)
  public void test02()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.setNormalizeDocLength(true);
      naiveBayesMultinomialText0.m_useStopList = true;
      IteratedLovinsStemmer iteratedLovinsStemmer0 = new IteratedLovinsStemmer();
      InputMappedClassifier inputMappedClassifier0 = new InputMappedClassifier();
      Capabilities capabilities0 = inputMappedClassifier0.getCapabilities();
      TestInstances testInstances0 = TestInstances.forCapabilities(capabilities0);
      Instances instances0 = testInstances0.generate();
      naiveBayesMultinomialText0.buildClassifier(instances0);
      System.setCurrentTimeMillis(1691L);
      Locale.getISOLanguages();
      naiveBayesMultinomialText0.toString();
      NaiveBayesMultinomialText naiveBayesMultinomialText1 = new NaiveBayesMultinomialText();
      int[] intArray0 = new int[0];
      BinarySparseInstance binarySparseInstance0 = new BinarySparseInstance((-267.77445), intArray0, (-2));
      FileSystemHandling.appendStringToFile((EvoSuiteFile) null, "");
      InputMappedClassifier inputMappedClassifier1 = new InputMappedClassifier();
      inputMappedClassifier1.getCapabilities();
      naiveBayesMultinomialText0.distributionForInstance(binarySparseInstance0);
      naiveBayesMultinomialText1.pruneDictionary();
      NaiveBayesMultinomialText naiveBayesMultinomialText2 = new NaiveBayesMultinomialText();
      capabilities0.clone();
      naiveBayesMultinomialText2.getOptions();
      LinearNNSearch linearNNSearch0 = new LinearNNSearch(instances0);
      Instance instance0 = linearNNSearch0.nearestNeighbour(binarySparseInstance0);
      naiveBayesMultinomialText0.distributionForInstance(instance0);
      try { 
        naiveBayesMultinomialText0.updateClassifier((Instance) binarySparseInstance0, true);
        fail("Expecting exception: RuntimeException");
      
      } catch(RuntimeException e) {
         //
         // DenseInstance doesn't have access to a dataset!
         //
         verifyException("weka.core.AbstractInstance", e);
      }
  }

  /**
  //Test case number: 3
  /*Coverage entropy=0.6931471805599453
  */
  @Test(timeout = 4000)
  public void test03()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      String string0 = naiveBayesMultinomialText0.periodicPruningTipText();
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", string0);
  }

  /**
  //Test case number: 4
  /*Coverage entropy=0.6931471805599453
  */
  @Test(timeout = 4000)
  public void test04()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      String string0 = naiveBayesMultinomialText0.normalizeDocLengthTipText();
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", string0);
  }

  /**
  //Test case number: 5
  /*Coverage entropy=1.0986122886681096
  */
  @Test(timeout = 4000)
  public void test05()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.pruneDictionary();
      naiveBayesMultinomialText0.globalInfo();
      System.setCurrentTimeMillis(62);
  }

  /**
  //Test case number: 6
  /*Coverage entropy=0.6931471805599453
  */
  @Test(timeout = 4000)
  public void test06()  throws Throwable  {
      SerializedClassifier serializedClassifier0 = new SerializedClassifier();
      String[] stringArray0 = new String[7];
      NaiveBayesMultinomialText.main(stringArray0);
      assertEquals(7, stringArray0.length);
  }

  /**
  //Test case number: 7
  /*Coverage entropy=0.6931471805599453
  */
  @Test(timeout = 4000)
  public void test07()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.LNormTipText();
      Random.setNextRandom(1);
  }

  /**
  //Test case number: 8
  /*Coverage entropy=0.6931471805599453
  */
  @Test(timeout = 4000)
  public void test08()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      String string0 = naiveBayesMultinomialText0.useWordFrequenciesTipText();
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("Use word frequencies rather than binary bag of words representation", string0);
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
  }

  /**
  //Test case number: 9
  /*Coverage entropy=0.6931471805599453
  */
  @Test(timeout = 4000)
  public void test09()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      String string0 = naiveBayesMultinomialText0.minWordFrequencyTipText();
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", string0);
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
  }

  /**
  //Test case number: 10
  /*Coverage entropy=1.715263227902811
  */
  @Test(timeout = 4000)
  public void test10()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      String[] stringArray0 = new String[4];
      stringArray0[0] = "-tokenizer";
      stringArray0[1] = "-tokenizer";
      EvoSuiteFile evoSuiteFile0 = new EvoSuiteFile("/home/ubuntu/evosuite_readability_gen/projects/107_weka");
      FileSystemHandling.setPermissions(evoSuiteFile0, false, true, false);
      stringArray0[2] = "The norm of the instances after normalization.";
      stringArray0[3] = "-tokenizer";
      try { 
        naiveBayesMultinomialText0.setOptions(stringArray0);
        fail("Expecting exception: ClassNotFoundException");
      
      } catch(ClassNotFoundException e) {
      }
  }

  /**
  //Test case number: 11
  /*Coverage entropy=2.5769551160408337
  */
  @Test(timeout = 4000)
  public void test11()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.setNormalizeDocLength(true);
      naiveBayesMultinomialText0.m_useStopList = true;
      IteratedLovinsStemmer iteratedLovinsStemmer0 = new IteratedLovinsStemmer();
      InputMappedClassifier inputMappedClassifier0 = new InputMappedClassifier();
      Capabilities capabilities0 = inputMappedClassifier0.getCapabilities();
      TestInstances testInstances0 = TestInstances.forCapabilities(capabilities0);
      Instances instances0 = testInstances0.generate();
      naiveBayesMultinomialText0.buildClassifier(instances0);
      naiveBayesMultinomialText0.toString();
      naiveBayesMultinomialText0.pruneDictionary();
      naiveBayesMultinomialText0.setOptions(testInstances0.DEFAULT_WORDS);
      naiveBayesMultinomialText0.setOptions(testInstances0.DEFAULT_WORDS);
      String[] stringArray0 = naiveBayesMultinomialText0.getOptions();
      LinearNNSearch linearNNSearch0 = new LinearNNSearch(instances0);
      CoverTree coverTree0 = new CoverTree();
      double[] doubleArray0 = new double[9];
      doubleArray0[0] = (double) (-1);
      capabilities0.enableAllClasses();
      naiveBayesMultinomialText0.setOptions(stringArray0);
      naiveBayesMultinomialText0.getOptions();
      naiveBayesMultinomialText0.getOptions();
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
  }

  /**
  //Test case number: 12
  /*Coverage entropy=3.2580965380214835
  */
  @Test(timeout = 4000)
  public void test12()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      AbstractClassifier.makeCopy(naiveBayesMultinomialText0);
      SnowballStemmer snowballStemmer0 = new SnowballStemmer();
      naiveBayesMultinomialText0.m_stemmer = (Stemmer) snowballStemmer0;
      String[] stringArray0 = naiveBayesMultinomialText0.getOptions();
      naiveBayesMultinomialText0.setLowercaseTokens(false);
      naiveBayesMultinomialText0.setOptions(stringArray0);
      String[] stringArray1 = new String[8];
      stringArray1[0] = "org.tartarus.snowball.ext";
      stringArray1[3] = "org.tartarus.snowball.ext";
      stringArray1[4] = "org.tartarus.snowball.ext";
      // Undeclared exception!
      try { 
        snowballStemmer0.stem((String) null);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
      }
  }

  /**
  //Test case number: 13
  /*Coverage entropy=1.7826121223721445
  */
  @Test(timeout = 4000)
  public void test13()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      assertNotNull(naiveBayesMultinomialText0);
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      
      naiveBayesMultinomialText0.setNormalizeDocLength(true);
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertTrue(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      
      boolean boolean0 = FileSystemHandling.shouldAllThrowIOExceptions();
      assertTrue(boolean0);
      
      TestInstances testInstances0 = new TestInstances();
      assertNotNull(testInstances0);
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(1, testInstances0.getSeed());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(0, testInstances0.getNumString());
      assertFalse(testInstances0.getNoClass());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      
      Instances instances0 = testInstances0.generate("-not-nominal-atts");
      assertNotNull(instances0);
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(1, testInstances0.getSeed());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(0, testInstances0.getNumString());
      assertFalse(testInstances0.getNoClass());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(1, instances0.classIndex());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(20, instances0.numInstances());
      assertEquals(20, instances0.size());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(2, instances0.numAttributes());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(2, instances0.numClasses());
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      
      naiveBayesMultinomialText0.buildClassifier(instances0);
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertTrue(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(1, testInstances0.getSeed());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(0, testInstances0.getNumString());
      assertFalse(testInstances0.getNoClass());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(1, instances0.classIndex());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(20, instances0.numInstances());
      assertEquals(20, instances0.size());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(2, instances0.numAttributes());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(2, instances0.numClasses());
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      
      WordTokenizer wordTokenizer0 = new WordTokenizer();
      assertNotNull(wordTokenizer0);
      assertEquals("Set of delimiter characters to use in tokenizing (\\r, \\n and \\t can be used for carriage-return, line-feed and tab)", wordTokenizer0.delimitersTipText());
      assertEquals(" \r\n\t.,;:'\"()?!", wordTokenizer0.getDelimiters());
      assertEquals("A simple tokenizer that is using the java.util.StringTokenizer class to tokenize the strings.", wordTokenizer0.globalInfo());
      
      testInstances0.setNumRelationalDate((-2));
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(1, testInstances0.getSeed());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(0, testInstances0.getNumString());
      assertFalse(testInstances0.getNoClass());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals((-2), testInstances0.getNumRelationalDate());
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      
      int[] intArray0 = new int[8];
      naiveBayesMultinomialText0.pruneDictionary();
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertTrue(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      
      intArray0[1] = (-1);
      intArray0[3] = (-2);
      intArray0[5] = (-2);
      BinarySparseInstance binarySparseInstance0 = new BinarySparseInstance((-1), intArray0, (-805));
      assertNotNull(binarySparseInstance0);
      assertArrayEquals(new int[] {0, (-1), 0, (-2), 0, (-2), 0, 0}, intArray0);
      assertEquals((-805), binarySparseInstance0.numAttributes());
      assertEquals((-1.0), binarySparseInstance0.weight(), 0.01);
      assertEquals(8, binarySparseInstance0.numValues());
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertEquals(8, intArray0.length);
      
      double[] doubleArray0 = naiveBayesMultinomialText0.distributionForInstance(binarySparseInstance0);
      assertNotNull(doubleArray0);
      assertArrayEquals(new int[] {0, (-1), 0, (-2), 0, (-2), 0, 0}, intArray0);
      assertArrayEquals(new double[] {0.5454545454545454, 0.4545454545454546}, doubleArray0, 0.01);
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertTrue(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals((-805), binarySparseInstance0.numAttributes());
      assertEquals((-1.0), binarySparseInstance0.weight(), 0.01);
      assertEquals(8, binarySparseInstance0.numValues());
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertEquals(8, intArray0.length);
      assertEquals(2, doubleArray0.length);
      
      BallTree ballTree0 = new BallTree();
      assertNotNull(ballTree0);
      assertEquals(0.0, ballTree0.measureMaxDepth(), 0.01);
      assertEquals("Whether to calculate performance statistics for the NN search or not", ballTree0.measurePerformanceTipText());
      assertEquals("The distance function to use for finding neighbours (default: weka.core.EuclideanDistance). ", ballTree0.distanceFunctionTipText());
      assertEquals(0.0, ballTree0.measureNumLeaves(), 0.01);
      assertEquals(0.0, ballTree0.measureTreeSize(), 0.01);
      assertFalse(ballTree0.getMeasurePerformance());
      assertEquals("The tree constructor being used.", ballTree0.ballTreeConstructorTipText());
      
      NaiveBayesMultinomialText naiveBayesMultinomialText1 = (NaiveBayesMultinomialText)AbstractClassifier.makeCopy(naiveBayesMultinomialText0);
      assertNotSame(naiveBayesMultinomialText1, naiveBayesMultinomialText0);
      assertNotSame(naiveBayesMultinomialText0, naiveBayesMultinomialText1);
      assertNotNull(naiveBayesMultinomialText1);
      assertFalse(naiveBayesMultinomialText1.equals((Object)naiveBayesMultinomialText0));
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText1.lowercaseTokensTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText1.minWordFrequencyTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText1.LNormTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText1.stemmerTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText1.normalizeDocLengthTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText1.normTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText1.tokenizerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText1.useStopListTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText1.periodicPruningTipText());
      assertTrue(naiveBayesMultinomialText1.getNormalizeDocLength());
      assertEquals(3.0, naiveBayesMultinomialText1.getMinWordFrequency(), 0.01);
      assertFalse(naiveBayesMultinomialText1.getDebug());
      assertFalse(naiveBayesMultinomialText1.getUseStopList());
      assertEquals(1.0, naiveBayesMultinomialText1.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText1.getLowercaseTokens());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText1.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText1.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText1.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText1.globalInfo());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText1.stopwordsTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText1.debugTipText());
      assertEquals(2.0, naiveBayesMultinomialText1.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertTrue(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      
      try { 
        ballTree0.nearestNeighbour(binarySparseInstance0);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("weka.core.neighboursearch.BallTree", e);
      }
  }

  /**
  //Test case number: 14
  /*Coverage entropy=0.6931471805599453
  */
  @Test(timeout = 4000)
  public void test14()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      assertNotNull(naiveBayesMultinomialText0);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      
      String string0 = naiveBayesMultinomialText0.stemmerTipText();
      assertNotNull(string0);
      assertEquals("The stemming algorithm to use on the words.", string0);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
  }

  /**
  //Test case number: 15
  /*Coverage entropy=0.6931471805599453
  */
  @Test(timeout = 4000)
  public void test15()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      assertNotNull(naiveBayesMultinomialText0);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      
      String string0 = naiveBayesMultinomialText0.stopwordsTipText();
      assertNotNull(string0);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", string0);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
  }

  /**
  //Test case number: 16
  /*Coverage entropy=0.6931471805599453
  */
  @Test(timeout = 4000)
  public void test16()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      assertNotNull(naiveBayesMultinomialText0);
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      
      String string0 = naiveBayesMultinomialText0.lowercaseTokensTipText();
      assertNotNull(string0);
      assertEquals("Whether to convert all tokens to lowercase", string0);
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
  }

  /**
  //Test case number: 17
  /*Coverage entropy=0.6931471805599453
  */
  @Test(timeout = 4000)
  public void test17()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      assertNotNull(naiveBayesMultinomialText0);
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      
      String string0 = naiveBayesMultinomialText0.tokenizerTipText();
      assertNotNull(string0);
      assertEquals("The tokenizing algorithm to use on the strings.", string0);
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
  }

  /**
  //Test case number: 18
  /*Coverage entropy=2.257971678971788
  */
  @Test(timeout = 4000)
  public void test18()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      assertNotNull(naiveBayesMultinomialText0);
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      
      LovinsStemmer lovinsStemmer0 = new LovinsStemmer();
      assertNotNull(lovinsStemmer0);
      
      naiveBayesMultinomialText0.setStemmer(lovinsStemmer0);
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      
      naiveBayesMultinomialText0.setUseWordFrequencies(true);
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertTrue(naiveBayesMultinomialText0.getUseWordFrequencies());
      
      InputMappedClassifier inputMappedClassifier0 = new InputMappedClassifier();
      assertNotNull(inputMappedClassifier0);
      assertTrue(inputMappedClassifier0.getIgnoreCaseForNames());
      assertEquals("If set to true, classifier may output additional info to the console.", inputMappedClassifier0.debugTipText());
      assertEquals("The base classifier to be used.", inputMappedClassifier0.classifierTipText());
      assertEquals("Trim white space from each end of attribute names and nominal values before matching.", inputMappedClassifier0.trimTipText());
      assertEquals("Ignore case when matching attribute names and nomina values.", inputMappedClassifier0.ignoreCaseForNamesTipText());
      assertEquals("Wrapper classifier that addresses incompatible training and test data by building a mapping between the training data that a classifier has been built with and the incoming test instances' structure. Model attributes that are not found in the incoming instances receive missing values, so do incoming nominal attribute values that the classifier has not seen before. A new classifier can be trained or an existing one loaded from a file.", inputMappedClassifier0.globalInfo());
      assertEquals("Set the path from which to load a model. Loading occurs when the first test instance is received. Environment variables can be used in the supplied path.", inputMappedClassifier0.modelPathTipText());
      assertFalse(inputMappedClassifier0.getSuppressMappingReport());
      assertTrue(inputMappedClassifier0.getTrim());
      assertEquals(0, inputMappedClassifier0.graphType());
      assertFalse(inputMappedClassifier0.getDebug());
      assertEquals("Don't output a report of model-to-input mappings.", inputMappedClassifier0.suppressMappingReportTipText());
      assertEquals("", inputMappedClassifier0.getModelPath());
      
      Capabilities capabilities0 = inputMappedClassifier0.getCapabilities();
      assertNotNull(capabilities0);
      assertTrue(inputMappedClassifier0.getIgnoreCaseForNames());
      assertEquals("If set to true, classifier may output additional info to the console.", inputMappedClassifier0.debugTipText());
      assertEquals("The base classifier to be used.", inputMappedClassifier0.classifierTipText());
      assertEquals("Trim white space from each end of attribute names and nominal values before matching.", inputMappedClassifier0.trimTipText());
      assertEquals("Ignore case when matching attribute names and nomina values.", inputMappedClassifier0.ignoreCaseForNamesTipText());
      assertEquals("Wrapper classifier that addresses incompatible training and test data by building a mapping between the training data that a classifier has been built with and the incoming test instances' structure. Model attributes that are not found in the incoming instances receive missing values, so do incoming nominal attribute values that the classifier has not seen before. A new classifier can be trained or an existing one loaded from a file.", inputMappedClassifier0.globalInfo());
      assertEquals("Set the path from which to load a model. Loading occurs when the first test instance is received. Environment variables can be used in the supplied path.", inputMappedClassifier0.modelPathTipText());
      assertFalse(inputMappedClassifier0.getSuppressMappingReport());
      assertTrue(inputMappedClassifier0.getTrim());
      assertEquals(0, inputMappedClassifier0.graphType());
      assertFalse(inputMappedClassifier0.getDebug());
      assertEquals("Don't output a report of model-to-input mappings.", inputMappedClassifier0.suppressMappingReportTipText());
      assertEquals("", inputMappedClassifier0.getModelPath());
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      assertTrue(capabilities0.hasDependencies());
      
      TestInstances testInstances0 = TestInstances.forCapabilities(capabilities0);
      assertNotNull(testInstances0);
      assertTrue(inputMappedClassifier0.getIgnoreCaseForNames());
      assertEquals("If set to true, classifier may output additional info to the console.", inputMappedClassifier0.debugTipText());
      assertEquals("The base classifier to be used.", inputMappedClassifier0.classifierTipText());
      assertEquals("Trim white space from each end of attribute names and nominal values before matching.", inputMappedClassifier0.trimTipText());
      assertEquals("Ignore case when matching attribute names and nomina values.", inputMappedClassifier0.ignoreCaseForNamesTipText());
      assertEquals("Wrapper classifier that addresses incompatible training and test data by building a mapping between the training data that a classifier has been built with and the incoming test instances' structure. Model attributes that are not found in the incoming instances receive missing values, so do incoming nominal attribute values that the classifier has not seen before. A new classifier can be trained or an existing one loaded from a file.", inputMappedClassifier0.globalInfo());
      assertEquals("Set the path from which to load a model. Loading occurs when the first test instance is received. Environment variables can be used in the supplied path.", inputMappedClassifier0.modelPathTipText());
      assertFalse(inputMappedClassifier0.getSuppressMappingReport());
      assertTrue(inputMappedClassifier0.getTrim());
      assertEquals(0, inputMappedClassifier0.graphType());
      assertFalse(inputMappedClassifier0.getDebug());
      assertEquals("Don't output a report of model-to-input mappings.", inputMappedClassifier0.suppressMappingReportTipText());
      assertEquals("", inputMappedClassifier0.getModelPath());
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      assertTrue(capabilities0.hasDependencies());
      assertEquals(5, testInstances0.getNumAttributes());
      assertEquals(1, testInstances0.getNumString());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(4, testInstances0.getNumClasses());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(20, testInstances0.getNumInstances());
      assertFalse(testInstances0.getNoClass());
      assertEquals(1, testInstances0.getNumRelationalString());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      
      Instances instances0 = testInstances0.generate();
      assertNotNull(instances0);
      assertTrue(inputMappedClassifier0.getIgnoreCaseForNames());
      assertEquals("If set to true, classifier may output additional info to the console.", inputMappedClassifier0.debugTipText());
      assertEquals("The base classifier to be used.", inputMappedClassifier0.classifierTipText());
      assertEquals("Trim white space from each end of attribute names and nominal values before matching.", inputMappedClassifier0.trimTipText());
      assertEquals("Ignore case when matching attribute names and nomina values.", inputMappedClassifier0.ignoreCaseForNamesTipText());
      assertEquals("Wrapper classifier that addresses incompatible training and test data by building a mapping between the training data that a classifier has been built with and the incoming test instances' structure. Model attributes that are not found in the incoming instances receive missing values, so do incoming nominal attribute values that the classifier has not seen before. A new classifier can be trained or an existing one loaded from a file.", inputMappedClassifier0.globalInfo());
      assertEquals("Set the path from which to load a model. Loading occurs when the first test instance is received. Environment variables can be used in the supplied path.", inputMappedClassifier0.modelPathTipText());
      assertFalse(inputMappedClassifier0.getSuppressMappingReport());
      assertTrue(inputMappedClassifier0.getTrim());
      assertEquals(0, inputMappedClassifier0.graphType());
      assertFalse(inputMappedClassifier0.getDebug());
      assertEquals("Don't output a report of model-to-input mappings.", inputMappedClassifier0.suppressMappingReportTipText());
      assertEquals("", inputMappedClassifier0.getModelPath());
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      assertTrue(capabilities0.hasDependencies());
      assertEquals(5, testInstances0.getNumAttributes());
      assertEquals(1, testInstances0.getNumString());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(4, testInstances0.getNumClasses());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(20, testInstances0.getNumInstances());
      assertFalse(testInstances0.getNoClass());
      assertEquals(1, testInstances0.getNumRelationalString());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(4, instances0.classIndex());
      assertEquals(4, instances0.numClasses());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(20, instances0.numInstances());
      assertTrue(instances0.checkForStringAttributes());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(20, instances0.size());
      assertEquals(5, instances0.numAttributes());
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      
      naiveBayesMultinomialText0.buildClassifier(instances0);
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertTrue(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertTrue(inputMappedClassifier0.getIgnoreCaseForNames());
      assertEquals("If set to true, classifier may output additional info to the console.", inputMappedClassifier0.debugTipText());
      assertEquals("The base classifier to be used.", inputMappedClassifier0.classifierTipText());
      assertEquals("Trim white space from each end of attribute names and nominal values before matching.", inputMappedClassifier0.trimTipText());
      assertEquals("Ignore case when matching attribute names and nomina values.", inputMappedClassifier0.ignoreCaseForNamesTipText());
      assertEquals("Wrapper classifier that addresses incompatible training and test data by building a mapping between the training data that a classifier has been built with and the incoming test instances' structure. Model attributes that are not found in the incoming instances receive missing values, so do incoming nominal attribute values that the classifier has not seen before. A new classifier can be trained or an existing one loaded from a file.", inputMappedClassifier0.globalInfo());
      assertEquals("Set the path from which to load a model. Loading occurs when the first test instance is received. Environment variables can be used in the supplied path.", inputMappedClassifier0.modelPathTipText());
      assertFalse(inputMappedClassifier0.getSuppressMappingReport());
      assertTrue(inputMappedClassifier0.getTrim());
      assertEquals(0, inputMappedClassifier0.graphType());
      assertFalse(inputMappedClassifier0.getDebug());
      assertEquals("Don't output a report of model-to-input mappings.", inputMappedClassifier0.suppressMappingReportTipText());
      assertEquals("", inputMappedClassifier0.getModelPath());
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      assertTrue(capabilities0.hasDependencies());
      assertEquals(5, testInstances0.getNumAttributes());
      assertEquals(1, testInstances0.getNumString());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(4, testInstances0.getNumClasses());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(20, testInstances0.getNumInstances());
      assertFalse(testInstances0.getNoClass());
      assertEquals(1, testInstances0.getNumRelationalString());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(4, instances0.classIndex());
      assertEquals(4, instances0.numClasses());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(20, instances0.numInstances());
      assertTrue(instances0.checkForStringAttributes());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(20, instances0.size());
      assertEquals(5, instances0.numAttributes());
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      
      String string0 = naiveBayesMultinomialText0.toString();
      assertNotNull(string0);
      assertEquals("The independent probability of a class\n--------------------------------------\nclass1\t9.0\nclass2\t4.0\nclass3\t7.0\nclass4\t4.0\n\nThe probability of a word given the class\n-----------------------------------------\n\tclass1\tclass2\tclass3\tclass4\t\nover\t2.718281828459045\t2.718281828459045\t22026.465794806718\t22026.465794806718\t\nquick\t22026.465794806718\t2.718281828459045\t22026.465794806718\t22026.465794806718\t\nlaz\t22026.465794806718\t2.718281828459045\t22026.465794806718\t2.718281828459045\t\nth\t1.7848230096318725E8\t22026.465794806718\t22026.465794806718\t2.718281828459045\t\nbrown\t22026.465794806718\t22026.465794806718\t2.718281828459045\t2.718281828459045\t\ndog\t22026.465794806718\t2.718281828459045\t22026.465794806718\t2.718281828459045\t\nfox\t2.718281828459045\t2.718281828459045\t22026.465794806718\t22026.465794806718\t\njump\t1.7848230096318725E8\t22026.465794806718\t2.718281828459045\t2.718281828459045\t\n", string0);
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertTrue(naiveBayesMultinomialText0.getUseWordFrequencies());
      
      naiveBayesMultinomialText0.pruneDictionary();
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertTrue(naiveBayesMultinomialText0.getUseWordFrequencies());
      
      String[] stringArray0 = naiveBayesMultinomialText0.getOptions();
      assertNotNull(stringArray0);
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertTrue(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals(13, stringArray0.length);
  }

  /**
  //Test case number: 19
  /*Coverage entropy=2.6390573296152584
  */
  @Test(timeout = 4000)
  public void test19()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      assertNotNull(naiveBayesMultinomialText0);
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      
      MockFile mockFile0 = new MockFile("2{r)dt|", "2{r)dt|");
      assertNotNull(mockFile0);
      
      naiveBayesMultinomialText0.setStopwords(mockFile0);
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertFalse(mockFile0.exists());
      assertEquals(0L, mockFile0.getUsableSpace());
      assertEquals("/home/ubuntu/evosuite_readability_gen/projects/107_weka/2{r)dt|/2{r)dt|", mockFile0.toString());
      assertFalse(mockFile0.isHidden());
      assertEquals(0L, mockFile0.length());
      assertFalse(mockFile0.isDirectory());
      assertEquals(0L, mockFile0.lastModified());
      assertEquals(0L, mockFile0.getFreeSpace());
      assertFalse(mockFile0.canWrite());
      assertTrue(mockFile0.isAbsolute());
      assertFalse(mockFile0.canRead());
      assertEquals("/home/ubuntu/evosuite_readability_gen/projects/107_weka/2{r)dt|", mockFile0.getParent());
      assertEquals(0L, mockFile0.getTotalSpace());
      assertTrue(mockFile0.isFile());
      assertFalse(mockFile0.canExecute());
      assertEquals("2{r)dt|", mockFile0.getName());
      
      String[] stringArray0 = naiveBayesMultinomialText0.getOptions();
      assertNotNull(stringArray0);
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(14, stringArray0.length);
  }

  /**
  //Test case number: 20
  /*Coverage entropy=0.6931471805599453
  */
  @Test(timeout = 4000)
  public void test20()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      assertNotNull(naiveBayesMultinomialText0);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      
      String string0 = naiveBayesMultinomialText0.useStopListTipText();
      assertNotNull(string0);
      assertEquals("If true, ignores all words that are on the stoplist.", string0);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
  }

  /**
  //Test case number: 21
  /*Coverage entropy=0.6931471805599453
  */
  @Test(timeout = 4000)
  public void test21()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      assertNotNull(naiveBayesMultinomialText0);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      
      String string0 = naiveBayesMultinomialText0.getRevision();
      assertNotNull(string0);
      assertEquals("9122", string0);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
  }

  /**
  //Test case number: 22
  /*Coverage entropy=0.6931471805599453
  */
  @Test(timeout = 4000)
  public void test22()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      assertNotNull(naiveBayesMultinomialText0);
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      
      String string0 = naiveBayesMultinomialText0.normTipText();
      assertNotNull(string0);
      assertEquals("The norm of the instances after normalization.", string0);
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
  }

  /**
  //Test case number: 23
  /*Coverage entropy=1.9594500048969377
  */
  @Test(timeout = 4000)
  public void test23()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      assertNotNull(naiveBayesMultinomialText0);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      
      LovinsStemmer lovinsStemmer0 = new LovinsStemmer();
      assertNotNull(lovinsStemmer0);
      
      naiveBayesMultinomialText0.setStemmer(lovinsStemmer0);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      
      naiveBayesMultinomialText0.setUseWordFrequencies(true);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertTrue(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      
      InputMappedClassifier inputMappedClassifier0 = new InputMappedClassifier();
      assertNotNull(inputMappedClassifier0);
      assertTrue(inputMappedClassifier0.getIgnoreCaseForNames());
      assertEquals("Trim white space from each end of attribute names and nominal values before matching.", inputMappedClassifier0.trimTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", inputMappedClassifier0.debugTipText());
      assertEquals("Ignore case when matching attribute names and nomina values.", inputMappedClassifier0.ignoreCaseForNamesTipText());
      assertEquals("The base classifier to be used.", inputMappedClassifier0.classifierTipText());
      assertFalse(inputMappedClassifier0.getSuppressMappingReport());
      assertEquals("Wrapper classifier that addresses incompatible training and test data by building a mapping between the training data that a classifier has been built with and the incoming test instances' structure. Model attributes that are not found in the incoming instances receive missing values, so do incoming nominal attribute values that the classifier has not seen before. A new classifier can be trained or an existing one loaded from a file.", inputMappedClassifier0.globalInfo());
      assertFalse(inputMappedClassifier0.getDebug());
      assertTrue(inputMappedClassifier0.getTrim());
      assertEquals("", inputMappedClassifier0.getModelPath());
      assertEquals("Set the path from which to load a model. Loading occurs when the first test instance is received. Environment variables can be used in the supplied path.", inputMappedClassifier0.modelPathTipText());
      assertEquals("Don't output a report of model-to-input mappings.", inputMappedClassifier0.suppressMappingReportTipText());
      assertEquals(0, inputMappedClassifier0.graphType());
      
      Capabilities capabilities0 = inputMappedClassifier0.getCapabilities();
      assertNotNull(capabilities0);
      assertTrue(inputMappedClassifier0.getIgnoreCaseForNames());
      assertEquals("Trim white space from each end of attribute names and nominal values before matching.", inputMappedClassifier0.trimTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", inputMappedClassifier0.debugTipText());
      assertEquals("Ignore case when matching attribute names and nomina values.", inputMappedClassifier0.ignoreCaseForNamesTipText());
      assertEquals("The base classifier to be used.", inputMappedClassifier0.classifierTipText());
      assertFalse(inputMappedClassifier0.getSuppressMappingReport());
      assertEquals("Wrapper classifier that addresses incompatible training and test data by building a mapping between the training data that a classifier has been built with and the incoming test instances' structure. Model attributes that are not found in the incoming instances receive missing values, so do incoming nominal attribute values that the classifier has not seen before. A new classifier can be trained or an existing one loaded from a file.", inputMappedClassifier0.globalInfo());
      assertFalse(inputMappedClassifier0.getDebug());
      assertTrue(inputMappedClassifier0.getTrim());
      assertEquals("", inputMappedClassifier0.getModelPath());
      assertEquals("Set the path from which to load a model. Loading occurs when the first test instance is received. Environment variables can be used in the supplied path.", inputMappedClassifier0.modelPathTipText());
      assertEquals("Don't output a report of model-to-input mappings.", inputMappedClassifier0.suppressMappingReportTipText());
      assertEquals(0, inputMappedClassifier0.graphType());
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      assertTrue(capabilities0.hasDependencies());
      
      TestInstances testInstances0 = TestInstances.forCapabilities(capabilities0);
      assertNotNull(testInstances0);
      assertTrue(inputMappedClassifier0.getIgnoreCaseForNames());
      assertEquals("Trim white space from each end of attribute names and nominal values before matching.", inputMappedClassifier0.trimTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", inputMappedClassifier0.debugTipText());
      assertEquals("Ignore case when matching attribute names and nomina values.", inputMappedClassifier0.ignoreCaseForNamesTipText());
      assertEquals("The base classifier to be used.", inputMappedClassifier0.classifierTipText());
      assertFalse(inputMappedClassifier0.getSuppressMappingReport());
      assertEquals("Wrapper classifier that addresses incompatible training and test data by building a mapping between the training data that a classifier has been built with and the incoming test instances' structure. Model attributes that are not found in the incoming instances receive missing values, so do incoming nominal attribute values that the classifier has not seen before. A new classifier can be trained or an existing one loaded from a file.", inputMappedClassifier0.globalInfo());
      assertFalse(inputMappedClassifier0.getDebug());
      assertTrue(inputMappedClassifier0.getTrim());
      assertEquals("", inputMappedClassifier0.getModelPath());
      assertEquals("Set the path from which to load a model. Loading occurs when the first test instance is received. Environment variables can be used in the supplied path.", inputMappedClassifier0.modelPathTipText());
      assertEquals("Don't output a report of model-to-input mappings.", inputMappedClassifier0.suppressMappingReportTipText());
      assertEquals(0, inputMappedClassifier0.graphType());
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      assertTrue(capabilities0.hasDependencies());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(1, testInstances0.getSeed());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getNumString());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(5, testInstances0.getNumAttributes());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(1, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(20, testInstances0.getNumInstances());
      assertFalse(testInstances0.getNoClass());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(4, testInstances0.getNumClasses());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      
      Instances instances0 = testInstances0.generate();
      assertNotNull(instances0);
      assertTrue(inputMappedClassifier0.getIgnoreCaseForNames());
      assertEquals("Trim white space from each end of attribute names and nominal values before matching.", inputMappedClassifier0.trimTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", inputMappedClassifier0.debugTipText());
      assertEquals("Ignore case when matching attribute names and nomina values.", inputMappedClassifier0.ignoreCaseForNamesTipText());
      assertEquals("The base classifier to be used.", inputMappedClassifier0.classifierTipText());
      assertFalse(inputMappedClassifier0.getSuppressMappingReport());
      assertEquals("Wrapper classifier that addresses incompatible training and test data by building a mapping between the training data that a classifier has been built with and the incoming test instances' structure. Model attributes that are not found in the incoming instances receive missing values, so do incoming nominal attribute values that the classifier has not seen before. A new classifier can be trained or an existing one loaded from a file.", inputMappedClassifier0.globalInfo());
      assertFalse(inputMappedClassifier0.getDebug());
      assertTrue(inputMappedClassifier0.getTrim());
      assertEquals("", inputMappedClassifier0.getModelPath());
      assertEquals("Set the path from which to load a model. Loading occurs when the first test instance is received. Environment variables can be used in the supplied path.", inputMappedClassifier0.modelPathTipText());
      assertEquals("Don't output a report of model-to-input mappings.", inputMappedClassifier0.suppressMappingReportTipText());
      assertEquals(0, inputMappedClassifier0.graphType());
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      assertTrue(capabilities0.hasDependencies());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(1, testInstances0.getSeed());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getNumString());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(5, testInstances0.getNumAttributes());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(1, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(20, testInstances0.getNumInstances());
      assertFalse(testInstances0.getNoClass());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(4, testInstances0.getNumClasses());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals("Testdata", instances0.relationName());
      assertEquals(4, instances0.numClasses());
      assertTrue(instances0.checkForStringAttributes());
      assertEquals(5, instances0.numAttributes());
      assertEquals(4, instances0.classIndex());
      assertEquals(20, instances0.size());
      assertEquals(20, instances0.numInstances());
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      
      naiveBayesMultinomialText0.buildClassifier(instances0);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertTrue(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertTrue(inputMappedClassifier0.getIgnoreCaseForNames());
      assertEquals("Trim white space from each end of attribute names and nominal values before matching.", inputMappedClassifier0.trimTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", inputMappedClassifier0.debugTipText());
      assertEquals("Ignore case when matching attribute names and nomina values.", inputMappedClassifier0.ignoreCaseForNamesTipText());
      assertEquals("The base classifier to be used.", inputMappedClassifier0.classifierTipText());
      assertFalse(inputMappedClassifier0.getSuppressMappingReport());
      assertEquals("Wrapper classifier that addresses incompatible training and test data by building a mapping between the training data that a classifier has been built with and the incoming test instances' structure. Model attributes that are not found in the incoming instances receive missing values, so do incoming nominal attribute values that the classifier has not seen before. A new classifier can be trained or an existing one loaded from a file.", inputMappedClassifier0.globalInfo());
      assertFalse(inputMappedClassifier0.getDebug());
      assertTrue(inputMappedClassifier0.getTrim());
      assertEquals("", inputMappedClassifier0.getModelPath());
      assertEquals("Set the path from which to load a model. Loading occurs when the first test instance is received. Environment variables can be used in the supplied path.", inputMappedClassifier0.modelPathTipText());
      assertEquals("Don't output a report of model-to-input mappings.", inputMappedClassifier0.suppressMappingReportTipText());
      assertEquals(0, inputMappedClassifier0.graphType());
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      assertTrue(capabilities0.hasDependencies());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(1, testInstances0.getSeed());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getNumString());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(5, testInstances0.getNumAttributes());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(1, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(20, testInstances0.getNumInstances());
      assertFalse(testInstances0.getNoClass());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(4, testInstances0.getNumClasses());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals("Testdata", instances0.relationName());
      assertEquals(4, instances0.numClasses());
      assertTrue(instances0.checkForStringAttributes());
      assertEquals(5, instances0.numAttributes());
      assertEquals(4, instances0.classIndex());
      assertEquals(20, instances0.size());
      assertEquals(20, instances0.numInstances());
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      
      String string0 = naiveBayesMultinomialText0.toString();
      assertNotNull(string0);
      assertEquals("The independent probability of a class\n--------------------------------------\nclass1\t9.0\nclass2\t4.0\nclass3\t7.0\nclass4\t4.0\n\nThe probability of a word given the class\n-----------------------------------------\n\tclass1\tclass2\tclass3\tclass4\t\nover\t2.718281828459045\t2.718281828459045\t22026.465794806718\t22026.465794806718\t\nquick\t22026.465794806718\t2.718281828459045\t22026.465794806718\t22026.465794806718\t\nlaz\t22026.465794806718\t2.718281828459045\t22026.465794806718\t2.718281828459045\t\nth\t1.7848230096318725E8\t22026.465794806718\t22026.465794806718\t2.718281828459045\t\nbrown\t22026.465794806718\t22026.465794806718\t2.718281828459045\t2.718281828459045\t\ndog\t22026.465794806718\t2.718281828459045\t22026.465794806718\t2.718281828459045\t\nfox\t2.718281828459045\t2.718281828459045\t22026.465794806718\t22026.465794806718\t\njump\t1.7848230096318725E8\t22026.465794806718\t2.718281828459045\t2.718281828459045\t\n", string0);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertTrue(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      
      naiveBayesMultinomialText0.pruneDictionary();
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertTrue(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      
      naiveBayesMultinomialText0.setOptions(testInstances0.DEFAULT_WORDS);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertTrue(inputMappedClassifier0.getIgnoreCaseForNames());
      assertEquals("Trim white space from each end of attribute names and nominal values before matching.", inputMappedClassifier0.trimTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", inputMappedClassifier0.debugTipText());
      assertEquals("Ignore case when matching attribute names and nomina values.", inputMappedClassifier0.ignoreCaseForNamesTipText());
      assertEquals("The base classifier to be used.", inputMappedClassifier0.classifierTipText());
      assertFalse(inputMappedClassifier0.getSuppressMappingReport());
      assertEquals("Wrapper classifier that addresses incompatible training and test data by building a mapping between the training data that a classifier has been built with and the incoming test instances' structure. Model attributes that are not found in the incoming instances receive missing values, so do incoming nominal attribute values that the classifier has not seen before. A new classifier can be trained or an existing one loaded from a file.", inputMappedClassifier0.globalInfo());
      assertFalse(inputMappedClassifier0.getDebug());
      assertTrue(inputMappedClassifier0.getTrim());
      assertEquals("", inputMappedClassifier0.getModelPath());
      assertEquals("Set the path from which to load a model. Loading occurs when the first test instance is received. Environment variables can be used in the supplied path.", inputMappedClassifier0.modelPathTipText());
      assertEquals("Don't output a report of model-to-input mappings.", inputMappedClassifier0.suppressMappingReportTipText());
      assertEquals(0, inputMappedClassifier0.graphType());
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      assertTrue(capabilities0.hasDependencies());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(1, testInstances0.getSeed());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getNumString());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(5, testInstances0.getNumAttributes());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(1, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(20, testInstances0.getNumInstances());
      assertFalse(testInstances0.getNoClass());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(4, testInstances0.getNumClasses());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
  }

  /**
  //Test case number: 24
  /*Coverage entropy=2.5649493574615376
  */
  @Test(timeout = 4000)
  public void test24()  throws Throwable  {
      String string0 = "7z_:&B',WpyVu";
      SystemInUtil.addInputLine("7z_:&B',WpyVu");
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      assertNotNull(naiveBayesMultinomialText0);
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      
      EvoSuiteFile evoSuiteFile0 = null;
      boolean boolean0 = FileSystemHandling.shouldThrowIOException((EvoSuiteFile) null);
      assertFalse(boolean0);
      
      naiveBayesMultinomialText0.m_lowercaseTokens = true;
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertTrue(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      
      String[] stringArray0 = naiveBayesMultinomialText0.getOptions();
      assertNotNull(stringArray0);
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertTrue(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals(13, stringArray0.length);
      
      TestInstances testInstances0 = new TestInstances();
      assertNotNull(testInstances0);
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(20, testInstances0.getNumInstances());
      assertFalse(testInstances0.getNoClass());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      
      // Undeclared exception!
      try { 
        testInstances0.getRelationalFormat(698);
        fail("Expecting exception: ArrayIndexOutOfBoundsException");
      
      } catch(ArrayIndexOutOfBoundsException e) {
         //
         // 698
         //
         verifyException("weka.core.TestInstances", e);
      }
  }

  /**
  //Test case number: 25
  /*Coverage entropy=2.5649493574615376
  */
  @Test(timeout = 4000)
  public void test25()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      assertNotNull(naiveBayesMultinomialText0);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      
      naiveBayesMultinomialText0.m_wordFrequencies = true;
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertTrue(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      
      naiveBayesMultinomialText0.m_normalize = true;
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertTrue(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertTrue(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      
      Random.setNextRandom((-795));
      String[] stringArray0 = naiveBayesMultinomialText0.getOptions();
      assertNotNull(stringArray0);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertTrue(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertTrue(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals(14, stringArray0.length);
  }

  /**
  //Test case number: 26
  /*Coverage entropy=2.520902686420625
  */
  @Test(timeout = 4000)
  public void test26()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      assertNotNull(naiveBayesMultinomialText0);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      
      naiveBayesMultinomialText0.m_useStopList = true;
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertTrue(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      
      double[] doubleArray0 = new double[1];
      WordTokenizer wordTokenizer0 = new WordTokenizer();
      assertNotNull(wordTokenizer0);
      assertEquals(" \r\n\t.,;:'\"()?!", wordTokenizer0.getDelimiters());
      assertEquals("Set of delimiter characters to use in tokenizing (\\r, \\n and \\t can be used for carriage-return, line-feed and tab)", wordTokenizer0.delimitersTipText());
      assertEquals("A simple tokenizer that is using the java.util.StringTokenizer class to tokenize the strings.", wordTokenizer0.globalInfo());
      
      String[] stringArray0 = naiveBayesMultinomialText0.getOptions();
      assertNotNull(stringArray0);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertTrue(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(13, stringArray0.length);
      
      naiveBayesMultinomialText0.setTokenizer(wordTokenizer0);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertTrue(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(" \r\n\t.,;:'\"()?!", wordTokenizer0.getDelimiters());
      assertEquals("Set of delimiter characters to use in tokenizing (\\r, \\n and \\t can be used for carriage-return, line-feed and tab)", wordTokenizer0.delimitersTipText());
      assertEquals("A simple tokenizer that is using the java.util.StringTokenizer class to tokenize the strings.", wordTokenizer0.globalInfo());
      
      naiveBayesMultinomialText0.m_inputVector = null;
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertTrue(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      
      // Undeclared exception!
      try { 
        naiveBayesMultinomialText0.tokenizeInstance((Instance) null, true);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("weka.classifiers.bayes.NaiveBayesMultinomialText", e);
      }
  }

  /**
  //Test case number: 27
  /*Coverage entropy=1.6094379124341005
  */
  @Test(timeout = 4000)
  public void test27()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      assertNotNull(naiveBayesMultinomialText0);
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      
      Instance instance0 = null;
      int int0 = 815;
      Classifier[] classifierArray0 = AbstractClassifier.makeCopies(naiveBayesMultinomialText0, 815);
      assertNotNull(classifierArray0);
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(815, classifierArray0.length);
      
      SystemInUtil.addInputLine("~t[ Tx<+MP_iXH<n=c}");
      boolean boolean0 = FileSystemHandling.appendLineToFile((EvoSuiteFile) null, "$Revision: 9122 $");
      assertFalse(boolean0);
      
      File file0 = MockFile.createTempFile("$Revision: 9122 $", "XJi*qyW!t");
      assertNotNull(file0);
      assertTrue(file0.isAbsolute());
      assertEquals("/tmp", file0.getParent());
      assertTrue(file0.canExecute());
      assertEquals(0L, file0.getTotalSpace());
      assertTrue(file0.exists());
      assertEquals(1392409281320L, file0.lastModified());
      assertTrue(file0.isFile());
      assertTrue(file0.canRead());
      assertEquals("$Revision: 9122 $0XJi*qyW!t", file0.getName());
      assertEquals(0L, file0.getUsableSpace());
      assertFalse(file0.isHidden());
      assertEquals("/tmp/$Revision: 9122 $0XJi*qyW!t", file0.toString());
      assertEquals(0L, file0.length());
      assertFalse(file0.isDirectory());
      assertTrue(file0.canWrite());
      assertEquals(0L, file0.getFreeSpace());
      
      naiveBayesMultinomialText0.setStopwords(file0);
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertTrue(naiveBayesMultinomialText0.getUseStopList());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertTrue(file0.isAbsolute());
      assertEquals("/tmp", file0.getParent());
      assertTrue(file0.canExecute());
      assertEquals(0L, file0.getTotalSpace());
      assertTrue(file0.exists());
      assertEquals(1392409281320L, file0.lastModified());
      assertTrue(file0.isFile());
      assertTrue(file0.canRead());
      assertEquals("$Revision: 9122 $0XJi*qyW!t", file0.getName());
      assertEquals(0L, file0.getUsableSpace());
      assertFalse(file0.isHidden());
      assertEquals("/tmp/$Revision: 9122 $0XJi*qyW!t", file0.toString());
      assertEquals(0L, file0.length());
      assertFalse(file0.isDirectory());
      assertTrue(file0.canWrite());
      assertEquals(0L, file0.getFreeSpace());
      
      boolean boolean1 = false;
      // Undeclared exception!
      try { 
        naiveBayesMultinomialText0.tokenizeInstance((Instance) null, false);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("weka.classifiers.bayes.NaiveBayesMultinomialText", e);
      }
  }

  /**
  //Test case number: 28
  /*Coverage entropy=1.825333102396375
  */
  @Test(timeout = 4000)
  public void test28()  throws Throwable  {
      AlphabeticTokenizer alphabeticTokenizer0 = new AlphabeticTokenizer();
      assertNotNull(alphabeticTokenizer0);
      assertEquals("Alphabetic string tokenizer, tokens are to be formed only from contiguous alphabetic sequences.", alphabeticTokenizer0.globalInfo());
      
      EvoSuiteFile evoSuiteFile0 = new EvoSuiteFile("/home/ubuntu/wekafiles/props");
      boolean boolean0 = FileSystemHandling.shouldThrowIOException(evoSuiteFile0);
      assertTrue(boolean0);
      
      TestInstances testInstances0 = new TestInstances();
      assertNotNull(testInstances0);
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getSeed());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(0, testInstances0.getNumString());
      assertFalse(testInstances0.getNoClass());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      
      Instances instances0 = testInstances0.generate(" ");
      assertNotNull(instances0);
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getSeed());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(0, testInstances0.getNumString());
      assertFalse(testInstances0.getNoClass());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(2, instances0.numAttributes());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals("Testdata", instances0.relationName());
      assertEquals(2, instances0.numClasses());
      assertEquals(20, instances0.size());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20, instances0.numInstances());
      assertEquals(1, instances0.classIndex());
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      assertNotNull(naiveBayesMultinomialText0);
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      
      naiveBayesMultinomialText0.buildClassifier(instances0);
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getSeed());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(0, testInstances0.getNumString());
      assertFalse(testInstances0.getNoClass());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(2, instances0.numAttributes());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals("Testdata", instances0.relationName());
      assertEquals(2, instances0.numClasses());
      assertEquals(20, instances0.size());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20, instances0.numInstances());
      assertEquals(1, instances0.classIndex());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      
      WordTokenizer wordTokenizer0 = (WordTokenizer)naiveBayesMultinomialText0.getTokenizer();
      assertNotNull(wordTokenizer0);
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals(" \r\n\t.,;:'\"()?!", wordTokenizer0.getDelimiters());
      assertEquals("A simple tokenizer that is using the java.util.StringTokenizer class to tokenize the strings.", wordTokenizer0.globalInfo());
      assertEquals("Set of delimiter characters to use in tokenizing (\\r, \\n and \\t can be used for carriage-return, line-feed and tab)", wordTokenizer0.delimitersTipText());
      
      testInstances0.setNumRelationalDate((-1662));
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getSeed());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(0, testInstances0.getNumString());
      assertFalse(testInstances0.getNoClass());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals((-1662), testInstances0.getNumRelationalDate());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      
      int[] intArray0 = new int[3];
      intArray0[0] = 41;
      intArray0[2] = 2566;
      BinarySparseInstance binarySparseInstance0 = new BinarySparseInstance(1666.0, intArray0, (-2));
      assertNotNull(binarySparseInstance0);
      assertArrayEquals(new int[] {41, 0, 2566}, intArray0);
      assertEquals((-2), binarySparseInstance0.numAttributes());
      assertEquals(1666.0, binarySparseInstance0.weight(), 0.01);
      assertEquals(3, binarySparseInstance0.numValues());
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertEquals(3, intArray0.length);
      
      boolean boolean1 = FileSystemHandling.appendStringToFile(evoSuiteFile0, " ");
      assertTrue(boolean1);
      assertTrue(boolean1 == boolean0);
      
      double[] doubleArray0 = naiveBayesMultinomialText0.distributionForInstance(binarySparseInstance0);
      assertNotNull(doubleArray0);
      assertArrayEquals(new int[] {41, 0, 2566}, intArray0);
      assertArrayEquals(new double[] {0.5454545454545454, 0.4545454545454546}, doubleArray0, 0.01);
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals((-2), binarySparseInstance0.numAttributes());
      assertEquals(1666.0, binarySparseInstance0.weight(), 0.01);
      assertEquals(3, binarySparseInstance0.numValues());
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertEquals(3, intArray0.length);
      assertEquals(2, doubleArray0.length);
      
      BallTree ballTree0 = new BallTree();
      assertNotNull(ballTree0);
      assertEquals("The tree constructor being used.", ballTree0.ballTreeConstructorTipText());
      assertFalse(ballTree0.getMeasurePerformance());
      assertEquals(0.0, ballTree0.measureTreeSize(), 0.01);
      assertEquals("Whether to calculate performance statistics for the NN search or not", ballTree0.measurePerformanceTipText());
      assertEquals("The distance function to use for finding neighbours (default: weka.core.EuclideanDistance). ", ballTree0.distanceFunctionTipText());
      assertEquals(0.0, ballTree0.measureNumLeaves(), 0.01);
      assertEquals(0.0, ballTree0.measureMaxDepth(), 0.01);
      
      try { 
        ballTree0.nearestNeighbour(binarySparseInstance0);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("weka.core.neighboursearch.BallTree", e);
      }
  }

  /**
  //Test case number: 29
  /*Coverage entropy=1.6672961803613524
  */
  @Test(timeout = 4000)
  public void test29()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      assertNotNull(naiveBayesMultinomialText0);
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      
      LovinsStemmer lovinsStemmer0 = new LovinsStemmer();
      assertNotNull(lovinsStemmer0);
      
      naiveBayesMultinomialText0.setStemmer(lovinsStemmer0);
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      
      naiveBayesMultinomialText0.setUseWordFrequencies(true);
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertTrue(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      
      naiveBayesMultinomialText0.setPeriodicPruning(9);
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals(9, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertTrue(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      
      InputMappedClassifier inputMappedClassifier0 = new InputMappedClassifier();
      assertNotNull(inputMappedClassifier0);
      assertEquals("Ignore case when matching attribute names and nomina values.", inputMappedClassifier0.ignoreCaseForNamesTipText());
      assertTrue(inputMappedClassifier0.getTrim());
      assertEquals("", inputMappedClassifier0.getModelPath());
      assertEquals("Set the path from which to load a model. Loading occurs when the first test instance is received. Environment variables can be used in the supplied path.", inputMappedClassifier0.modelPathTipText());
      assertEquals("Trim white space from each end of attribute names and nominal values before matching.", inputMappedClassifier0.trimTipText());
      assertEquals(0, inputMappedClassifier0.graphType());
      assertEquals("Don't output a report of model-to-input mappings.", inputMappedClassifier0.suppressMappingReportTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", inputMappedClassifier0.debugTipText());
      assertFalse(inputMappedClassifier0.getSuppressMappingReport());
      assertEquals("Wrapper classifier that addresses incompatible training and test data by building a mapping between the training data that a classifier has been built with and the incoming test instances' structure. Model attributes that are not found in the incoming instances receive missing values, so do incoming nominal attribute values that the classifier has not seen before. A new classifier can be trained or an existing one loaded from a file.", inputMappedClassifier0.globalInfo());
      assertTrue(inputMappedClassifier0.getIgnoreCaseForNames());
      assertEquals("The base classifier to be used.", inputMappedClassifier0.classifierTipText());
      assertFalse(inputMappedClassifier0.getDebug());
      
      Capabilities capabilities0 = inputMappedClassifier0.getCapabilities();
      assertNotNull(capabilities0);
      assertEquals("Ignore case when matching attribute names and nomina values.", inputMappedClassifier0.ignoreCaseForNamesTipText());
      assertTrue(inputMappedClassifier0.getTrim());
      assertEquals("", inputMappedClassifier0.getModelPath());
      assertEquals("Set the path from which to load a model. Loading occurs when the first test instance is received. Environment variables can be used in the supplied path.", inputMappedClassifier0.modelPathTipText());
      assertEquals("Trim white space from each end of attribute names and nominal values before matching.", inputMappedClassifier0.trimTipText());
      assertEquals(0, inputMappedClassifier0.graphType());
      assertEquals("Don't output a report of model-to-input mappings.", inputMappedClassifier0.suppressMappingReportTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", inputMappedClassifier0.debugTipText());
      assertFalse(inputMappedClassifier0.getSuppressMappingReport());
      assertEquals("Wrapper classifier that addresses incompatible training and test data by building a mapping between the training data that a classifier has been built with and the incoming test instances' structure. Model attributes that are not found in the incoming instances receive missing values, so do incoming nominal attribute values that the classifier has not seen before. A new classifier can be trained or an existing one loaded from a file.", inputMappedClassifier0.globalInfo());
      assertTrue(inputMappedClassifier0.getIgnoreCaseForNames());
      assertEquals("The base classifier to be used.", inputMappedClassifier0.classifierTipText());
      assertFalse(inputMappedClassifier0.getDebug());
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      assertTrue(capabilities0.hasDependencies());
      
      TestInstances testInstances0 = TestInstances.forCapabilities(capabilities0);
      assertNotNull(testInstances0);
      assertEquals("Ignore case when matching attribute names and nomina values.", inputMappedClassifier0.ignoreCaseForNamesTipText());
      assertTrue(inputMappedClassifier0.getTrim());
      assertEquals("", inputMappedClassifier0.getModelPath());
      assertEquals("Set the path from which to load a model. Loading occurs when the first test instance is received. Environment variables can be used in the supplied path.", inputMappedClassifier0.modelPathTipText());
      assertEquals("Trim white space from each end of attribute names and nominal values before matching.", inputMappedClassifier0.trimTipText());
      assertEquals(0, inputMappedClassifier0.graphType());
      assertEquals("Don't output a report of model-to-input mappings.", inputMappedClassifier0.suppressMappingReportTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", inputMappedClassifier0.debugTipText());
      assertFalse(inputMappedClassifier0.getSuppressMappingReport());
      assertEquals("Wrapper classifier that addresses incompatible training and test data by building a mapping between the training data that a classifier has been built with and the incoming test instances' structure. Model attributes that are not found in the incoming instances receive missing values, so do incoming nominal attribute values that the classifier has not seen before. A new classifier can be trained or an existing one loaded from a file.", inputMappedClassifier0.globalInfo());
      assertTrue(inputMappedClassifier0.getIgnoreCaseForNames());
      assertEquals("The base classifier to be used.", inputMappedClassifier0.classifierTipText());
      assertFalse(inputMappedClassifier0.getDebug());
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      assertTrue(capabilities0.hasDependencies());
      assertEquals(1, testInstances0.getNumString());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getSeed());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(5, testInstances0.getNumAttributes());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(4, testInstances0.getNumClasses());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(20, testInstances0.getNumInstances());
      assertFalse(testInstances0.getNoClass());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(1, testInstances0.getNumRelationalString());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      
      Instances instances0 = testInstances0.generate();
      assertNotNull(instances0);
      assertEquals("Ignore case when matching attribute names and nomina values.", inputMappedClassifier0.ignoreCaseForNamesTipText());
      assertTrue(inputMappedClassifier0.getTrim());
      assertEquals("", inputMappedClassifier0.getModelPath());
      assertEquals("Set the path from which to load a model. Loading occurs when the first test instance is received. Environment variables can be used in the supplied path.", inputMappedClassifier0.modelPathTipText());
      assertEquals("Trim white space from each end of attribute names and nominal values before matching.", inputMappedClassifier0.trimTipText());
      assertEquals(0, inputMappedClassifier0.graphType());
      assertEquals("Don't output a report of model-to-input mappings.", inputMappedClassifier0.suppressMappingReportTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", inputMappedClassifier0.debugTipText());
      assertFalse(inputMappedClassifier0.getSuppressMappingReport());
      assertEquals("Wrapper classifier that addresses incompatible training and test data by building a mapping between the training data that a classifier has been built with and the incoming test instances' structure. Model attributes that are not found in the incoming instances receive missing values, so do incoming nominal attribute values that the classifier has not seen before. A new classifier can be trained or an existing one loaded from a file.", inputMappedClassifier0.globalInfo());
      assertTrue(inputMappedClassifier0.getIgnoreCaseForNames());
      assertEquals("The base classifier to be used.", inputMappedClassifier0.classifierTipText());
      assertFalse(inputMappedClassifier0.getDebug());
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      assertTrue(capabilities0.hasDependencies());
      assertEquals(1, testInstances0.getNumString());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getSeed());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(5, testInstances0.getNumAttributes());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(4, testInstances0.getNumClasses());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(20, testInstances0.getNumInstances());
      assertFalse(testInstances0.getNoClass());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(1, testInstances0.getNumRelationalString());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(5, instances0.numAttributes());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(20, instances0.size());
      assertEquals(20, instances0.numInstances());
      assertEquals(4, instances0.classIndex());
      assertEquals(4, instances0.numClasses());
      assertTrue(instances0.checkForStringAttributes());
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      
      naiveBayesMultinomialText0.buildClassifier(instances0);
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals(9, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertTrue(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Ignore case when matching attribute names and nomina values.", inputMappedClassifier0.ignoreCaseForNamesTipText());
      assertTrue(inputMappedClassifier0.getTrim());
      assertEquals("", inputMappedClassifier0.getModelPath());
      assertEquals("Set the path from which to load a model. Loading occurs when the first test instance is received. Environment variables can be used in the supplied path.", inputMappedClassifier0.modelPathTipText());
      assertEquals("Trim white space from each end of attribute names and nominal values before matching.", inputMappedClassifier0.trimTipText());
      assertEquals(0, inputMappedClassifier0.graphType());
      assertEquals("Don't output a report of model-to-input mappings.", inputMappedClassifier0.suppressMappingReportTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", inputMappedClassifier0.debugTipText());
      assertFalse(inputMappedClassifier0.getSuppressMappingReport());
      assertEquals("Wrapper classifier that addresses incompatible training and test data by building a mapping between the training data that a classifier has been built with and the incoming test instances' structure. Model attributes that are not found in the incoming instances receive missing values, so do incoming nominal attribute values that the classifier has not seen before. A new classifier can be trained or an existing one loaded from a file.", inputMappedClassifier0.globalInfo());
      assertTrue(inputMappedClassifier0.getIgnoreCaseForNames());
      assertEquals("The base classifier to be used.", inputMappedClassifier0.classifierTipText());
      assertFalse(inputMappedClassifier0.getDebug());
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      assertTrue(capabilities0.hasDependencies());
      assertEquals(1, testInstances0.getNumString());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getSeed());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(5, testInstances0.getNumAttributes());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(4, testInstances0.getNumClasses());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(20, testInstances0.getNumInstances());
      assertFalse(testInstances0.getNoClass());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(1, testInstances0.getNumRelationalString());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(5, instances0.numAttributes());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(20, instances0.size());
      assertEquals(20, instances0.numInstances());
      assertEquals(4, instances0.classIndex());
      assertEquals(4, instances0.numClasses());
      assertTrue(instances0.checkForStringAttributes());
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      
      String string0 = naiveBayesMultinomialText0.toString();
      assertNotNull(string0);
      assertEquals("The independent probability of a class\n--------------------------------------\nclass1\t9.0\nclass2\t4.0\nclass3\t7.0\nclass4\t4.0\n\nThe probability of a word given the class\n-----------------------------------------\n\tclass1\tclass2\tclass3\tclass4\t\nover\t-------------\t-------------\t22026.465794806718\t22026.465794806718\t\nquick\t22026.465794806718\t2.718281828459045\t22026.465794806718\t22026.465794806718\t\nlaz\t22026.465794806718\t-------------\t22026.465794806718\t-------------\t\nth\t1.7848230096318725E8\t22026.465794806718\t22026.465794806718\t-------------\t\nbrown\t22026.465794806718\t22026.465794806718\t-------------\t-------------\t\ndog\t22026.465794806718\t-------------\t22026.465794806718\t-------------\t\nfox\t-------------\t-------------\t22026.465794806718\t22026.465794806718\t\njump\t1.7848230096318725E8\t22026.465794806718\t2.718281828459045\t2.718281828459045\t\n", string0);
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals(9, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertTrue(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      
      naiveBayesMultinomialText0.pruneDictionary();
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals(9, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertTrue(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      
      System.setCurrentTimeMillis((-2));
  }

  /**
  //Test case number: 30
  /*Coverage entropy=2.3025850929940455
  */
  @Test(timeout = 4000)
  public void test30()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      assertNotNull(naiveBayesMultinomialText0);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      
      IteratedLovinsStemmer iteratedLovinsStemmer0 = new IteratedLovinsStemmer();
      assertNotNull(iteratedLovinsStemmer0);
      
      LovinsStemmer lovinsStemmer0 = new LovinsStemmer();
      assertNotNull(lovinsStemmer0);
      
      naiveBayesMultinomialText0.setStemmer(lovinsStemmer0);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      
      EvoSuiteFile evoSuiteFile0 = new EvoSuiteFile("/home/ubuntu/wekafiles/props");
      boolean boolean0 = FileSystemHandling.createFolder(evoSuiteFile0);
      assertTrue(boolean0);
      
      LinkedHashMap<String, NaiveBayesMultinomialText.Count> linkedHashMap0 = new LinkedHashMap<String, NaiveBayesMultinomialText.Count>();
      assertNotNull(linkedHashMap0);
      assertEquals(0, linkedHashMap0.size());
      assertTrue(linkedHashMap0.isEmpty());
      
      naiveBayesMultinomialText0.m_normalize = true;
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertTrue(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      
      naiveBayesMultinomialText0.setUseWordFrequencies(true);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertTrue(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertTrue(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      
      TestInstances testInstances0 = new TestInstances();
      assertNotNull(testInstances0);
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertFalse(testInstances0.getNoClass());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(0, testInstances0.getNumDate());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      
      InputMappedClassifier inputMappedClassifier0 = new InputMappedClassifier();
      assertNotNull(inputMappedClassifier0);
      assertEquals("Wrapper classifier that addresses incompatible training and test data by building a mapping between the training data that a classifier has been built with and the incoming test instances' structure. Model attributes that are not found in the incoming instances receive missing values, so do incoming nominal attribute values that the classifier has not seen before. A new classifier can be trained or an existing one loaded from a file.", inputMappedClassifier0.globalInfo());
      assertEquals("Trim white space from each end of attribute names and nominal values before matching.", inputMappedClassifier0.trimTipText());
      assertEquals("Ignore case when matching attribute names and nomina values.", inputMappedClassifier0.ignoreCaseForNamesTipText());
      assertEquals(0, inputMappedClassifier0.graphType());
      assertEquals("Set the path from which to load a model. Loading occurs when the first test instance is received. Environment variables can be used in the supplied path.", inputMappedClassifier0.modelPathTipText());
      assertEquals("", inputMappedClassifier0.getModelPath());
      assertFalse(inputMappedClassifier0.getSuppressMappingReport());
      assertFalse(inputMappedClassifier0.getDebug());
      assertEquals("If set to true, classifier may output additional info to the console.", inputMappedClassifier0.debugTipText());
      assertEquals("The base classifier to be used.", inputMappedClassifier0.classifierTipText());
      assertTrue(inputMappedClassifier0.getTrim());
      assertTrue(inputMappedClassifier0.getIgnoreCaseForNames());
      assertEquals("Don't output a report of model-to-input mappings.", inputMappedClassifier0.suppressMappingReportTipText());
      
      Capabilities capabilities0 = inputMappedClassifier0.getCapabilities();
      assertNotNull(capabilities0);
      assertEquals("Wrapper classifier that addresses incompatible training and test data by building a mapping between the training data that a classifier has been built with and the incoming test instances' structure. Model attributes that are not found in the incoming instances receive missing values, so do incoming nominal attribute values that the classifier has not seen before. A new classifier can be trained or an existing one loaded from a file.", inputMappedClassifier0.globalInfo());
      assertEquals("Trim white space from each end of attribute names and nominal values before matching.", inputMappedClassifier0.trimTipText());
      assertEquals("Ignore case when matching attribute names and nomina values.", inputMappedClassifier0.ignoreCaseForNamesTipText());
      assertEquals(0, inputMappedClassifier0.graphType());
      assertEquals("Set the path from which to load a model. Loading occurs when the first test instance is received. Environment variables can be used in the supplied path.", inputMappedClassifier0.modelPathTipText());
      assertEquals("", inputMappedClassifier0.getModelPath());
      assertFalse(inputMappedClassifier0.getSuppressMappingReport());
      assertFalse(inputMappedClassifier0.getDebug());
      assertEquals("If set to true, classifier may output additional info to the console.", inputMappedClassifier0.debugTipText());
      assertEquals("The base classifier to be used.", inputMappedClassifier0.classifierTipText());
      assertTrue(inputMappedClassifier0.getTrim());
      assertTrue(inputMappedClassifier0.getIgnoreCaseForNames());
      assertEquals("Don't output a report of model-to-input mappings.", inputMappedClassifier0.suppressMappingReportTipText());
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      assertTrue(capabilities0.hasDependencies());
      
      TestInstances testInstances1 = TestInstances.forCapabilities(capabilities0);
      assertNotSame(testInstances1, testInstances0);
      assertNotNull(testInstances1);
      assertFalse(testInstances1.equals((Object)testInstances0));
      assertEquals("Wrapper classifier that addresses incompatible training and test data by building a mapping between the training data that a classifier has been built with and the incoming test instances' structure. Model attributes that are not found in the incoming instances receive missing values, so do incoming nominal attribute values that the classifier has not seen before. A new classifier can be trained or an existing one loaded from a file.", inputMappedClassifier0.globalInfo());
      assertEquals("Trim white space from each end of attribute names and nominal values before matching.", inputMappedClassifier0.trimTipText());
      assertEquals("Ignore case when matching attribute names and nomina values.", inputMappedClassifier0.ignoreCaseForNamesTipText());
      assertEquals(0, inputMappedClassifier0.graphType());
      assertEquals("Set the path from which to load a model. Loading occurs when the first test instance is received. Environment variables can be used in the supplied path.", inputMappedClassifier0.modelPathTipText());
      assertEquals("", inputMappedClassifier0.getModelPath());
      assertFalse(inputMappedClassifier0.getSuppressMappingReport());
      assertFalse(inputMappedClassifier0.getDebug());
      assertEquals("If set to true, classifier may output additional info to the console.", inputMappedClassifier0.debugTipText());
      assertEquals("The base classifier to be used.", inputMappedClassifier0.classifierTipText());
      assertTrue(inputMappedClassifier0.getTrim());
      assertTrue(inputMappedClassifier0.getIgnoreCaseForNames());
      assertEquals("Don't output a report of model-to-input mappings.", inputMappedClassifier0.suppressMappingReportTipText());
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      assertTrue(capabilities0.hasDependencies());
      assertEquals(1, testInstances1.getNumRelationalNominal());
      assertEquals(1, testInstances1.getSeed());
      assertEquals((-1), testInstances1.getClassIndex());
      assertEquals(1, testInstances1.getNumString());
      assertEquals(1, testInstances1.getClassType());
      assertEquals(5, testInstances1.getNumAttributes());
      assertEquals(1, testInstances1.getNumNumeric());
      assertEquals(1, testInstances1.getNumRelationalNumeric());
      assertEquals(4, testInstances1.getNumClasses());
      assertEquals(2, testInstances1.getNumRelationalNominalValues());
      assertFalse(testInstances1.getMultiInstance());
      assertEquals(20, testInstances1.getNumInstances());
      assertFalse(testInstances1.getNoClass());
      assertEquals(1, testInstances1.getNumRelationalString());
      assertEquals(0, testInstances1.getNumRelational());
      assertEquals("Testdata", testInstances1.getRelation());
      assertEquals(1, testInstances1.getNumDate());
      assertEquals(" ", testInstances1.getWordSeparators());
      assertEquals(1, testInstances1.getNumRelationalDate());
      assertEquals(1, testInstances1.getNumNominal());
      assertEquals(2, testInstances1.getNumNominalValues());
      assertEquals(10, testInstances1.getNumInstancesRelational());
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      
      testInstances1.setNumClasses(2074);
      assertNotSame(testInstances1, testInstances0);
      assertFalse(testInstances1.equals((Object)testInstances0));
      assertEquals("Wrapper classifier that addresses incompatible training and test data by building a mapping between the training data that a classifier has been built with and the incoming test instances' structure. Model attributes that are not found in the incoming instances receive missing values, so do incoming nominal attribute values that the classifier has not seen before. A new classifier can be trained or an existing one loaded from a file.", inputMappedClassifier0.globalInfo());
      assertEquals("Trim white space from each end of attribute names and nominal values before matching.", inputMappedClassifier0.trimTipText());
      assertEquals("Ignore case when matching attribute names and nomina values.", inputMappedClassifier0.ignoreCaseForNamesTipText());
      assertEquals(0, inputMappedClassifier0.graphType());
      assertEquals("Set the path from which to load a model. Loading occurs when the first test instance is received. Environment variables can be used in the supplied path.", inputMappedClassifier0.modelPathTipText());
      assertEquals("", inputMappedClassifier0.getModelPath());
      assertFalse(inputMappedClassifier0.getSuppressMappingReport());
      assertFalse(inputMappedClassifier0.getDebug());
      assertEquals("If set to true, classifier may output additional info to the console.", inputMappedClassifier0.debugTipText());
      assertEquals("The base classifier to be used.", inputMappedClassifier0.classifierTipText());
      assertTrue(inputMappedClassifier0.getTrim());
      assertTrue(inputMappedClassifier0.getIgnoreCaseForNames());
      assertEquals("Don't output a report of model-to-input mappings.", inputMappedClassifier0.suppressMappingReportTipText());
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      assertTrue(capabilities0.hasDependencies());
      assertEquals(1, testInstances1.getNumRelationalNominal());
      assertEquals(1, testInstances1.getSeed());
      assertEquals((-1), testInstances1.getClassIndex());
      assertEquals(1, testInstances1.getNumString());
      assertEquals(1, testInstances1.getClassType());
      assertEquals(5, testInstances1.getNumAttributes());
      assertEquals(1, testInstances1.getNumNumeric());
      assertEquals(1, testInstances1.getNumRelationalNumeric());
      assertEquals(2, testInstances1.getNumRelationalNominalValues());
      assertEquals(2074, testInstances1.getNumClasses());
      assertFalse(testInstances1.getMultiInstance());
      assertEquals(20, testInstances1.getNumInstances());
      assertFalse(testInstances1.getNoClass());
      assertEquals(1, testInstances1.getNumRelationalString());
      assertEquals(0, testInstances1.getNumRelational());
      assertEquals("Testdata", testInstances1.getRelation());
      assertEquals(1, testInstances1.getNumDate());
      assertEquals(" ", testInstances1.getWordSeparators());
      assertEquals(1, testInstances1.getNumRelationalDate());
      assertEquals(1, testInstances1.getNumNominal());
      assertEquals(2, testInstances1.getNumNominalValues());
      assertEquals(10, testInstances1.getNumInstancesRelational());
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      
      Instances instances0 = testInstances1.generate();
      assertNotSame(testInstances1, testInstances0);
      assertNotNull(instances0);
      assertFalse(testInstances1.equals((Object)testInstances0));
      assertEquals("Wrapper classifier that addresses incompatible training and test data by building a mapping between the training data that a classifier has been built with and the incoming test instances' structure. Model attributes that are not found in the incoming instances receive missing values, so do incoming nominal attribute values that the classifier has not seen before. A new classifier can be trained or an existing one loaded from a file.", inputMappedClassifier0.globalInfo());
      assertEquals("Trim white space from each end of attribute names and nominal values before matching.", inputMappedClassifier0.trimTipText());
      assertEquals("Ignore case when matching attribute names and nomina values.", inputMappedClassifier0.ignoreCaseForNamesTipText());
      assertEquals(0, inputMappedClassifier0.graphType());
      assertEquals("Set the path from which to load a model. Loading occurs when the first test instance is received. Environment variables can be used in the supplied path.", inputMappedClassifier0.modelPathTipText());
      assertEquals("", inputMappedClassifier0.getModelPath());
      assertFalse(inputMappedClassifier0.getSuppressMappingReport());
      assertFalse(inputMappedClassifier0.getDebug());
      assertEquals("If set to true, classifier may output additional info to the console.", inputMappedClassifier0.debugTipText());
      assertEquals("The base classifier to be used.", inputMappedClassifier0.classifierTipText());
      assertTrue(inputMappedClassifier0.getTrim());
      assertTrue(inputMappedClassifier0.getIgnoreCaseForNames());
      assertEquals("Don't output a report of model-to-input mappings.", inputMappedClassifier0.suppressMappingReportTipText());
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      assertTrue(capabilities0.hasDependencies());
      assertEquals(1, testInstances1.getNumRelationalNominal());
      assertEquals(1, testInstances1.getSeed());
      assertEquals((-1), testInstances1.getClassIndex());
      assertEquals(1, testInstances1.getNumString());
      assertEquals(1, testInstances1.getClassType());
      assertEquals(5, testInstances1.getNumAttributes());
      assertEquals(1, testInstances1.getNumNumeric());
      assertEquals(1, testInstances1.getNumRelationalNumeric());
      assertEquals(2, testInstances1.getNumRelationalNominalValues());
      assertEquals(2074, testInstances1.getNumClasses());
      assertFalse(testInstances1.getMultiInstance());
      assertEquals(20, testInstances1.getNumInstances());
      assertFalse(testInstances1.getNoClass());
      assertEquals(1, testInstances1.getNumRelationalString());
      assertEquals(0, testInstances1.getNumRelational());
      assertEquals("Testdata", testInstances1.getRelation());
      assertEquals(1, testInstances1.getNumDate());
      assertEquals(" ", testInstances1.getWordSeparators());
      assertEquals(1, testInstances1.getNumRelationalDate());
      assertEquals(1, testInstances1.getNumNominal());
      assertEquals(2, testInstances1.getNumNominalValues());
      assertEquals(10, testInstances1.getNumInstancesRelational());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertTrue(instances0.checkForStringAttributes());
      assertEquals(5, instances0.numAttributes());
      assertEquals(2074, instances0.numClasses());
      assertEquals(20, instances0.size());
      assertEquals(4, instances0.classIndex());
      assertEquals(20, instances0.numInstances());
      assertEquals("Testdata", instances0.relationName());
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      
      naiveBayesMultinomialText0.buildClassifier(instances0);
  }

  /**
  //Test case number: 31
  /*Coverage entropy=1.51824032320692
  */
  @Test(timeout = 4000)
  public void test31()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      assertNotNull(naiveBayesMultinomialText0);
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      
      IteratedLovinsStemmer iteratedLovinsStemmer0 = new IteratedLovinsStemmer();
      assertNotNull(iteratedLovinsStemmer0);
      
      LovinsStemmer lovinsStemmer0 = new LovinsStemmer();
      assertNotNull(lovinsStemmer0);
      
      naiveBayesMultinomialText0.setStemmer(lovinsStemmer0);
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      
      EvoSuiteFile evoSuiteFile0 = new EvoSuiteFile("/home/ubuntu/wekafiles/props");
      boolean boolean0 = FileSystemHandling.createFolder(evoSuiteFile0);
      assertTrue(boolean0);
      
      LinkedHashMap<String, NaiveBayesMultinomialText.Count> linkedHashMap0 = new LinkedHashMap<String, NaiveBayesMultinomialText.Count>();
      assertNotNull(linkedHashMap0);
      assertTrue(linkedHashMap0.isEmpty());
      assertEquals(0, linkedHashMap0.size());
      
      boolean boolean1 = FileSystemHandling.shouldAllThrowIOExceptions();
      assertTrue(boolean1);
      assertTrue(boolean1 == boolean0);
      
      naiveBayesMultinomialText0.setUseWordFrequencies(true);
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertTrue(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      
      TestInstances testInstances0 = new TestInstances();
      assertNotNull(testInstances0);
      assertEquals(0, testInstances0.getNumString());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals((-1), testInstances0.getClassIndex());
      assertFalse(testInstances0.getNoClass());
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      
      InputMappedClassifier inputMappedClassifier0 = new InputMappedClassifier();
      assertNotNull(inputMappedClassifier0);
      assertEquals("Wrapper classifier that addresses incompatible training and test data by building a mapping between the training data that a classifier has been built with and the incoming test instances' structure. Model attributes that are not found in the incoming instances receive missing values, so do incoming nominal attribute values that the classifier has not seen before. A new classifier can be trained or an existing one loaded from a file.", inputMappedClassifier0.globalInfo());
      assertTrue(inputMappedClassifier0.getIgnoreCaseForNames());
      assertEquals("Trim white space from each end of attribute names and nominal values before matching.", inputMappedClassifier0.trimTipText());
      assertEquals("The base classifier to be used.", inputMappedClassifier0.classifierTipText());
      assertEquals("Ignore case when matching attribute names and nomina values.", inputMappedClassifier0.ignoreCaseForNamesTipText());
      assertFalse(inputMappedClassifier0.getSuppressMappingReport());
      assertEquals("Don't output a report of model-to-input mappings.", inputMappedClassifier0.suppressMappingReportTipText());
      assertEquals("Set the path from which to load a model. Loading occurs when the first test instance is received. Environment variables can be used in the supplied path.", inputMappedClassifier0.modelPathTipText());
      assertFalse(inputMappedClassifier0.getDebug());
      assertEquals(0, inputMappedClassifier0.graphType());
      assertEquals("If set to true, classifier may output additional info to the console.", inputMappedClassifier0.debugTipText());
      assertTrue(inputMappedClassifier0.getTrim());
      assertEquals("", inputMappedClassifier0.getModelPath());
      
      String string0 = inputMappedClassifier0.toString();
      assertNotNull(string0);
      assertEquals("InputMappedClassifier:\n\nZeroR: No model built yet.", string0);
      assertEquals("Wrapper classifier that addresses incompatible training and test data by building a mapping between the training data that a classifier has been built with and the incoming test instances' structure. Model attributes that are not found in the incoming instances receive missing values, so do incoming nominal attribute values that the classifier has not seen before. A new classifier can be trained or an existing one loaded from a file.", inputMappedClassifier0.globalInfo());
      assertTrue(inputMappedClassifier0.getIgnoreCaseForNames());
      assertEquals("Trim white space from each end of attribute names and nominal values before matching.", inputMappedClassifier0.trimTipText());
      assertEquals("The base classifier to be used.", inputMappedClassifier0.classifierTipText());
      assertEquals("Ignore case when matching attribute names and nomina values.", inputMappedClassifier0.ignoreCaseForNamesTipText());
      assertFalse(inputMappedClassifier0.getSuppressMappingReport());
      assertEquals("Don't output a report of model-to-input mappings.", inputMappedClassifier0.suppressMappingReportTipText());
      assertEquals("Set the path from which to load a model. Loading occurs when the first test instance is received. Environment variables can be used in the supplied path.", inputMappedClassifier0.modelPathTipText());
      assertFalse(inputMappedClassifier0.getDebug());
      assertEquals(0, inputMappedClassifier0.graphType());
      assertEquals("If set to true, classifier may output additional info to the console.", inputMappedClassifier0.debugTipText());
      assertTrue(inputMappedClassifier0.getTrim());
      assertEquals("", inputMappedClassifier0.getModelPath());
      
      Capabilities capabilities0 = inputMappedClassifier0.getCapabilities();
      assertNotNull(capabilities0);
      assertEquals("Wrapper classifier that addresses incompatible training and test data by building a mapping between the training data that a classifier has been built with and the incoming test instances' structure. Model attributes that are not found in the incoming instances receive missing values, so do incoming nominal attribute values that the classifier has not seen before. A new classifier can be trained or an existing one loaded from a file.", inputMappedClassifier0.globalInfo());
      assertTrue(inputMappedClassifier0.getIgnoreCaseForNames());
      assertEquals("Trim white space from each end of attribute names and nominal values before matching.", inputMappedClassifier0.trimTipText());
      assertEquals("The base classifier to be used.", inputMappedClassifier0.classifierTipText());
      assertEquals("Ignore case when matching attribute names and nomina values.", inputMappedClassifier0.ignoreCaseForNamesTipText());
      assertFalse(inputMappedClassifier0.getSuppressMappingReport());
      assertEquals("Don't output a report of model-to-input mappings.", inputMappedClassifier0.suppressMappingReportTipText());
      assertEquals("Set the path from which to load a model. Loading occurs when the first test instance is received. Environment variables can be used in the supplied path.", inputMappedClassifier0.modelPathTipText());
      assertFalse(inputMappedClassifier0.getDebug());
      assertEquals(0, inputMappedClassifier0.graphType());
      assertEquals("If set to true, classifier may output additional info to the console.", inputMappedClassifier0.debugTipText());
      assertTrue(inputMappedClassifier0.getTrim());
      assertEquals("", inputMappedClassifier0.getModelPath());
      assertTrue(capabilities0.hasDependencies());
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      
      TestInstances testInstances1 = TestInstances.forCapabilities(capabilities0);
      assertNotSame(testInstances1, testInstances0);
      assertNotNull(testInstances1);
      assertFalse(testInstances1.equals((Object)testInstances0));
      assertEquals("Wrapper classifier that addresses incompatible training and test data by building a mapping between the training data that a classifier has been built with and the incoming test instances' structure. Model attributes that are not found in the incoming instances receive missing values, so do incoming nominal attribute values that the classifier has not seen before. A new classifier can be trained or an existing one loaded from a file.", inputMappedClassifier0.globalInfo());
      assertTrue(inputMappedClassifier0.getIgnoreCaseForNames());
      assertEquals("Trim white space from each end of attribute names and nominal values before matching.", inputMappedClassifier0.trimTipText());
      assertEquals("The base classifier to be used.", inputMappedClassifier0.classifierTipText());
      assertEquals("Ignore case when matching attribute names and nomina values.", inputMappedClassifier0.ignoreCaseForNamesTipText());
      assertFalse(inputMappedClassifier0.getSuppressMappingReport());
      assertEquals("Don't output a report of model-to-input mappings.", inputMappedClassifier0.suppressMappingReportTipText());
      assertEquals("Set the path from which to load a model. Loading occurs when the first test instance is received. Environment variables can be used in the supplied path.", inputMappedClassifier0.modelPathTipText());
      assertFalse(inputMappedClassifier0.getDebug());
      assertEquals(0, inputMappedClassifier0.graphType());
      assertEquals("If set to true, classifier may output additional info to the console.", inputMappedClassifier0.debugTipText());
      assertTrue(inputMappedClassifier0.getTrim());
      assertEquals("", inputMappedClassifier0.getModelPath());
      assertTrue(capabilities0.hasDependencies());
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      assertEquals(0, testInstances1.getNumRelational());
      assertEquals(1, testInstances1.getNumRelationalDate());
      assertEquals(4, testInstances1.getNumClasses());
      assertEquals(5, testInstances1.getNumAttributes());
      assertFalse(testInstances1.getNoClass());
      assertEquals(2, testInstances1.getNumRelationalNominalValues());
      assertEquals(1, testInstances1.getNumNumeric());
      assertEquals(1, testInstances1.getNumString());
      assertEquals(1, testInstances1.getNumRelationalNumeric());
      assertEquals(" ", testInstances1.getWordSeparators());
      assertEquals("Testdata", testInstances1.getRelation());
      assertEquals(1, testInstances1.getNumRelationalString());
      assertEquals(1, testInstances1.getNumRelationalNominal());
      assertEquals(1, testInstances1.getSeed());
      assertFalse(testInstances1.getMultiInstance());
      assertEquals((-1), testInstances1.getClassIndex());
      assertEquals(20, testInstances1.getNumInstances());
      assertEquals(10, testInstances1.getNumInstancesRelational());
      assertEquals(1, testInstances1.getClassType());
      assertEquals(1, testInstances1.getNumDate());
      assertEquals(1, testInstances1.getNumNominal());
      assertEquals(2, testInstances1.getNumNominalValues());
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      
      testInstances1.setNumClasses(2074);
      assertNotSame(testInstances1, testInstances0);
      assertFalse(testInstances1.equals((Object)testInstances0));
      assertEquals("Wrapper classifier that addresses incompatible training and test data by building a mapping between the training data that a classifier has been built with and the incoming test instances' structure. Model attributes that are not found in the incoming instances receive missing values, so do incoming nominal attribute values that the classifier has not seen before. A new classifier can be trained or an existing one loaded from a file.", inputMappedClassifier0.globalInfo());
      assertTrue(inputMappedClassifier0.getIgnoreCaseForNames());
      assertEquals("Trim white space from each end of attribute names and nominal values before matching.", inputMappedClassifier0.trimTipText());
      assertEquals("The base classifier to be used.", inputMappedClassifier0.classifierTipText());
      assertEquals("Ignore case when matching attribute names and nomina values.", inputMappedClassifier0.ignoreCaseForNamesTipText());
      assertFalse(inputMappedClassifier0.getSuppressMappingReport());
      assertEquals("Don't output a report of model-to-input mappings.", inputMappedClassifier0.suppressMappingReportTipText());
      assertEquals("Set the path from which to load a model. Loading occurs when the first test instance is received. Environment variables can be used in the supplied path.", inputMappedClassifier0.modelPathTipText());
      assertFalse(inputMappedClassifier0.getDebug());
      assertEquals(0, inputMappedClassifier0.graphType());
      assertEquals("If set to true, classifier may output additional info to the console.", inputMappedClassifier0.debugTipText());
      assertTrue(inputMappedClassifier0.getTrim());
      assertEquals("", inputMappedClassifier0.getModelPath());
      assertTrue(capabilities0.hasDependencies());
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      assertEquals(0, testInstances1.getNumRelational());
      assertEquals(1, testInstances1.getNumRelationalDate());
      assertEquals(5, testInstances1.getNumAttributes());
      assertFalse(testInstances1.getNoClass());
      assertEquals(2, testInstances1.getNumRelationalNominalValues());
      assertEquals(1, testInstances1.getNumNumeric());
      assertEquals(1, testInstances1.getNumString());
      assertEquals(1, testInstances1.getNumRelationalNumeric());
      assertEquals(" ", testInstances1.getWordSeparators());
      assertEquals("Testdata", testInstances1.getRelation());
      assertEquals(1, testInstances1.getNumRelationalString());
      assertEquals(1, testInstances1.getNumRelationalNominal());
      assertEquals(1, testInstances1.getSeed());
      assertFalse(testInstances1.getMultiInstance());
      assertEquals((-1), testInstances1.getClassIndex());
      assertEquals(20, testInstances1.getNumInstances());
      assertEquals(2074, testInstances1.getNumClasses());
      assertEquals(10, testInstances1.getNumInstancesRelational());
      assertEquals(1, testInstances1.getClassType());
      assertEquals(1, testInstances1.getNumDate());
      assertEquals(1, testInstances1.getNumNominal());
      assertEquals(2, testInstances1.getNumNominalValues());
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      
      naiveBayesMultinomialText0.setUseStopList(true);
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertTrue(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertTrue(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      
      Instances instances0 = testInstances1.generate();
      assertNotSame(testInstances1, testInstances0);
      assertNotNull(instances0);
      assertFalse(testInstances1.equals((Object)testInstances0));
      assertEquals(2074, instances0.numClasses());
      assertEquals(5, instances0.numAttributes());
      assertEquals(20, instances0.numInstances());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(20, instances0.size());
      assertTrue(instances0.checkForStringAttributes());
      assertEquals(4, instances0.classIndex());
      assertEquals("Testdata", instances0.relationName());
      assertEquals("Wrapper classifier that addresses incompatible training and test data by building a mapping between the training data that a classifier has been built with and the incoming test instances' structure. Model attributes that are not found in the incoming instances receive missing values, so do incoming nominal attribute values that the classifier has not seen before. A new classifier can be trained or an existing one loaded from a file.", inputMappedClassifier0.globalInfo());
      assertTrue(inputMappedClassifier0.getIgnoreCaseForNames());
      assertEquals("Trim white space from each end of attribute names and nominal values before matching.", inputMappedClassifier0.trimTipText());
      assertEquals("The base classifier to be used.", inputMappedClassifier0.classifierTipText());
      assertEquals("Ignore case when matching attribute names and nomina values.", inputMappedClassifier0.ignoreCaseForNamesTipText());
      assertFalse(inputMappedClassifier0.getSuppressMappingReport());
      assertEquals("Don't output a report of model-to-input mappings.", inputMappedClassifier0.suppressMappingReportTipText());
      assertEquals("Set the path from which to load a model. Loading occurs when the first test instance is received. Environment variables can be used in the supplied path.", inputMappedClassifier0.modelPathTipText());
      assertFalse(inputMappedClassifier0.getDebug());
      assertEquals(0, inputMappedClassifier0.graphType());
      assertEquals("If set to true, classifier may output additional info to the console.", inputMappedClassifier0.debugTipText());
      assertTrue(inputMappedClassifier0.getTrim());
      assertEquals("", inputMappedClassifier0.getModelPath());
      assertTrue(capabilities0.hasDependencies());
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      assertEquals(0, testInstances1.getNumRelational());
      assertEquals(1, testInstances1.getNumRelationalDate());
      assertEquals(5, testInstances1.getNumAttributes());
      assertFalse(testInstances1.getNoClass());
      assertEquals(2, testInstances1.getNumRelationalNominalValues());
      assertEquals(1, testInstances1.getNumNumeric());
      assertEquals(1, testInstances1.getNumString());
      assertEquals(1, testInstances1.getNumRelationalNumeric());
      assertEquals(" ", testInstances1.getWordSeparators());
      assertEquals("Testdata", testInstances1.getRelation());
      assertEquals(1, testInstances1.getNumRelationalString());
      assertEquals(1, testInstances1.getNumRelationalNominal());
      assertEquals(1, testInstances1.getSeed());
      assertFalse(testInstances1.getMultiInstance());
      assertEquals((-1), testInstances1.getClassIndex());
      assertEquals(20, testInstances1.getNumInstances());
      assertEquals(2074, testInstances1.getNumClasses());
      assertEquals(10, testInstances1.getNumInstancesRelational());
      assertEquals(1, testInstances1.getClassType());
      assertEquals(1, testInstances1.getNumDate());
      assertEquals(1, testInstances1.getNumNominal());
      assertEquals(2, testInstances1.getNumNominalValues());
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      
      naiveBayesMultinomialText0.buildClassifier(instances0);
  }

  /**
  //Test case number: 32
  /*Coverage entropy=2.5649493574615376
  */
  @Test(timeout = 4000)
  public void test32()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      assertNotNull(naiveBayesMultinomialText0);
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      
      naiveBayesMultinomialText0.m_stemmer = null;
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      
      String[] stringArray0 = naiveBayesMultinomialText0.getOptions();
      assertNotNull(stringArray0);
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(10, stringArray0.length);
  }

  /**
  //Test case number: 33
  /*Coverage entropy=2.1972245773362196
  */
  @Test(timeout = 4000)
  public void test33()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      assertNotNull(naiveBayesMultinomialText0);
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      
      IteratedLovinsStemmer iteratedLovinsStemmer0 = new IteratedLovinsStemmer();
      assertNotNull(iteratedLovinsStemmer0);
      
      LovinsStemmer lovinsStemmer0 = new LovinsStemmer();
      assertNotNull(lovinsStemmer0);
      
      naiveBayesMultinomialText0.setStemmer(lovinsStemmer0);
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      
      EvoSuiteFile evoSuiteFile0 = new EvoSuiteFile("/home/ubuntu/wekafiles/props");
      boolean boolean0 = FileSystemHandling.createFolder(evoSuiteFile0);
      assertTrue(boolean0);
      
      LinkedHashMap<String, NaiveBayesMultinomialText.Count> linkedHashMap0 = naiveBayesMultinomialText0.m_inputVector;
      assertNull(linkedHashMap0);
      
      TestInstances testInstances0 = new TestInstances();
      assertNotNull(testInstances0);
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(20, testInstances0.getNumInstances());
      assertFalse(testInstances0.getNoClass());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      
      InputMappedClassifier inputMappedClassifier0 = new InputMappedClassifier();
      assertNotNull(inputMappedClassifier0);
      assertEquals("Trim white space from each end of attribute names and nominal values before matching.", inputMappedClassifier0.trimTipText());
      assertEquals(0, inputMappedClassifier0.graphType());
      assertEquals("Ignore case when matching attribute names and nomina values.", inputMappedClassifier0.ignoreCaseForNamesTipText());
      assertEquals("Wrapper classifier that addresses incompatible training and test data by building a mapping between the training data that a classifier has been built with and the incoming test instances' structure. Model attributes that are not found in the incoming instances receive missing values, so do incoming nominal attribute values that the classifier has not seen before. A new classifier can be trained or an existing one loaded from a file.", inputMappedClassifier0.globalInfo());
      assertTrue(inputMappedClassifier0.getIgnoreCaseForNames());
      assertEquals("If set to true, classifier may output additional info to the console.", inputMappedClassifier0.debugTipText());
      assertFalse(inputMappedClassifier0.getDebug());
      assertTrue(inputMappedClassifier0.getTrim());
      assertEquals("", inputMappedClassifier0.getModelPath());
      assertEquals("Don't output a report of model-to-input mappings.", inputMappedClassifier0.suppressMappingReportTipText());
      assertFalse(inputMappedClassifier0.getSuppressMappingReport());
      assertEquals("The base classifier to be used.", inputMappedClassifier0.classifierTipText());
      assertEquals("Set the path from which to load a model. Loading occurs when the first test instance is received. Environment variables can be used in the supplied path.", inputMappedClassifier0.modelPathTipText());
      
      Capabilities capabilities0 = inputMappedClassifier0.getCapabilities();
      assertNotNull(capabilities0);
      assertEquals("Trim white space from each end of attribute names and nominal values before matching.", inputMappedClassifier0.trimTipText());
      assertEquals(0, inputMappedClassifier0.graphType());
      assertEquals("Ignore case when matching attribute names and nomina values.", inputMappedClassifier0.ignoreCaseForNamesTipText());
      assertEquals("Wrapper classifier that addresses incompatible training and test data by building a mapping between the training data that a classifier has been built with and the incoming test instances' structure. Model attributes that are not found in the incoming instances receive missing values, so do incoming nominal attribute values that the classifier has not seen before. A new classifier can be trained or an existing one loaded from a file.", inputMappedClassifier0.globalInfo());
      assertTrue(inputMappedClassifier0.getIgnoreCaseForNames());
      assertEquals("If set to true, classifier may output additional info to the console.", inputMappedClassifier0.debugTipText());
      assertFalse(inputMappedClassifier0.getDebug());
      assertTrue(inputMappedClassifier0.getTrim());
      assertEquals("", inputMappedClassifier0.getModelPath());
      assertEquals("Don't output a report of model-to-input mappings.", inputMappedClassifier0.suppressMappingReportTipText());
      assertFalse(inputMappedClassifier0.getSuppressMappingReport());
      assertEquals("The base classifier to be used.", inputMappedClassifier0.classifierTipText());
      assertEquals("Set the path from which to load a model. Loading occurs when the first test instance is received. Environment variables can be used in the supplied path.", inputMappedClassifier0.modelPathTipText());
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      assertTrue(capabilities0.hasDependencies());
      
      TestInstances testInstances1 = TestInstances.forCapabilities(capabilities0);
      assertNotSame(testInstances1, testInstances0);
      assertNotNull(testInstances1);
      assertFalse(testInstances1.equals((Object)testInstances0));
      assertEquals("Trim white space from each end of attribute names and nominal values before matching.", inputMappedClassifier0.trimTipText());
      assertEquals(0, inputMappedClassifier0.graphType());
      assertEquals("Ignore case when matching attribute names and nomina values.", inputMappedClassifier0.ignoreCaseForNamesTipText());
      assertEquals("Wrapper classifier that addresses incompatible training and test data by building a mapping between the training data that a classifier has been built with and the incoming test instances' structure. Model attributes that are not found in the incoming instances receive missing values, so do incoming nominal attribute values that the classifier has not seen before. A new classifier can be trained or an existing one loaded from a file.", inputMappedClassifier0.globalInfo());
      assertTrue(inputMappedClassifier0.getIgnoreCaseForNames());
      assertEquals("If set to true, classifier may output additional info to the console.", inputMappedClassifier0.debugTipText());
      assertFalse(inputMappedClassifier0.getDebug());
      assertTrue(inputMappedClassifier0.getTrim());
      assertEquals("", inputMappedClassifier0.getModelPath());
      assertEquals("Don't output a report of model-to-input mappings.", inputMappedClassifier0.suppressMappingReportTipText());
      assertFalse(inputMappedClassifier0.getSuppressMappingReport());
      assertEquals("The base classifier to be used.", inputMappedClassifier0.classifierTipText());
      assertEquals("Set the path from which to load a model. Loading occurs when the first test instance is received. Environment variables can be used in the supplied path.", inputMappedClassifier0.modelPathTipText());
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      assertTrue(capabilities0.hasDependencies());
      assertEquals(5, testInstances1.getNumAttributes());
      assertEquals(1, testInstances1.getNumString());
      assertEquals(1, testInstances1.getNumNumeric());
      assertEquals(1, testInstances1.getSeed());
      assertEquals(1, testInstances1.getNumRelationalNumeric());
      assertEquals(1, testInstances1.getNumRelationalNominal());
      assertEquals(1, testInstances1.getNumRelationalString());
      assertEquals(2, testInstances1.getNumNominalValues());
      assertEquals(1, testInstances1.getNumDate());
      assertFalse(testInstances1.getMultiInstance());
      assertEquals(20, testInstances1.getNumInstances());
      assertFalse(testInstances1.getNoClass());
      assertEquals(2, testInstances1.getNumRelationalNominalValues());
      assertEquals(10, testInstances1.getNumInstancesRelational());
      assertEquals(1, testInstances1.getClassType());
      assertEquals("Testdata", testInstances1.getRelation());
      assertEquals(" ", testInstances1.getWordSeparators());
      assertEquals(1, testInstances1.getNumRelationalDate());
      assertEquals((-1), testInstances1.getClassIndex());
      assertEquals(1, testInstances1.getNumNominal());
      assertEquals(0, testInstances1.getNumRelational());
      assertEquals(4, testInstances1.getNumClasses());
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      
      naiveBayesMultinomialText0.m_lowercaseTokens = true;
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertTrue(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      
      testInstances1.setNumClasses(2074);
      assertNotSame(testInstances1, testInstances0);
      assertFalse(testInstances1.equals((Object)testInstances0));
      assertEquals("Trim white space from each end of attribute names and nominal values before matching.", inputMappedClassifier0.trimTipText());
      assertEquals(0, inputMappedClassifier0.graphType());
      assertEquals("Ignore case when matching attribute names and nomina values.", inputMappedClassifier0.ignoreCaseForNamesTipText());
      assertEquals("Wrapper classifier that addresses incompatible training and test data by building a mapping between the training data that a classifier has been built with and the incoming test instances' structure. Model attributes that are not found in the incoming instances receive missing values, so do incoming nominal attribute values that the classifier has not seen before. A new classifier can be trained or an existing one loaded from a file.", inputMappedClassifier0.globalInfo());
      assertTrue(inputMappedClassifier0.getIgnoreCaseForNames());
      assertEquals("If set to true, classifier may output additional info to the console.", inputMappedClassifier0.debugTipText());
      assertFalse(inputMappedClassifier0.getDebug());
      assertTrue(inputMappedClassifier0.getTrim());
      assertEquals("", inputMappedClassifier0.getModelPath());
      assertEquals("Don't output a report of model-to-input mappings.", inputMappedClassifier0.suppressMappingReportTipText());
      assertFalse(inputMappedClassifier0.getSuppressMappingReport());
      assertEquals("The base classifier to be used.", inputMappedClassifier0.classifierTipText());
      assertEquals("Set the path from which to load a model. Loading occurs when the first test instance is received. Environment variables can be used in the supplied path.", inputMappedClassifier0.modelPathTipText());
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      assertTrue(capabilities0.hasDependencies());
      assertEquals(5, testInstances1.getNumAttributes());
      assertEquals(1, testInstances1.getNumString());
      assertEquals(1, testInstances1.getNumNumeric());
      assertEquals(1, testInstances1.getSeed());
      assertEquals(1, testInstances1.getNumRelationalNumeric());
      assertEquals(1, testInstances1.getNumRelationalNominal());
      assertEquals(1, testInstances1.getNumRelationalString());
      assertEquals(2, testInstances1.getNumNominalValues());
      assertEquals(1, testInstances1.getNumDate());
      assertFalse(testInstances1.getMultiInstance());
      assertEquals(20, testInstances1.getNumInstances());
      assertFalse(testInstances1.getNoClass());
      assertEquals(2, testInstances1.getNumRelationalNominalValues());
      assertEquals(2074, testInstances1.getNumClasses());
      assertEquals(10, testInstances1.getNumInstancesRelational());
      assertEquals(1, testInstances1.getClassType());
      assertEquals("Testdata", testInstances1.getRelation());
      assertEquals(" ", testInstances1.getWordSeparators());
      assertEquals(1, testInstances1.getNumRelationalDate());
      assertEquals((-1), testInstances1.getClassIndex());
      assertEquals(1, testInstances1.getNumNominal());
      assertEquals(0, testInstances1.getNumRelational());
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      
      Instances instances0 = testInstances1.generate();
      assertNotSame(testInstances1, testInstances0);
      assertNotNull(instances0);
      assertFalse(testInstances1.equals((Object)testInstances0));
      assertEquals("Trim white space from each end of attribute names and nominal values before matching.", inputMappedClassifier0.trimTipText());
      assertEquals(0, inputMappedClassifier0.graphType());
      assertEquals("Ignore case when matching attribute names and nomina values.", inputMappedClassifier0.ignoreCaseForNamesTipText());
      assertEquals("Wrapper classifier that addresses incompatible training and test data by building a mapping between the training data that a classifier has been built with and the incoming test instances' structure. Model attributes that are not found in the incoming instances receive missing values, so do incoming nominal attribute values that the classifier has not seen before. A new classifier can be trained or an existing one loaded from a file.", inputMappedClassifier0.globalInfo());
      assertTrue(inputMappedClassifier0.getIgnoreCaseForNames());
      assertEquals("If set to true, classifier may output additional info to the console.", inputMappedClassifier0.debugTipText());
      assertFalse(inputMappedClassifier0.getDebug());
      assertTrue(inputMappedClassifier0.getTrim());
      assertEquals("", inputMappedClassifier0.getModelPath());
      assertEquals("Don't output a report of model-to-input mappings.", inputMappedClassifier0.suppressMappingReportTipText());
      assertFalse(inputMappedClassifier0.getSuppressMappingReport());
      assertEquals("The base classifier to be used.", inputMappedClassifier0.classifierTipText());
      assertEquals("Set the path from which to load a model. Loading occurs when the first test instance is received. Environment variables can be used in the supplied path.", inputMappedClassifier0.modelPathTipText());
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      assertTrue(capabilities0.hasDependencies());
      assertEquals(5, testInstances1.getNumAttributes());
      assertEquals(1, testInstances1.getNumString());
      assertEquals(1, testInstances1.getNumNumeric());
      assertEquals(1, testInstances1.getSeed());
      assertEquals(1, testInstances1.getNumRelationalNumeric());
      assertEquals(1, testInstances1.getNumRelationalNominal());
      assertEquals(1, testInstances1.getNumRelationalString());
      assertEquals(2, testInstances1.getNumNominalValues());
      assertEquals(1, testInstances1.getNumDate());
      assertFalse(testInstances1.getMultiInstance());
      assertEquals(20, testInstances1.getNumInstances());
      assertFalse(testInstances1.getNoClass());
      assertEquals(2, testInstances1.getNumRelationalNominalValues());
      assertEquals(2074, testInstances1.getNumClasses());
      assertEquals(10, testInstances1.getNumInstancesRelational());
      assertEquals(1, testInstances1.getClassType());
      assertEquals("Testdata", testInstances1.getRelation());
      assertEquals(" ", testInstances1.getWordSeparators());
      assertEquals(1, testInstances1.getNumRelationalDate());
      assertEquals((-1), testInstances1.getClassIndex());
      assertEquals(1, testInstances1.getNumNominal());
      assertEquals(0, testInstances1.getNumRelational());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(2074, instances0.numClasses());
      assertEquals(20, instances0.numInstances());
      assertEquals(20, instances0.size());
      assertEquals(5, instances0.numAttributes());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(4, instances0.classIndex());
      assertTrue(instances0.checkForStringAttributes());
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      
      naiveBayesMultinomialText0.buildClassifier(instances0);
  }

  /**
  //Test case number: 34
  /*Coverage entropy=2.3025850929940455
  */
  @Test(timeout = 4000)
  public void test34()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      assertNotNull(naiveBayesMultinomialText0);
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      
      String[] stringArray0 = new String[4];
      stringArray0[0] = "\tThe number of nominal attributes in a rel. attribute (default 1).";
      stringArray0[1] = "-stopwords";
      stringArray0[2] = "-stopwords";
      stringArray0[3] = "";
      naiveBayesMultinomialText0.setOptions(stringArray0);
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(4, stringArray0.length);
  }

  /**
  //Test case number: 35
  /*Coverage entropy=2.4849066497880012
  */
  @Test(timeout = 4000)
  public void test35()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      assertNotNull(naiveBayesMultinomialText0);
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      
      naiveBayesMultinomialText0.m_useStopList = true;
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertTrue(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      
      IteratedLovinsStemmer iteratedLovinsStemmer0 = new IteratedLovinsStemmer();
      assertNotNull(iteratedLovinsStemmer0);
      
      InputMappedClassifier inputMappedClassifier0 = new InputMappedClassifier();
      assertNotNull(inputMappedClassifier0);
      assertFalse(inputMappedClassifier0.getDebug());
      assertEquals("", inputMappedClassifier0.getModelPath());
      assertTrue(inputMappedClassifier0.getTrim());
      assertFalse(inputMappedClassifier0.getSuppressMappingReport());
      assertEquals("Don't output a report of model-to-input mappings.", inputMappedClassifier0.suppressMappingReportTipText());
      assertEquals("Set the path from which to load a model. Loading occurs when the first test instance is received. Environment variables can be used in the supplied path.", inputMappedClassifier0.modelPathTipText());
      assertEquals("Ignore case when matching attribute names and nomina values.", inputMappedClassifier0.ignoreCaseForNamesTipText());
      assertEquals("The base classifier to be used.", inputMappedClassifier0.classifierTipText());
      assertEquals("Wrapper classifier that addresses incompatible training and test data by building a mapping between the training data that a classifier has been built with and the incoming test instances' structure. Model attributes that are not found in the incoming instances receive missing values, so do incoming nominal attribute values that the classifier has not seen before. A new classifier can be trained or an existing one loaded from a file.", inputMappedClassifier0.globalInfo());
      assertTrue(inputMappedClassifier0.getIgnoreCaseForNames());
      assertEquals("If set to true, classifier may output additional info to the console.", inputMappedClassifier0.debugTipText());
      assertEquals(0, inputMappedClassifier0.graphType());
      assertEquals("Trim white space from each end of attribute names and nominal values before matching.", inputMappedClassifier0.trimTipText());
      
      PrecomputedKernelMatrixKernel precomputedKernelMatrixKernel0 = new PrecomputedKernelMatrixKernel();
      assertNotNull(precomputedKernelMatrixKernel0);
      assertFalse(precomputedKernelMatrixKernel0.getChecksTurnedOff());
      assertEquals("Turns time-consuming checks off - use with caution.", precomputedKernelMatrixKernel0.checksTurnedOffTipText());
      assertEquals("This kernel is based on a static kernel matrix that is read from a file. Instances must have a single nominal attribute (excluding the class). This attribute must be the first attribute in the file and its values are used to reference rows/columns in the kernel matrix. The second attribute must be the class attribute.", precomputedKernelMatrixKernel0.globalInfo());
      assertEquals("Turns on the output of debugging information.", precomputedKernelMatrixKernel0.debugTipText());
      assertEquals(0, precomputedKernelMatrixKernel0.numCacheHits());
      assertEquals(0, precomputedKernelMatrixKernel0.numEvals());
      assertFalse(precomputedKernelMatrixKernel0.getDebug());
      assertEquals("The file holding the kernel matrix.", precomputedKernelMatrixKernel0.kernelMatrixFileTipText());
      
      Capabilities capabilities0 = precomputedKernelMatrixKernel0.getCapabilities();
      assertNotNull(capabilities0);
      assertFalse(precomputedKernelMatrixKernel0.getChecksTurnedOff());
      assertEquals("Turns time-consuming checks off - use with caution.", precomputedKernelMatrixKernel0.checksTurnedOffTipText());
      assertEquals("This kernel is based on a static kernel matrix that is read from a file. Instances must have a single nominal attribute (excluding the class). This attribute must be the first attribute in the file and its values are used to reference rows/columns in the kernel matrix. The second attribute must be the class attribute.", precomputedKernelMatrixKernel0.globalInfo());
      assertEquals("Turns on the output of debugging information.", precomputedKernelMatrixKernel0.debugTipText());
      assertEquals(0, precomputedKernelMatrixKernel0.numCacheHits());
      assertEquals(0, precomputedKernelMatrixKernel0.numEvals());
      assertFalse(precomputedKernelMatrixKernel0.getDebug());
      assertEquals("The file holding the kernel matrix.", precomputedKernelMatrixKernel0.kernelMatrixFileTipText());
      assertFalse(capabilities0.hasDependencies());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      
      File file0 = precomputedKernelMatrixKernel0.getKernelMatrixFile();
      assertNotNull(file0);
      assertFalse(precomputedKernelMatrixKernel0.getChecksTurnedOff());
      assertEquals("Turns time-consuming checks off - use with caution.", precomputedKernelMatrixKernel0.checksTurnedOffTipText());
      assertEquals("This kernel is based on a static kernel matrix that is read from a file. Instances must have a single nominal attribute (excluding the class). This attribute must be the first attribute in the file and its values are used to reference rows/columns in the kernel matrix. The second attribute must be the class attribute.", precomputedKernelMatrixKernel0.globalInfo());
      assertEquals("Turns on the output of debugging information.", precomputedKernelMatrixKernel0.debugTipText());
      assertEquals(0, precomputedKernelMatrixKernel0.numCacheHits());
      assertEquals(0, precomputedKernelMatrixKernel0.numEvals());
      assertFalse(precomputedKernelMatrixKernel0.getDebug());
      assertEquals("The file holding the kernel matrix.", precomputedKernelMatrixKernel0.kernelMatrixFileTipText());
      assertFalse(file0.isDirectory());
      assertEquals(0L, file0.length());
      assertFalse(file0.exists());
      assertEquals(0L, file0.getTotalSpace());
      assertFalse(file0.canExecute());
      assertTrue(file0.isFile());
      assertEquals(0L, file0.lastModified());
      assertFalse(file0.canWrite());
      assertEquals("kernelMatrix.matrix", file0.toString());
      assertFalse(file0.canRead());
      assertFalse(file0.isAbsolute());
      assertEquals("kernelMatrix.matrix", file0.getName());
      assertNull(file0.getParent());
      assertEquals(0L, file0.getUsableSpace());
      assertEquals(0L, file0.getFreeSpace());
      assertFalse(file0.isHidden());
      
      naiveBayesMultinomialText0.setStopwords(file0);
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertTrue(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertFalse(precomputedKernelMatrixKernel0.getChecksTurnedOff());
      assertEquals("Turns time-consuming checks off - use with caution.", precomputedKernelMatrixKernel0.checksTurnedOffTipText());
      assertEquals("This kernel is based on a static kernel matrix that is read from a file. Instances must have a single nominal attribute (excluding the class). This attribute must be the first attribute in the file and its values are used to reference rows/columns in the kernel matrix. The second attribute must be the class attribute.", precomputedKernelMatrixKernel0.globalInfo());
      assertEquals("Turns on the output of debugging information.", precomputedKernelMatrixKernel0.debugTipText());
      assertEquals(0, precomputedKernelMatrixKernel0.numCacheHits());
      assertEquals(0, precomputedKernelMatrixKernel0.numEvals());
      assertFalse(precomputedKernelMatrixKernel0.getDebug());
      assertEquals("The file holding the kernel matrix.", precomputedKernelMatrixKernel0.kernelMatrixFileTipText());
      assertFalse(file0.isDirectory());
      assertEquals(0L, file0.length());
      assertFalse(file0.exists());
      assertEquals(0L, file0.getTotalSpace());
      assertFalse(file0.canExecute());
      assertTrue(file0.isFile());
      assertEquals(0L, file0.lastModified());
      assertFalse(file0.canWrite());
      assertEquals("kernelMatrix.matrix", file0.toString());
      assertFalse(file0.canRead());
      assertFalse(file0.isAbsolute());
      assertEquals("kernelMatrix.matrix", file0.getName());
      assertNull(file0.getParent());
      assertEquals(0L, file0.getUsableSpace());
      assertEquals(0L, file0.getFreeSpace());
      assertFalse(file0.isHidden());
      
      naiveBayesMultinomialText0.setUseWordFrequencies(true);
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertTrue(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertTrue(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      
      InputMappedClassifier inputMappedClassifier1 = new InputMappedClassifier();
      assertNotNull(inputMappedClassifier1);
      assertFalse(inputMappedClassifier1.equals((Object)inputMappedClassifier0));
      assertEquals("Wrapper classifier that addresses incompatible training and test data by building a mapping between the training data that a classifier has been built with and the incoming test instances' structure. Model attributes that are not found in the incoming instances receive missing values, so do incoming nominal attribute values that the classifier has not seen before. A new classifier can be trained or an existing one loaded from a file.", inputMappedClassifier1.globalInfo());
      assertTrue(inputMappedClassifier1.getIgnoreCaseForNames());
      assertEquals("Trim white space from each end of attribute names and nominal values before matching.", inputMappedClassifier1.trimTipText());
      assertEquals(0, inputMappedClassifier1.graphType());
      assertEquals("If set to true, classifier may output additional info to the console.", inputMappedClassifier1.debugTipText());
      assertTrue(inputMappedClassifier1.getTrim());
      assertEquals("", inputMappedClassifier1.getModelPath());
      assertFalse(inputMappedClassifier1.getSuppressMappingReport());
      assertEquals("Set the path from which to load a model. Loading occurs when the first test instance is received. Environment variables can be used in the supplied path.", inputMappedClassifier1.modelPathTipText());
      assertFalse(inputMappedClassifier1.getDebug());
      assertEquals("Don't output a report of model-to-input mappings.", inputMappedClassifier1.suppressMappingReportTipText());
      assertEquals("The base classifier to be used.", inputMappedClassifier1.classifierTipText());
      assertEquals("Ignore case when matching attribute names and nomina values.", inputMappedClassifier1.ignoreCaseForNamesTipText());
      
      Capabilities capabilities1 = inputMappedClassifier0.getCapabilities();
      assertNotSame(inputMappedClassifier0, inputMappedClassifier1);
      assertNotSame(capabilities1, capabilities0);
      assertNotNull(capabilities1);
      assertFalse(inputMappedClassifier0.equals((Object)inputMappedClassifier1));
      assertFalse(capabilities1.equals((Object)capabilities0));
      assertFalse(inputMappedClassifier0.getDebug());
      assertEquals("", inputMappedClassifier0.getModelPath());
      assertTrue(inputMappedClassifier0.getTrim());
      assertFalse(inputMappedClassifier0.getSuppressMappingReport());
      assertEquals("Don't output a report of model-to-input mappings.", inputMappedClassifier0.suppressMappingReportTipText());
      assertEquals("Set the path from which to load a model. Loading occurs when the first test instance is received. Environment variables can be used in the supplied path.", inputMappedClassifier0.modelPathTipText());
      assertEquals("Ignore case when matching attribute names and nomina values.", inputMappedClassifier0.ignoreCaseForNamesTipText());
      assertEquals("The base classifier to be used.", inputMappedClassifier0.classifierTipText());
      assertEquals("Wrapper classifier that addresses incompatible training and test data by building a mapping between the training data that a classifier has been built with and the incoming test instances' structure. Model attributes that are not found in the incoming instances receive missing values, so do incoming nominal attribute values that the classifier has not seen before. A new classifier can be trained or an existing one loaded from a file.", inputMappedClassifier0.globalInfo());
      assertTrue(inputMappedClassifier0.getIgnoreCaseForNames());
      assertEquals("If set to true, classifier may output additional info to the console.", inputMappedClassifier0.debugTipText());
      assertEquals(0, inputMappedClassifier0.graphType());
      assertEquals("Trim white space from each end of attribute names and nominal values before matching.", inputMappedClassifier0.trimTipText());
      assertTrue(capabilities1.hasDependencies());
      assertEquals(0, capabilities1.getMinimumNumberInstances());
      
      GaussianProcesses gaussianProcesses0 = new GaussianProcesses();
      assertNotNull(gaussianProcesses0);
      assertEquals(1.0, gaussianProcesses0.getNoise(), 0.01);
      assertEquals("Determines how/if the data will be transformed.", gaussianProcesses0.filterTypeTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", gaussianProcesses0.debugTipText());
      assertFalse(gaussianProcesses0.getDebug());
      assertEquals("The level of Gaussian Noise (added to the diagonal of the Covariance Matrix), after the target has been normalized/standardized/left unchanged).", gaussianProcesses0.noiseTipText());
      assertEquals(" Implements Gaussian processes for regression without hyperparameter-tuning. To make choosing an appropriate noise level easier, this implementation applies normalization/standardization to the target attribute as well as the other attributes (if  normalization/standardizaton is turned on). Missing values are replaced by the global mean/mode. Nominal attributes are converted to binary ones. Note that kernel caching is turned off if the kernel used implements CachedKernel.", gaussianProcesses0.globalInfo());
      assertEquals("The kernel to use.", gaussianProcesses0.kernelTipText());
      assertEquals(0, GaussianProcesses.FILTER_NORMALIZE);
      assertEquals(1, GaussianProcesses.FILTER_STANDARDIZE);
      assertEquals(2, GaussianProcesses.FILTER_NONE);
      
      String[] stringArray0 = new String[2];
      AbstractClassifier.runClassifier(inputMappedClassifier0, stringArray0);
      assertNotSame(inputMappedClassifier0, inputMappedClassifier1);
      assertFalse(inputMappedClassifier0.equals((Object)inputMappedClassifier1));
      assertFalse(inputMappedClassifier0.getDebug());
      assertEquals("", inputMappedClassifier0.getModelPath());
      assertTrue(inputMappedClassifier0.getTrim());
      assertFalse(inputMappedClassifier0.getSuppressMappingReport());
      assertEquals("Don't output a report of model-to-input mappings.", inputMappedClassifier0.suppressMappingReportTipText());
      assertEquals("Set the path from which to load a model. Loading occurs when the first test instance is received. Environment variables can be used in the supplied path.", inputMappedClassifier0.modelPathTipText());
      assertEquals("Ignore case when matching attribute names and nomina values.", inputMappedClassifier0.ignoreCaseForNamesTipText());
      assertEquals("The base classifier to be used.", inputMappedClassifier0.classifierTipText());
      assertEquals("Wrapper classifier that addresses incompatible training and test data by building a mapping between the training data that a classifier has been built with and the incoming test instances' structure. Model attributes that are not found in the incoming instances receive missing values, so do incoming nominal attribute values that the classifier has not seen before. A new classifier can be trained or an existing one loaded from a file.", inputMappedClassifier0.globalInfo());
      assertTrue(inputMappedClassifier0.getIgnoreCaseForNames());
      assertEquals("If set to true, classifier may output additional info to the console.", inputMappedClassifier0.debugTipText());
      assertEquals(0, inputMappedClassifier0.graphType());
      assertEquals("Trim white space from each end of attribute names and nominal values before matching.", inputMappedClassifier0.trimTipText());
      assertEquals(2, stringArray0.length);
      
      Capabilities capabilities2 = gaussianProcesses0.getCapabilities();
      assertNotSame(capabilities2, capabilities0);
      assertNotSame(capabilities2, capabilities1);
      assertNotNull(capabilities2);
      assertFalse(capabilities2.equals((Object)capabilities0));
      assertFalse(capabilities2.equals((Object)capabilities1));
      assertEquals(1.0, gaussianProcesses0.getNoise(), 0.01);
      assertEquals("Determines how/if the data will be transformed.", gaussianProcesses0.filterTypeTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", gaussianProcesses0.debugTipText());
      assertFalse(gaussianProcesses0.getDebug());
      assertEquals("The level of Gaussian Noise (added to the diagonal of the Covariance Matrix), after the target has been normalized/standardized/left unchanged).", gaussianProcesses0.noiseTipText());
      assertEquals(" Implements Gaussian processes for regression without hyperparameter-tuning. To make choosing an appropriate noise level easier, this implementation applies normalization/standardization to the target attribute as well as the other attributes (if  normalization/standardizaton is turned on). Missing values are replaced by the global mean/mode. Nominal attributes are converted to binary ones. Note that kernel caching is turned off if the kernel used implements CachedKernel.", gaussianProcesses0.globalInfo());
      assertEquals("The kernel to use.", gaussianProcesses0.kernelTipText());
      assertEquals(1, capabilities2.getMinimumNumberInstances());
      assertTrue(capabilities2.hasDependencies());
      assertEquals(0, GaussianProcesses.FILTER_NORMALIZE);
      assertEquals(1, GaussianProcesses.FILTER_STANDARDIZE);
      assertEquals(2, GaussianProcesses.FILTER_NONE);
      
      TestInstances testInstances0 = TestInstances.forCapabilities(capabilities2);
      assertNotSame(capabilities2, capabilities0);
      assertNotSame(capabilities2, capabilities1);
      assertNotNull(testInstances0);
      assertFalse(capabilities2.equals((Object)capabilities0));
      assertFalse(capabilities2.equals((Object)capabilities1));
      assertEquals(1.0, gaussianProcesses0.getNoise(), 0.01);
      assertEquals("Determines how/if the data will be transformed.", gaussianProcesses0.filterTypeTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", gaussianProcesses0.debugTipText());
      assertFalse(gaussianProcesses0.getDebug());
      assertEquals("The level of Gaussian Noise (added to the diagonal of the Covariance Matrix), after the target has been normalized/standardized/left unchanged).", gaussianProcesses0.noiseTipText());
      assertEquals(" Implements Gaussian processes for regression without hyperparameter-tuning. To make choosing an appropriate noise level easier, this implementation applies normalization/standardization to the target attribute as well as the other attributes (if  normalization/standardizaton is turned on). Missing values are replaced by the global mean/mode. Nominal attributes are converted to binary ones. Note that kernel caching is turned off if the kernel used implements CachedKernel.", gaussianProcesses0.globalInfo());
      assertEquals("The kernel to use.", gaussianProcesses0.kernelTipText());
      assertEquals(1, capabilities2.getMinimumNumberInstances());
      assertTrue(capabilities2.hasDependencies());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(3, testInstances0.getNumAttributes());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(0, testInstances0.getNumRelational());
      assertFalse(testInstances0.getNoClass());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(1, testInstances0.getSeed());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(0, testInstances0.getClassType());
      assertEquals(0, testInstances0.getNumString());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(0, GaussianProcesses.FILTER_NORMALIZE);
      assertEquals(1, GaussianProcesses.FILTER_STANDARDIZE);
      assertEquals(2, GaussianProcesses.FILTER_NONE);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      
      TestInstances testInstances1 = new TestInstances();
      assertNotNull(testInstances1);
      assertFalse(testInstances1.equals((Object)testInstances0));
      assertEquals(10, testInstances1.getNumInstancesRelational());
      assertFalse(testInstances1.getMultiInstance());
      assertEquals(20, testInstances1.getNumInstances());
      assertFalse(testInstances1.getNoClass());
      assertEquals(0, testInstances1.getNumRelationalDate());
      assertEquals(1, testInstances1.getNumNominal());
      assertEquals("Testdata", testInstances1.getRelation());
      assertEquals(" ", testInstances1.getWordSeparators());
      assertEquals(0, testInstances1.getNumRelational());
      assertEquals(2, testInstances1.getNumClasses());
      assertEquals(1, testInstances1.getClassType());
      assertEquals(0, testInstances1.getNumDate());
      assertEquals(2, testInstances1.getNumNominalValues());
      assertEquals(2, testInstances1.getNumAttributes());
      assertEquals(1, testInstances1.getNumRelationalNominal());
      assertEquals(1, testInstances1.getSeed());
      assertEquals((-1), testInstances1.getClassIndex());
      assertEquals(0, testInstances1.getNumRelationalNumeric());
      assertEquals(0, testInstances1.getNumRelationalString());
      assertEquals(0, testInstances1.getNumNumeric());
      assertEquals(2, testInstances1.getNumRelationalNominalValues());
      assertEquals(0, testInstances1.getNumString());
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      
      testInstances1.setClassType(1);
      assertNotSame(testInstances1, testInstances0);
      assertFalse(testInstances1.equals((Object)testInstances0));
      assertEquals(10, testInstances1.getNumInstancesRelational());
      assertFalse(testInstances1.getMultiInstance());
      assertEquals(20, testInstances1.getNumInstances());
      assertFalse(testInstances1.getNoClass());
      assertEquals(0, testInstances1.getNumRelationalDate());
      assertEquals(1, testInstances1.getNumNominal());
      assertEquals("Testdata", testInstances1.getRelation());
      assertEquals(" ", testInstances1.getWordSeparators());
      assertEquals(0, testInstances1.getNumRelational());
      assertEquals(2, testInstances1.getNumClasses());
      assertEquals(1, testInstances1.getClassType());
      assertEquals(0, testInstances1.getNumDate());
      assertEquals(2, testInstances1.getNumNominalValues());
      assertEquals(2, testInstances1.getNumAttributes());
      assertEquals(1, testInstances1.getNumRelationalNominal());
      assertEquals(1, testInstances1.getSeed());
      assertEquals((-1), testInstances1.getClassIndex());
      assertEquals(0, testInstances1.getNumRelationalNumeric());
      assertEquals(0, testInstances1.getNumRelationalString());
      assertEquals(0, testInstances1.getNumNumeric());
      assertEquals(2, testInstances1.getNumRelationalNominalValues());
      assertEquals(0, testInstances1.getNumString());
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      
      Instances instances0 = testInstances1.generate();
      assertNotSame(testInstances1, testInstances0);
      assertNotNull(instances0);
      assertFalse(testInstances1.equals((Object)testInstances0));
      assertEquals(10, testInstances1.getNumInstancesRelational());
      assertFalse(testInstances1.getMultiInstance());
      assertEquals(20, testInstances1.getNumInstances());
      assertFalse(testInstances1.getNoClass());
      assertEquals(0, testInstances1.getNumRelationalDate());
      assertEquals(1, testInstances1.getNumNominal());
      assertEquals("Testdata", testInstances1.getRelation());
      assertEquals(" ", testInstances1.getWordSeparators());
      assertEquals(0, testInstances1.getNumRelational());
      assertEquals(2, testInstances1.getNumClasses());
      assertEquals(1, testInstances1.getClassType());
      assertEquals(0, testInstances1.getNumDate());
      assertEquals(2, testInstances1.getNumNominalValues());
      assertEquals(2, testInstances1.getNumAttributes());
      assertEquals(1, testInstances1.getNumRelationalNominal());
      assertEquals(1, testInstances1.getSeed());
      assertEquals((-1), testInstances1.getClassIndex());
      assertEquals(0, testInstances1.getNumRelationalNumeric());
      assertEquals(0, testInstances1.getNumRelationalString());
      assertEquals(0, testInstances1.getNumNumeric());
      assertEquals(2, testInstances1.getNumRelationalNominalValues());
      assertEquals(0, testInstances1.getNumString());
      assertEquals(1, instances0.classIndex());
      assertEquals(20, instances0.numInstances());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(2, instances0.numAttributes());
      assertEquals(2, instances0.numClasses());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20, instances0.size());
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      
      naiveBayesMultinomialText0.buildClassifier(instances0);
      assertNotSame(testInstances1, testInstances0);
      assertFalse(testInstances1.equals((Object)testInstances0));
      assertEquals(10, testInstances1.getNumInstancesRelational());
      assertFalse(testInstances1.getMultiInstance());
      assertEquals(20, testInstances1.getNumInstances());
      assertFalse(testInstances1.getNoClass());
      assertEquals(0, testInstances1.getNumRelationalDate());
      assertEquals(1, testInstances1.getNumNominal());
      assertEquals("Testdata", testInstances1.getRelation());
      assertEquals(" ", testInstances1.getWordSeparators());
      assertEquals(0, testInstances1.getNumRelational());
      assertEquals(2, testInstances1.getNumClasses());
      assertEquals(1, testInstances1.getClassType());
      assertEquals(0, testInstances1.getNumDate());
      assertEquals(2, testInstances1.getNumNominalValues());
      assertEquals(2, testInstances1.getNumAttributes());
      assertEquals(1, testInstances1.getNumRelationalNominal());
      assertEquals(1, testInstances1.getSeed());
      assertEquals((-1), testInstances1.getClassIndex());
      assertEquals(0, testInstances1.getNumRelationalNumeric());
      assertEquals(0, testInstances1.getNumRelationalString());
      assertEquals(0, testInstances1.getNumNumeric());
      assertEquals(2, testInstances1.getNumRelationalNominalValues());
      assertEquals(0, testInstances1.getNumString());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertTrue(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertTrue(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(1, instances0.classIndex());
      assertEquals(20, instances0.numInstances());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(2, instances0.numAttributes());
      assertEquals(2, instances0.numClasses());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20, instances0.size());
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      
      String string0 = naiveBayesMultinomialText0.toString();
      assertNotNull(string0);
      assertEquals("The independent probability of a class\n--------------------------------------\nclass1\t12.0\nclass2\t10.0\n\nThe probability of a word given the class\n-----------------------------------------\n\tclass1\tclass2\t\n", string0);
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertTrue(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertTrue(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      
      naiveBayesMultinomialText0.pruneDictionary();
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertTrue(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertTrue(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
  }

  /**
  //Test case number: 36
  /*Coverage entropy=2.2917778291357482
  */
  @Test(timeout = 4000)
  public void test36()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      assertNotNull(naiveBayesMultinomialText0);
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      
      String[] stringArray0 = new String[4];
      stringArray0[0] = "-tokenizer";
      stringArray0[1] = "-tokenizer";
      InputMappedClassifier inputMappedClassifier0 = new InputMappedClassifier();
      assertNotNull(inputMappedClassifier0);
      assertEquals("", inputMappedClassifier0.getModelPath());
      assertFalse(inputMappedClassifier0.getDebug());
      assertTrue(inputMappedClassifier0.getTrim());
      assertFalse(inputMappedClassifier0.getSuppressMappingReport());
      assertEquals("Don't output a report of model-to-input mappings.", inputMappedClassifier0.suppressMappingReportTipText());
      assertEquals("Set the path from which to load a model. Loading occurs when the first test instance is received. Environment variables can be used in the supplied path.", inputMappedClassifier0.modelPathTipText());
      assertEquals("Ignore case when matching attribute names and nomina values.", inputMappedClassifier0.ignoreCaseForNamesTipText());
      assertEquals("The base classifier to be used.", inputMappedClassifier0.classifierTipText());
      assertEquals("Wrapper classifier that addresses incompatible training and test data by building a mapping between the training data that a classifier has been built with and the incoming test instances' structure. Model attributes that are not found in the incoming instances receive missing values, so do incoming nominal attribute values that the classifier has not seen before. A new classifier can be trained or an existing one loaded from a file.", inputMappedClassifier0.globalInfo());
      assertTrue(inputMappedClassifier0.getIgnoreCaseForNames());
      assertEquals("Trim white space from each end of attribute names and nominal values before matching.", inputMappedClassifier0.trimTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", inputMappedClassifier0.debugTipText());
      assertEquals(0, inputMappedClassifier0.graphType());
      
      Capabilities capabilities0 = inputMappedClassifier0.getCapabilities();
      assertNotNull(capabilities0);
      assertEquals("", inputMappedClassifier0.getModelPath());
      assertFalse(inputMappedClassifier0.getDebug());
      assertTrue(inputMappedClassifier0.getTrim());
      assertFalse(inputMappedClassifier0.getSuppressMappingReport());
      assertEquals("Don't output a report of model-to-input mappings.", inputMappedClassifier0.suppressMappingReportTipText());
      assertEquals("Set the path from which to load a model. Loading occurs when the first test instance is received. Environment variables can be used in the supplied path.", inputMappedClassifier0.modelPathTipText());
      assertEquals("Ignore case when matching attribute names and nomina values.", inputMappedClassifier0.ignoreCaseForNamesTipText());
      assertEquals("The base classifier to be used.", inputMappedClassifier0.classifierTipText());
      assertEquals("Wrapper classifier that addresses incompatible training and test data by building a mapping between the training data that a classifier has been built with and the incoming test instances' structure. Model attributes that are not found in the incoming instances receive missing values, so do incoming nominal attribute values that the classifier has not seen before. A new classifier can be trained or an existing one loaded from a file.", inputMappedClassifier0.globalInfo());
      assertTrue(inputMappedClassifier0.getIgnoreCaseForNames());
      assertEquals("Trim white space from each end of attribute names and nominal values before matching.", inputMappedClassifier0.trimTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", inputMappedClassifier0.debugTipText());
      assertEquals(0, inputMappedClassifier0.graphType());
      assertTrue(capabilities0.hasDependencies());
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      
      LinkedHashMap<String, NaiveBayesMultinomialText.Count> linkedHashMap0 = new LinkedHashMap<String, NaiveBayesMultinomialText.Count>();
      assertNotNull(linkedHashMap0);
      assertEquals(0, linkedHashMap0.size());
      assertTrue(linkedHashMap0.isEmpty());
      
      TestInstances testInstances0 = TestInstances.forCapabilities(capabilities0);
      assertNotNull(testInstances0);
      assertEquals("", inputMappedClassifier0.getModelPath());
      assertFalse(inputMappedClassifier0.getDebug());
      assertTrue(inputMappedClassifier0.getTrim());
      assertFalse(inputMappedClassifier0.getSuppressMappingReport());
      assertEquals("Don't output a report of model-to-input mappings.", inputMappedClassifier0.suppressMappingReportTipText());
      assertEquals("Set the path from which to load a model. Loading occurs when the first test instance is received. Environment variables can be used in the supplied path.", inputMappedClassifier0.modelPathTipText());
      assertEquals("Ignore case when matching attribute names and nomina values.", inputMappedClassifier0.ignoreCaseForNamesTipText());
      assertEquals("The base classifier to be used.", inputMappedClassifier0.classifierTipText());
      assertEquals("Wrapper classifier that addresses incompatible training and test data by building a mapping between the training data that a classifier has been built with and the incoming test instances' structure. Model attributes that are not found in the incoming instances receive missing values, so do incoming nominal attribute values that the classifier has not seen before. A new classifier can be trained or an existing one loaded from a file.", inputMappedClassifier0.globalInfo());
      assertTrue(inputMappedClassifier0.getIgnoreCaseForNames());
      assertEquals("Trim white space from each end of attribute names and nominal values before matching.", inputMappedClassifier0.trimTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", inputMappedClassifier0.debugTipText());
      assertEquals(0, inputMappedClassifier0.graphType());
      assertTrue(capabilities0.hasDependencies());
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(4, testInstances0.getNumClasses());
      assertEquals(5, testInstances0.getNumAttributes());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertFalse(testInstances0.getNoClass());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(1, testInstances0.getNumString());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      
      Instances instances0 = testInstances0.generate();
      assertNotNull(instances0);
      assertEquals("", inputMappedClassifier0.getModelPath());
      assertFalse(inputMappedClassifier0.getDebug());
      assertTrue(inputMappedClassifier0.getTrim());
      assertFalse(inputMappedClassifier0.getSuppressMappingReport());
      assertEquals("Don't output a report of model-to-input mappings.", inputMappedClassifier0.suppressMappingReportTipText());
      assertEquals("Set the path from which to load a model. Loading occurs when the first test instance is received. Environment variables can be used in the supplied path.", inputMappedClassifier0.modelPathTipText());
      assertEquals("Ignore case when matching attribute names and nomina values.", inputMappedClassifier0.ignoreCaseForNamesTipText());
      assertEquals("The base classifier to be used.", inputMappedClassifier0.classifierTipText());
      assertEquals("Wrapper classifier that addresses incompatible training and test data by building a mapping between the training data that a classifier has been built with and the incoming test instances' structure. Model attributes that are not found in the incoming instances receive missing values, so do incoming nominal attribute values that the classifier has not seen before. A new classifier can be trained or an existing one loaded from a file.", inputMappedClassifier0.globalInfo());
      assertTrue(inputMappedClassifier0.getIgnoreCaseForNames());
      assertEquals("Trim white space from each end of attribute names and nominal values before matching.", inputMappedClassifier0.trimTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", inputMappedClassifier0.debugTipText());
      assertEquals(0, inputMappedClassifier0.graphType());
      assertTrue(capabilities0.hasDependencies());
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(4, testInstances0.getNumClasses());
      assertEquals(5, testInstances0.getNumAttributes());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertFalse(testInstances0.getNoClass());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(1, testInstances0.getNumString());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(4, instances0.numClasses());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(20, instances0.numInstances());
      assertTrue(instances0.checkForStringAttributes());
      assertEquals(5, instances0.numAttributes());
      assertEquals(20, instances0.size());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(4, instances0.classIndex());
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      
      naiveBayesMultinomialText0.buildClassifier(instances0);
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("", inputMappedClassifier0.getModelPath());
      assertFalse(inputMappedClassifier0.getDebug());
      assertTrue(inputMappedClassifier0.getTrim());
      assertFalse(inputMappedClassifier0.getSuppressMappingReport());
      assertEquals("Don't output a report of model-to-input mappings.", inputMappedClassifier0.suppressMappingReportTipText());
      assertEquals("Set the path from which to load a model. Loading occurs when the first test instance is received. Environment variables can be used in the supplied path.", inputMappedClassifier0.modelPathTipText());
      assertEquals("Ignore case when matching attribute names and nomina values.", inputMappedClassifier0.ignoreCaseForNamesTipText());
      assertEquals("The base classifier to be used.", inputMappedClassifier0.classifierTipText());
      assertEquals("Wrapper classifier that addresses incompatible training and test data by building a mapping between the training data that a classifier has been built with and the incoming test instances' structure. Model attributes that are not found in the incoming instances receive missing values, so do incoming nominal attribute values that the classifier has not seen before. A new classifier can be trained or an existing one loaded from a file.", inputMappedClassifier0.globalInfo());
      assertTrue(inputMappedClassifier0.getIgnoreCaseForNames());
      assertEquals("Trim white space from each end of attribute names and nominal values before matching.", inputMappedClassifier0.trimTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", inputMappedClassifier0.debugTipText());
      assertEquals(0, inputMappedClassifier0.graphType());
      assertTrue(capabilities0.hasDependencies());
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(4, testInstances0.getNumClasses());
      assertEquals(5, testInstances0.getNumAttributes());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertFalse(testInstances0.getNoClass());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(1, testInstances0.getNumString());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(4, instances0.numClasses());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(20, instances0.numInstances());
      assertTrue(instances0.checkForStringAttributes());
      assertEquals(5, instances0.numAttributes());
      assertEquals(20, instances0.size());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(4, instances0.classIndex());
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      
      String string0 = naiveBayesMultinomialText0.toString();
      assertNotNull(string0);
      assertEquals("The independent probability of a class\n--------------------------------------\nclass1\t9.0\nclass2\t4.0\nclass3\t7.0\nclass4\t4.0\n\nThe probability of a word given the class\n-----------------------------------------\n\tclass1\tclass2\tclass3\tclass4\t\nover\t2.718281828459045\t2.718281828459045\t7.38905609893065\t7.38905609893065\t\nthe\t7.38905609893065\t7.38905609893065\t2.718281828459045\t2.718281828459045\t\nThe\t7.38905609893065\t2.718281828459045\t7.38905609893065\t2.718281828459045\t\nquick\t7.38905609893065\t2.718281828459045\t7.38905609893065\t7.38905609893065\t\nlazy\t7.38905609893065\t2.718281828459045\t7.38905609893065\t2.718281828459045\t\njumps\t20.085536923187668\t7.38905609893065\t2.718281828459045\t2.718281828459045\t\nbrown\t7.38905609893065\t7.38905609893065\t2.718281828459045\t2.718281828459045\t\ndog\t7.38905609893065\t2.718281828459045\t7.38905609893065\t2.718281828459045\t\nfox\t2.718281828459045\t2.718281828459045\t7.38905609893065\t7.38905609893065\t\n", string0);
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      
      int[] intArray0 = new int[0];
      BinarySparseInstance binarySparseInstance0 = new BinarySparseInstance((-267.77445), intArray0, (-1));
      assertNotNull(binarySparseInstance0);
      assertArrayEquals(new int[] {}, intArray0);
      assertEquals((-267.77445), binarySparseInstance0.weight(), 0.01);
      assertEquals(0, binarySparseInstance0.numValues());
      assertEquals((-1), binarySparseInstance0.numAttributes());
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertEquals(0, intArray0.length);
      
      boolean boolean0 = FileSystemHandling.appendStringToFile((EvoSuiteFile) null, "Jgf");
      assertFalse(boolean0);
      
      Capabilities capabilities1 = inputMappedClassifier0.getCapabilities();
      assertNotSame(capabilities1, capabilities0);
      assertNotNull(capabilities1);
      assertFalse(capabilities1.equals((Object)capabilities0));
      assertEquals("", inputMappedClassifier0.getModelPath());
      assertFalse(inputMappedClassifier0.getDebug());
      assertTrue(inputMappedClassifier0.getTrim());
      assertFalse(inputMappedClassifier0.getSuppressMappingReport());
      assertEquals("Don't output a report of model-to-input mappings.", inputMappedClassifier0.suppressMappingReportTipText());
      assertEquals("Set the path from which to load a model. Loading occurs when the first test instance is received. Environment variables can be used in the supplied path.", inputMappedClassifier0.modelPathTipText());
      assertEquals("Ignore case when matching attribute names and nomina values.", inputMappedClassifier0.ignoreCaseForNamesTipText());
      assertEquals("The base classifier to be used.", inputMappedClassifier0.classifierTipText());
      assertEquals("Wrapper classifier that addresses incompatible training and test data by building a mapping between the training data that a classifier has been built with and the incoming test instances' structure. Model attributes that are not found in the incoming instances receive missing values, so do incoming nominal attribute values that the classifier has not seen before. A new classifier can be trained or an existing one loaded from a file.", inputMappedClassifier0.globalInfo());
      assertTrue(inputMappedClassifier0.getIgnoreCaseForNames());
      assertEquals("Trim white space from each end of attribute names and nominal values before matching.", inputMappedClassifier0.trimTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", inputMappedClassifier0.debugTipText());
      assertEquals(0, inputMappedClassifier0.graphType());
      assertEquals(0, capabilities1.getMinimumNumberInstances());
      assertTrue(capabilities1.hasDependencies());
      
      double[] doubleArray0 = naiveBayesMultinomialText0.distributionForInstance(binarySparseInstance0);
      assertNotNull(doubleArray0);
      assertArrayEquals(new int[] {}, intArray0);
      assertArrayEquals(new double[] {0.3750000000000001, 0.16666666666666669, 0.29166666666666663, 0.16666666666666669}, doubleArray0, 0.01);
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals((-267.77445), binarySparseInstance0.weight(), 0.01);
      assertEquals(0, binarySparseInstance0.numValues());
      assertEquals((-1), binarySparseInstance0.numAttributes());
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertEquals(0, intArray0.length);
      assertEquals(4, doubleArray0.length);
      
      naiveBayesMultinomialText0.pruneDictionary();
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      
      String[] stringArray1 = naiveBayesMultinomialText0.getOptions();
      assertNotSame(stringArray1, stringArray0);
      assertNotNull(stringArray1);
      assertFalse(stringArray1.equals((Object)stringArray0));
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(12, stringArray1.length);
      
      LinearNNSearch linearNNSearch0 = new LinearNNSearch(instances0);
      assertNotNull(linearNNSearch0);
      assertFalse(capabilities0.equals((Object)capabilities1));
      assertEquals("Whether to skip identical instances (with distance 0 to the target)", linearNNSearch0.skipIdenticalTipText());
      assertEquals("Whether to calculate performance statistics for the NN search or not", linearNNSearch0.measurePerformanceTipText());
      assertFalse(linearNNSearch0.getMeasurePerformance());
      assertFalse(linearNNSearch0.getSkipIdentical());
      assertEquals("Class implementing the brute force search algorithm for nearest neighbour search.", linearNNSearch0.globalInfo());
      assertEquals("The distance function to use for finding neighbours (default: weka.core.EuclideanDistance). ", linearNNSearch0.distanceFunctionTipText());
      assertEquals("", inputMappedClassifier0.getModelPath());
      assertFalse(inputMappedClassifier0.getDebug());
      assertTrue(inputMappedClassifier0.getTrim());
      assertFalse(inputMappedClassifier0.getSuppressMappingReport());
      assertEquals("Don't output a report of model-to-input mappings.", inputMappedClassifier0.suppressMappingReportTipText());
      assertEquals("Set the path from which to load a model. Loading occurs when the first test instance is received. Environment variables can be used in the supplied path.", inputMappedClassifier0.modelPathTipText());
      assertEquals("Ignore case when matching attribute names and nomina values.", inputMappedClassifier0.ignoreCaseForNamesTipText());
      assertEquals("The base classifier to be used.", inputMappedClassifier0.classifierTipText());
      assertEquals("Wrapper classifier that addresses incompatible training and test data by building a mapping between the training data that a classifier has been built with and the incoming test instances' structure. Model attributes that are not found in the incoming instances receive missing values, so do incoming nominal attribute values that the classifier has not seen before. A new classifier can be trained or an existing one loaded from a file.", inputMappedClassifier0.globalInfo());
      assertTrue(inputMappedClassifier0.getIgnoreCaseForNames());
      assertEquals("Trim white space from each end of attribute names and nominal values before matching.", inputMappedClassifier0.trimTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", inputMappedClassifier0.debugTipText());
      assertEquals(0, inputMappedClassifier0.graphType());
      assertTrue(capabilities0.hasDependencies());
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(4, testInstances0.getNumClasses());
      assertEquals(5, testInstances0.getNumAttributes());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertFalse(testInstances0.getNoClass());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(1, testInstances0.getNumString());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(4, instances0.numClasses());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(20, instances0.numInstances());
      assertTrue(instances0.checkForStringAttributes());
      assertEquals(5, instances0.numAttributes());
      assertEquals(20, instances0.size());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(4, instances0.classIndex());
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      
      DenseInstance denseInstance0 = (DenseInstance)linearNNSearch0.nearestNeighbour(binarySparseInstance0);
      assertNotSame(capabilities0, capabilities1);
      assertNotNull(denseInstance0);
      assertArrayEquals(new int[] {}, intArray0);
      assertFalse(capabilities0.equals((Object)capabilities1));
      assertEquals("Whether to skip identical instances (with distance 0 to the target)", linearNNSearch0.skipIdenticalTipText());
      assertEquals("Whether to calculate performance statistics for the NN search or not", linearNNSearch0.measurePerformanceTipText());
      assertFalse(linearNNSearch0.getMeasurePerformance());
      assertFalse(linearNNSearch0.getSkipIdentical());
      assertEquals("Class implementing the brute force search algorithm for nearest neighbour search.", linearNNSearch0.globalInfo());
      assertEquals("The distance function to use for finding neighbours (default: weka.core.EuclideanDistance). ", linearNNSearch0.distanceFunctionTipText());
      assertEquals(1.0, denseInstance0.weight(), 0.01);
      assertEquals(5, denseInstance0.numValues());
      assertEquals(4, denseInstance0.numClasses());
      assertEquals(4, denseInstance0.classIndex());
      assertEquals(5, denseInstance0.numAttributes());
      assertEquals("", inputMappedClassifier0.getModelPath());
      assertFalse(inputMappedClassifier0.getDebug());
      assertTrue(inputMappedClassifier0.getTrim());
      assertFalse(inputMappedClassifier0.getSuppressMappingReport());
      assertEquals("Don't output a report of model-to-input mappings.", inputMappedClassifier0.suppressMappingReportTipText());
      assertEquals("Set the path from which to load a model. Loading occurs when the first test instance is received. Environment variables can be used in the supplied path.", inputMappedClassifier0.modelPathTipText());
      assertEquals("Ignore case when matching attribute names and nomina values.", inputMappedClassifier0.ignoreCaseForNamesTipText());
      assertEquals("The base classifier to be used.", inputMappedClassifier0.classifierTipText());
      assertEquals("Wrapper classifier that addresses incompatible training and test data by building a mapping between the training data that a classifier has been built with and the incoming test instances' structure. Model attributes that are not found in the incoming instances receive missing values, so do incoming nominal attribute values that the classifier has not seen before. A new classifier can be trained or an existing one loaded from a file.", inputMappedClassifier0.globalInfo());
      assertTrue(inputMappedClassifier0.getIgnoreCaseForNames());
      assertEquals("Trim white space from each end of attribute names and nominal values before matching.", inputMappedClassifier0.trimTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", inputMappedClassifier0.debugTipText());
      assertEquals(0, inputMappedClassifier0.graphType());
      assertTrue(capabilities0.hasDependencies());
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(4, testInstances0.getNumClasses());
      assertEquals(5, testInstances0.getNumAttributes());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertFalse(testInstances0.getNoClass());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(1, testInstances0.getNumString());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(4, instances0.numClasses());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(20, instances0.numInstances());
      assertTrue(instances0.checkForStringAttributes());
      assertEquals(5, instances0.numAttributes());
      assertEquals(20, instances0.size());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(4, instances0.classIndex());
      assertEquals((-267.77445), binarySparseInstance0.weight(), 0.01);
      assertEquals(0, binarySparseInstance0.numValues());
      assertEquals((-1), binarySparseInstance0.numAttributes());
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertEquals(0, intArray0.length);
      
      double[] doubleArray1 = naiveBayesMultinomialText0.distributionForInstance(denseInstance0);
      assertNotSame(doubleArray1, doubleArray0);
      assertNotSame(capabilities0, capabilities1);
      assertNotNull(doubleArray1);
      assertArrayEquals(new double[] {0.3982300884955753, 0.12536873156342182, 0.351032448377581, 0.12536873156342182}, doubleArray1, 0.01);
      assertArrayEquals(new int[] {}, intArray0);
      assertFalse(doubleArray1.equals((Object)doubleArray0));
      assertFalse(capabilities0.equals((Object)capabilities1));
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("Whether to skip identical instances (with distance 0 to the target)", linearNNSearch0.skipIdenticalTipText());
      assertEquals("Whether to calculate performance statistics for the NN search or not", linearNNSearch0.measurePerformanceTipText());
      assertFalse(linearNNSearch0.getMeasurePerformance());
      assertFalse(linearNNSearch0.getSkipIdentical());
      assertEquals("Class implementing the brute force search algorithm for nearest neighbour search.", linearNNSearch0.globalInfo());
      assertEquals("The distance function to use for finding neighbours (default: weka.core.EuclideanDistance). ", linearNNSearch0.distanceFunctionTipText());
      assertEquals(1.0, denseInstance0.weight(), 0.01);
      assertEquals(5, denseInstance0.numValues());
      assertEquals(4, denseInstance0.numClasses());
      assertEquals(4, denseInstance0.classIndex());
      assertEquals(5, denseInstance0.numAttributes());
      assertEquals("", inputMappedClassifier0.getModelPath());
      assertFalse(inputMappedClassifier0.getDebug());
      assertTrue(inputMappedClassifier0.getTrim());
      assertFalse(inputMappedClassifier0.getSuppressMappingReport());
      assertEquals("Don't output a report of model-to-input mappings.", inputMappedClassifier0.suppressMappingReportTipText());
      assertEquals("Set the path from which to load a model. Loading occurs when the first test instance is received. Environment variables can be used in the supplied path.", inputMappedClassifier0.modelPathTipText());
      assertEquals("Ignore case when matching attribute names and nomina values.", inputMappedClassifier0.ignoreCaseForNamesTipText());
      assertEquals("The base classifier to be used.", inputMappedClassifier0.classifierTipText());
      assertEquals("Wrapper classifier that addresses incompatible training and test data by building a mapping between the training data that a classifier has been built with and the incoming test instances' structure. Model attributes that are not found in the incoming instances receive missing values, so do incoming nominal attribute values that the classifier has not seen before. A new classifier can be trained or an existing one loaded from a file.", inputMappedClassifier0.globalInfo());
      assertTrue(inputMappedClassifier0.getIgnoreCaseForNames());
      assertEquals("Trim white space from each end of attribute names and nominal values before matching.", inputMappedClassifier0.trimTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", inputMappedClassifier0.debugTipText());
      assertEquals(0, inputMappedClassifier0.graphType());
      assertTrue(capabilities0.hasDependencies());
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(4, testInstances0.getNumClasses());
      assertEquals(5, testInstances0.getNumAttributes());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertFalse(testInstances0.getNoClass());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(1, testInstances0.getNumString());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(4, instances0.numClasses());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(20, instances0.numInstances());
      assertTrue(instances0.checkForStringAttributes());
      assertEquals(5, instances0.numAttributes());
      assertEquals(20, instances0.size());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(4, instances0.classIndex());
      assertEquals((-267.77445), binarySparseInstance0.weight(), 0.01);
      assertEquals(0, binarySparseInstance0.numValues());
      assertEquals((-1), binarySparseInstance0.numAttributes());
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertEquals(4, doubleArray1.length);
      assertEquals(0, intArray0.length);
      
      naiveBayesMultinomialText0.updateClassifier((Instance) denseInstance0, false);
      assertNotSame(capabilities0, capabilities1);
      assertArrayEquals(new int[] {}, intArray0);
      assertFalse(capabilities0.equals((Object)capabilities1));
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("Whether to skip identical instances (with distance 0 to the target)", linearNNSearch0.skipIdenticalTipText());
      assertEquals("Whether to calculate performance statistics for the NN search or not", linearNNSearch0.measurePerformanceTipText());
      assertFalse(linearNNSearch0.getMeasurePerformance());
      assertFalse(linearNNSearch0.getSkipIdentical());
      assertEquals("Class implementing the brute force search algorithm for nearest neighbour search.", linearNNSearch0.globalInfo());
      assertEquals("The distance function to use for finding neighbours (default: weka.core.EuclideanDistance). ", linearNNSearch0.distanceFunctionTipText());
      assertEquals(1.0, denseInstance0.weight(), 0.01);
      assertEquals(5, denseInstance0.numValues());
      assertEquals(4, denseInstance0.numClasses());
      assertEquals(4, denseInstance0.classIndex());
      assertEquals(5, denseInstance0.numAttributes());
      assertEquals("", inputMappedClassifier0.getModelPath());
      assertFalse(inputMappedClassifier0.getDebug());
      assertTrue(inputMappedClassifier0.getTrim());
      assertFalse(inputMappedClassifier0.getSuppressMappingReport());
      assertEquals("Don't output a report of model-to-input mappings.", inputMappedClassifier0.suppressMappingReportTipText());
      assertEquals("Set the path from which to load a model. Loading occurs when the first test instance is received. Environment variables can be used in the supplied path.", inputMappedClassifier0.modelPathTipText());
      assertEquals("Ignore case when matching attribute names and nomina values.", inputMappedClassifier0.ignoreCaseForNamesTipText());
      assertEquals("The base classifier to be used.", inputMappedClassifier0.classifierTipText());
      assertEquals("Wrapper classifier that addresses incompatible training and test data by building a mapping between the training data that a classifier has been built with and the incoming test instances' structure. Model attributes that are not found in the incoming instances receive missing values, so do incoming nominal attribute values that the classifier has not seen before. A new classifier can be trained or an existing one loaded from a file.", inputMappedClassifier0.globalInfo());
      assertTrue(inputMappedClassifier0.getIgnoreCaseForNames());
      assertEquals("Trim white space from each end of attribute names and nominal values before matching.", inputMappedClassifier0.trimTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", inputMappedClassifier0.debugTipText());
      assertEquals(0, inputMappedClassifier0.graphType());
      assertTrue(capabilities0.hasDependencies());
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(4, testInstances0.getNumClasses());
      assertEquals(5, testInstances0.getNumAttributes());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertFalse(testInstances0.getNoClass());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(1, testInstances0.getNumString());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(4, instances0.numClasses());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(20, instances0.numInstances());
      assertTrue(instances0.checkForStringAttributes());
      assertEquals(5, instances0.numAttributes());
      assertEquals(20, instances0.size());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(4, instances0.classIndex());
      assertEquals((-267.77445), binarySparseInstance0.weight(), 0.01);
      assertEquals(0, binarySparseInstance0.numValues());
      assertEquals((-1), binarySparseInstance0.numAttributes());
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertEquals(0, intArray0.length);
      
      System.setCurrentTimeMillis(2050L);
  }

  /**
  //Test case number: 37
  /*Coverage entropy=3.332204510175204
  */
  @Test(timeout = 4000)
  public void test37()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      assertNotNull(naiveBayesMultinomialText0);
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      
      naiveBayesMultinomialText0.setNormalizeDocLength(true);
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertTrue(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      
      boolean boolean0 = FileSystemHandling.shouldAllThrowIOExceptions();
      assertTrue(boolean0);
      
      TestInstances testInstances0 = new TestInstances();
      assertNotNull(testInstances0);
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(20, testInstances0.getNumInstances());
      assertFalse(testInstances0.getNoClass());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      
      Instances instances0 = testInstances0.generate("-not-nominal-atts");
      assertNotNull(instances0);
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(20, testInstances0.getNumInstances());
      assertFalse(testInstances0.getNoClass());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(2, instances0.numClasses());
      assertEquals(20, instances0.size());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(2, instances0.numAttributes());
      assertEquals(20, instances0.numInstances());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals("Testdata", instances0.relationName());
      assertEquals(1, instances0.classIndex());
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      
      naiveBayesMultinomialText0.buildClassifier(instances0);
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertTrue(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(20, testInstances0.getNumInstances());
      assertFalse(testInstances0.getNoClass());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(2, instances0.numClasses());
      assertEquals(20, instances0.size());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(2, instances0.numAttributes());
      assertEquals(20, instances0.numInstances());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals("Testdata", instances0.relationName());
      assertEquals(1, instances0.classIndex());
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      
      WordTokenizer wordTokenizer0 = new WordTokenizer();
      assertNotNull(wordTokenizer0);
      assertEquals("Set of delimiter characters to use in tokenizing (\\r, \\n and \\t can be used for carriage-return, line-feed and tab)", wordTokenizer0.delimitersTipText());
      assertEquals(" \r\n\t.,;:'\"()?!", wordTokenizer0.getDelimiters());
      assertEquals("A simple tokenizer that is using the java.util.StringTokenizer class to tokenize the strings.", wordTokenizer0.globalInfo());
      
      NaiveBayesMultinomialText naiveBayesMultinomialText1 = new NaiveBayesMultinomialText();
      assertNotNull(naiveBayesMultinomialText1);
      assertFalse(naiveBayesMultinomialText1.equals((Object)naiveBayesMultinomialText0));
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText1.periodicPruningTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText1.globalInfo());
      assertFalse(naiveBayesMultinomialText1.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText1.getDebug());
      assertFalse(naiveBayesMultinomialText1.getUseStopList());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText1.useStopListTipText());
      assertEquals(0, naiveBayesMultinomialText1.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText1.useWordFrequenciesTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText1.LNormTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText1.stemmerTipText());
      assertFalse(naiveBayesMultinomialText1.getLowercaseTokens());
      assertEquals(3.0, naiveBayesMultinomialText1.getMinWordFrequency(), 0.01);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText1.normalizeDocLengthTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText1.lowercaseTokensTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText1.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText1.getNormalizeDocLength());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText1.tokenizerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText1.debugTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText1.normTipText());
      assertEquals(2.0, naiveBayesMultinomialText1.getLNorm(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText1.stopwordsTipText());
      assertEquals(1.0, naiveBayesMultinomialText1.getNorm(), 0.01);
      
      naiveBayesMultinomialText1.buildClassifier(instances0);
      assertNotSame(naiveBayesMultinomialText1, naiveBayesMultinomialText0);
      assertFalse(naiveBayesMultinomialText1.equals((Object)naiveBayesMultinomialText0));
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(20, testInstances0.getNumInstances());
      assertFalse(testInstances0.getNoClass());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(2, instances0.numClasses());
      assertEquals(20, instances0.size());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(2, instances0.numAttributes());
      assertEquals(20, instances0.numInstances());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals("Testdata", instances0.relationName());
      assertEquals(1, instances0.classIndex());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText1.periodicPruningTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText1.globalInfo());
      assertFalse(naiveBayesMultinomialText1.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText1.getDebug());
      assertFalse(naiveBayesMultinomialText1.getUseStopList());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText1.useStopListTipText());
      assertEquals(0, naiveBayesMultinomialText1.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText1.useWordFrequenciesTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText1.LNormTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText1.stemmerTipText());
      assertFalse(naiveBayesMultinomialText1.getLowercaseTokens());
      assertEquals(3.0, naiveBayesMultinomialText1.getMinWordFrequency(), 0.01);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText1.normalizeDocLengthTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText1.lowercaseTokensTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText1.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText1.getNormalizeDocLength());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText1.tokenizerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText1.debugTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText1.normTipText());
      assertEquals(2.0, naiveBayesMultinomialText1.getLNorm(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText1.stopwordsTipText());
      assertEquals(1.0, naiveBayesMultinomialText1.getNorm(), 0.01);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      
      naiveBayesMultinomialText1.pruneDictionary();
      assertNotSame(naiveBayesMultinomialText1, naiveBayesMultinomialText0);
      assertFalse(naiveBayesMultinomialText1.equals((Object)naiveBayesMultinomialText0));
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText1.periodicPruningTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText1.globalInfo());
      assertFalse(naiveBayesMultinomialText1.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText1.getDebug());
      assertFalse(naiveBayesMultinomialText1.getUseStopList());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText1.useStopListTipText());
      assertEquals(0, naiveBayesMultinomialText1.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText1.useWordFrequenciesTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText1.LNormTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText1.stemmerTipText());
      assertFalse(naiveBayesMultinomialText1.getLowercaseTokens());
      assertEquals(3.0, naiveBayesMultinomialText1.getMinWordFrequency(), 0.01);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText1.normalizeDocLengthTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText1.lowercaseTokensTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText1.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText1.getNormalizeDocLength());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText1.tokenizerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText1.debugTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText1.normTipText());
      assertEquals(2.0, naiveBayesMultinomialText1.getLNorm(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText1.stopwordsTipText());
      assertEquals(1.0, naiveBayesMultinomialText1.getNorm(), 0.01);
      
      naiveBayesMultinomialText1.setOptions(testInstances0.DEFAULT_WORDS);
      assertNotSame(naiveBayesMultinomialText1, naiveBayesMultinomialText0);
      assertFalse(naiveBayesMultinomialText1.equals((Object)naiveBayesMultinomialText0));
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(20, testInstances0.getNumInstances());
      assertFalse(testInstances0.getNoClass());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText1.periodicPruningTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText1.globalInfo());
      assertFalse(naiveBayesMultinomialText1.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText1.getDebug());
      assertFalse(naiveBayesMultinomialText1.getUseStopList());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText1.useStopListTipText());
      assertEquals(0, naiveBayesMultinomialText1.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText1.useWordFrequenciesTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText1.LNormTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText1.stemmerTipText());
      assertFalse(naiveBayesMultinomialText1.getLowercaseTokens());
      assertEquals(3.0, naiveBayesMultinomialText1.getMinWordFrequency(), 0.01);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText1.normalizeDocLengthTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText1.lowercaseTokensTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText1.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText1.getNormalizeDocLength());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText1.tokenizerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText1.debugTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText1.normTipText());
      assertEquals(2.0, naiveBayesMultinomialText1.getLNorm(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText1.stopwordsTipText());
      assertEquals(1.0, naiveBayesMultinomialText1.getNorm(), 0.01);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      
      naiveBayesMultinomialText1.setOptions(testInstances0.DEFAULT_WORDS);
      assertNotSame(naiveBayesMultinomialText1, naiveBayesMultinomialText0);
      assertFalse(naiveBayesMultinomialText1.equals((Object)naiveBayesMultinomialText0));
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(20, testInstances0.getNumInstances());
      assertFalse(testInstances0.getNoClass());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText1.periodicPruningTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText1.globalInfo());
      assertFalse(naiveBayesMultinomialText1.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText1.getDebug());
      assertFalse(naiveBayesMultinomialText1.getUseStopList());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText1.useStopListTipText());
      assertEquals(0, naiveBayesMultinomialText1.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText1.useWordFrequenciesTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText1.LNormTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText1.stemmerTipText());
      assertFalse(naiveBayesMultinomialText1.getLowercaseTokens());
      assertEquals(3.0, naiveBayesMultinomialText1.getMinWordFrequency(), 0.01);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText1.normalizeDocLengthTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText1.lowercaseTokensTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText1.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText1.getNormalizeDocLength());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText1.tokenizerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText1.debugTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText1.normTipText());
      assertEquals(2.0, naiveBayesMultinomialText1.getLNorm(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText1.stopwordsTipText());
      assertEquals(1.0, naiveBayesMultinomialText1.getNorm(), 0.01);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      
      String[] stringArray0 = naiveBayesMultinomialText1.getOptions();
      assertNotSame(naiveBayesMultinomialText1, naiveBayesMultinomialText0);
      assertNotNull(stringArray0);
      assertFalse(naiveBayesMultinomialText1.equals((Object)naiveBayesMultinomialText0));
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText1.periodicPruningTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText1.globalInfo());
      assertFalse(naiveBayesMultinomialText1.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText1.getDebug());
      assertFalse(naiveBayesMultinomialText1.getUseStopList());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText1.useStopListTipText());
      assertEquals(0, naiveBayesMultinomialText1.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText1.useWordFrequenciesTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText1.LNormTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText1.stemmerTipText());
      assertFalse(naiveBayesMultinomialText1.getLowercaseTokens());
      assertEquals(3.0, naiveBayesMultinomialText1.getMinWordFrequency(), 0.01);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText1.normalizeDocLengthTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText1.lowercaseTokensTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText1.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText1.getNormalizeDocLength());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText1.tokenizerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText1.debugTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText1.normTipText());
      assertEquals(2.0, naiveBayesMultinomialText1.getLNorm(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText1.stopwordsTipText());
      assertEquals(1.0, naiveBayesMultinomialText1.getNorm(), 0.01);
      assertEquals(12, stringArray0.length);
      
      LinearNNSearch linearNNSearch0 = new LinearNNSearch(instances0);
      assertNotNull(linearNNSearch0);
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(20, testInstances0.getNumInstances());
      assertFalse(testInstances0.getNoClass());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(2, instances0.numClasses());
      assertEquals(20, instances0.size());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(2, instances0.numAttributes());
      assertEquals(20, instances0.numInstances());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals("Testdata", instances0.relationName());
      assertEquals(1, instances0.classIndex());
      assertEquals("The distance function to use for finding neighbours (default: weka.core.EuclideanDistance). ", linearNNSearch0.distanceFunctionTipText());
      assertFalse(linearNNSearch0.getMeasurePerformance());
      assertFalse(linearNNSearch0.getSkipIdentical());
      assertEquals("Whether to skip identical instances (with distance 0 to the target)", linearNNSearch0.skipIdenticalTipText());
      assertEquals("Whether to calculate performance statistics for the NN search or not", linearNNSearch0.measurePerformanceTipText());
      assertEquals("Class implementing the brute force search algorithm for nearest neighbour search.", linearNNSearch0.globalInfo());
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      
      CoverTree coverTree0 = new CoverTree();
      assertNotNull(coverTree0);
      assertEquals(1.3, coverTree0.getBase(), 0.01);
      assertEquals("Whether to calculate performance statistics for the NN search or not", coverTree0.measurePerformanceTipText());
      assertEquals(0.0, coverTree0.measureNumLeaves(), 0.01);
      assertEquals(0.0, coverTree0.measureMaxDepth(), 0.01);
      assertEquals("The distance function to use for finding neighbours (default: weka.core.EuclideanDistance). ", coverTree0.distanceFunctionTipText());
      assertEquals(0.0, coverTree0.measureTreeSize(), 0.01);
      assertFalse(coverTree0.getMeasurePerformance());
      assertEquals("The base for the expansion constant.", coverTree0.baseTipText());
      
      MultiFilter multiFilter0 = new MultiFilter();
      assertNotNull(multiFilter0);
      assertFalse(multiFilter0.isOutputFormatDefined());
      assertFalse(multiFilter0.getDebug());
      assertEquals("The base filters to be used.", multiFilter0.filtersTipText());
      assertFalse(multiFilter0.isFirstBatchDone());
      assertEquals("Applies several filters successively. In case all supplied filters are StreamableFilters, it will act as a streamable one, too.", multiFilter0.globalInfo());
      assertFalse(multiFilter0.mayRemoveInstanceAfterFirstBatchDone());
      assertEquals("Turns on output of debugging information.", multiFilter0.debugTipText());
      assertTrue(multiFilter0.isNewBatch());
      
      Capabilities capabilities0 = multiFilter0.getCapabilities();
      assertNotNull(capabilities0);
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      assertFalse(capabilities0.hasDependencies());
      assertFalse(multiFilter0.isOutputFormatDefined());
      assertFalse(multiFilter0.getDebug());
      assertEquals("The base filters to be used.", multiFilter0.filtersTipText());
      assertFalse(multiFilter0.isFirstBatchDone());
      assertEquals("Applies several filters successively. In case all supplied filters are StreamableFilters, it will act as a streamable one, too.", multiFilter0.globalInfo());
      assertFalse(multiFilter0.mayRemoveInstanceAfterFirstBatchDone());
      assertEquals("Turns on output of debugging information.", multiFilter0.debugTipText());
      assertTrue(multiFilter0.isNewBatch());
      
      String[] stringArray1 = new String[8];
      assertFalse(stringArray1.equals((Object)stringArray0));
      
      stringArray1[0] = "@relation";
      stringArray1[1] = ".arff";
      stringArray1[2] = "";
      stringArray1[3] = "Min index (ints): ";
      stringArray1[4] = "@relation";
      stringArray1[5] = ".bsi";
      stringArray1[6] = "-tokenizer";
      stringArray1[7] = " ";
      try { 
        naiveBayesMultinomialText0.setOptions(stringArray1);
        fail("Expecting exception: Exception");
      
      } catch(Exception e) {
         //
         // Invalid tokenizer specification string
         //
         verifyException("weka.classifiers.bayes.NaiveBayesMultinomialText", e);
      }
  }

  /**
  //Test case number: 38
  /*Coverage entropy=2.5649493574615376
  */
  @Test(timeout = 4000)
  public void test38()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      assertNotNull(naiveBayesMultinomialText0);
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      
      LovinsStemmer lovinsStemmer0 = new LovinsStemmer();
      assertNotNull(lovinsStemmer0);
      
      naiveBayesMultinomialText0.setStemmer(lovinsStemmer0);
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      
      naiveBayesMultinomialText0.setUseWordFrequencies(true);
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertTrue(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      
      InputMappedClassifier inputMappedClassifier0 = new InputMappedClassifier();
      assertNotNull(inputMappedClassifier0);
      assertFalse(inputMappedClassifier0.getDebug());
      assertEquals("Set the path from which to load a model. Loading occurs when the first test instance is received. Environment variables can be used in the supplied path.", inputMappedClassifier0.modelPathTipText());
      assertTrue(inputMappedClassifier0.getTrim());
      assertEquals("", inputMappedClassifier0.getModelPath());
      assertEquals("Don't output a report of model-to-input mappings.", inputMappedClassifier0.suppressMappingReportTipText());
      assertEquals(0, inputMappedClassifier0.graphType());
      assertEquals("Trim white space from each end of attribute names and nominal values before matching.", inputMappedClassifier0.trimTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", inputMappedClassifier0.debugTipText());
      assertTrue(inputMappedClassifier0.getIgnoreCaseForNames());
      assertEquals("Ignore case when matching attribute names and nomina values.", inputMappedClassifier0.ignoreCaseForNamesTipText());
      assertEquals("The base classifier to be used.", inputMappedClassifier0.classifierTipText());
      assertFalse(inputMappedClassifier0.getSuppressMappingReport());
      assertEquals("Wrapper classifier that addresses incompatible training and test data by building a mapping between the training data that a classifier has been built with and the incoming test instances' structure. Model attributes that are not found in the incoming instances receive missing values, so do incoming nominal attribute values that the classifier has not seen before. A new classifier can be trained or an existing one loaded from a file.", inputMappedClassifier0.globalInfo());
      
      Capabilities capabilities0 = inputMappedClassifier0.getCapabilities();
      assertNotNull(capabilities0);
      assertFalse(inputMappedClassifier0.getDebug());
      assertEquals("Set the path from which to load a model. Loading occurs when the first test instance is received. Environment variables can be used in the supplied path.", inputMappedClassifier0.modelPathTipText());
      assertTrue(inputMappedClassifier0.getTrim());
      assertEquals("", inputMappedClassifier0.getModelPath());
      assertEquals("Don't output a report of model-to-input mappings.", inputMappedClassifier0.suppressMappingReportTipText());
      assertEquals(0, inputMappedClassifier0.graphType());
      assertEquals("Trim white space from each end of attribute names and nominal values before matching.", inputMappedClassifier0.trimTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", inputMappedClassifier0.debugTipText());
      assertTrue(inputMappedClassifier0.getIgnoreCaseForNames());
      assertEquals("Ignore case when matching attribute names and nomina values.", inputMappedClassifier0.ignoreCaseForNamesTipText());
      assertEquals("The base classifier to be used.", inputMappedClassifier0.classifierTipText());
      assertFalse(inputMappedClassifier0.getSuppressMappingReport());
      assertEquals("Wrapper classifier that addresses incompatible training and test data by building a mapping between the training data that a classifier has been built with and the incoming test instances' structure. Model attributes that are not found in the incoming instances receive missing values, so do incoming nominal attribute values that the classifier has not seen before. A new classifier can be trained or an existing one loaded from a file.", inputMappedClassifier0.globalInfo());
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      assertTrue(capabilities0.hasDependencies());
      
      TestInstances testInstances0 = TestInstances.forCapabilities(capabilities0);
      assertNotNull(testInstances0);
      assertFalse(inputMappedClassifier0.getDebug());
      assertEquals("Set the path from which to load a model. Loading occurs when the first test instance is received. Environment variables can be used in the supplied path.", inputMappedClassifier0.modelPathTipText());
      assertTrue(inputMappedClassifier0.getTrim());
      assertEquals("", inputMappedClassifier0.getModelPath());
      assertEquals("Don't output a report of model-to-input mappings.", inputMappedClassifier0.suppressMappingReportTipText());
      assertEquals(0, inputMappedClassifier0.graphType());
      assertEquals("Trim white space from each end of attribute names and nominal values before matching.", inputMappedClassifier0.trimTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", inputMappedClassifier0.debugTipText());
      assertTrue(inputMappedClassifier0.getIgnoreCaseForNames());
      assertEquals("Ignore case when matching attribute names and nomina values.", inputMappedClassifier0.ignoreCaseForNamesTipText());
      assertEquals("The base classifier to be used.", inputMappedClassifier0.classifierTipText());
      assertFalse(inputMappedClassifier0.getSuppressMappingReport());
      assertEquals("Wrapper classifier that addresses incompatible training and test data by building a mapping between the training data that a classifier has been built with and the incoming test instances' structure. Model attributes that are not found in the incoming instances receive missing values, so do incoming nominal attribute values that the classifier has not seen before. A new classifier can be trained or an existing one loaded from a file.", inputMappedClassifier0.globalInfo());
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      assertTrue(capabilities0.hasDependencies());
      assertEquals(1, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertFalse(testInstances0.getNoClass());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(1, testInstances0.getClassType());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(4, testInstances0.getNumClasses());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(5, testInstances0.getNumAttributes());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(1, testInstances0.getNumString());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      
      Instances instances0 = testInstances0.generate();
      assertNotNull(instances0);
      assertFalse(inputMappedClassifier0.getDebug());
      assertEquals("Set the path from which to load a model. Loading occurs when the first test instance is received. Environment variables can be used in the supplied path.", inputMappedClassifier0.modelPathTipText());
      assertTrue(inputMappedClassifier0.getTrim());
      assertEquals("", inputMappedClassifier0.getModelPath());
      assertEquals("Don't output a report of model-to-input mappings.", inputMappedClassifier0.suppressMappingReportTipText());
      assertEquals(0, inputMappedClassifier0.graphType());
      assertEquals("Trim white space from each end of attribute names and nominal values before matching.", inputMappedClassifier0.trimTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", inputMappedClassifier0.debugTipText());
      assertTrue(inputMappedClassifier0.getIgnoreCaseForNames());
      assertEquals("Ignore case when matching attribute names and nomina values.", inputMappedClassifier0.ignoreCaseForNamesTipText());
      assertEquals("The base classifier to be used.", inputMappedClassifier0.classifierTipText());
      assertFalse(inputMappedClassifier0.getSuppressMappingReport());
      assertEquals("Wrapper classifier that addresses incompatible training and test data by building a mapping between the training data that a classifier has been built with and the incoming test instances' structure. Model attributes that are not found in the incoming instances receive missing values, so do incoming nominal attribute values that the classifier has not seen before. A new classifier can be trained or an existing one loaded from a file.", inputMappedClassifier0.globalInfo());
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      assertTrue(capabilities0.hasDependencies());
      assertEquals(1, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertFalse(testInstances0.getNoClass());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(1, testInstances0.getClassType());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(4, testInstances0.getNumClasses());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(5, testInstances0.getNumAttributes());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(1, testInstances0.getNumString());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(20, instances0.numInstances());
      assertEquals(5, instances0.numAttributes());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(20, instances0.size());
      assertEquals(4, instances0.classIndex());
      assertEquals(4, instances0.numClasses());
      assertEquals("Testdata", instances0.relationName());
      assertTrue(instances0.checkForStringAttributes());
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      
      IBk iBk0 = new IBk();
      assertNotNull(iBk0);
      assertEquals("Gets the maximum number of instances allowed in the training pool. The addition of new instances above this value will result in old instances being removed. A value of 0 signifies no limit to the number of training instances.", iBk0.windowSizeTipText());
      assertEquals("The number of neighbours to use.", iBk0.KNNTipText());
      assertEquals(0, iBk0.getWindowSize());
      assertEquals("If set to true, classifier may output additional info to the console.", iBk0.debugTipText());
      assertEquals("Whether the mean squared error is used rather than mean absolute error when doing cross-validation for regression problems.", iBk0.meanSquaredTipText());
      assertEquals("The nearest neighbour search algorithm to use (Default: weka.core.neighboursearch.LinearNNSearch).", iBk0.nearestNeighbourSearchAlgorithmTipText());
      assertFalse(iBk0.getCrossValidate());
      assertFalse(iBk0.getDebug());
      assertEquals("Gets the distance weighting method used.", iBk0.distanceWeightingTipText());
      assertFalse(iBk0.getMeanSquared());
      assertEquals(1, iBk0.getKNN());
      assertEquals("Whether hold-one-out cross-validation will be used to select the best k value.", iBk0.crossValidateTipText());
      assertEquals(1, IBk.WEIGHT_NONE);
      assertEquals(2, IBk.WEIGHT_INVERSE);
      assertEquals(4, IBk.WEIGHT_SIMILARITY);
      
      double[] doubleArray0 = new double[5];
      doubleArray0[0] = (double) (-1);
      doubleArray0[2] = (double) 4;
      doubleArray0[4] = (double) 4;
      Instances instances1 = iBk0.pruneToK(instances0, doubleArray0, (-1));
      assertNotSame(instances0, instances1);
      assertNotSame(instances1, instances0);
      assertNotNull(instances1);
      assertArrayEquals(new double[] {(-1.0), 0.0, 4.0, 0.0, 4.0}, doubleArray0, 0.01);
      assertFalse(instances1.equals((Object)instances0));
      assertFalse(inputMappedClassifier0.getDebug());
      assertEquals("Set the path from which to load a model. Loading occurs when the first test instance is received. Environment variables can be used in the supplied path.", inputMappedClassifier0.modelPathTipText());
      assertTrue(inputMappedClassifier0.getTrim());
      assertEquals("", inputMappedClassifier0.getModelPath());
      assertEquals("Don't output a report of model-to-input mappings.", inputMappedClassifier0.suppressMappingReportTipText());
      assertEquals(0, inputMappedClassifier0.graphType());
      assertEquals("Trim white space from each end of attribute names and nominal values before matching.", inputMappedClassifier0.trimTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", inputMappedClassifier0.debugTipText());
      assertTrue(inputMappedClassifier0.getIgnoreCaseForNames());
      assertEquals("Ignore case when matching attribute names and nomina values.", inputMappedClassifier0.ignoreCaseForNamesTipText());
      assertEquals("The base classifier to be used.", inputMappedClassifier0.classifierTipText());
      assertFalse(inputMappedClassifier0.getSuppressMappingReport());
      assertEquals("Wrapper classifier that addresses incompatible training and test data by building a mapping between the training data that a classifier has been built with and the incoming test instances' structure. Model attributes that are not found in the incoming instances receive missing values, so do incoming nominal attribute values that the classifier has not seen before. A new classifier can be trained or an existing one loaded from a file.", inputMappedClassifier0.globalInfo());
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      assertTrue(capabilities0.hasDependencies());
      assertEquals(1, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertFalse(testInstances0.getNoClass());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(1, testInstances0.getClassType());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(4, testInstances0.getNumClasses());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(5, testInstances0.getNumAttributes());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(1, testInstances0.getNumString());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(20, instances0.numInstances());
      assertEquals(5, instances0.numAttributes());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(20, instances0.size());
      assertEquals(4, instances0.classIndex());
      assertEquals(4, instances0.numClasses());
      assertEquals("Testdata", instances0.relationName());
      assertTrue(instances0.checkForStringAttributes());
      assertEquals("Gets the maximum number of instances allowed in the training pool. The addition of new instances above this value will result in old instances being removed. A value of 0 signifies no limit to the number of training instances.", iBk0.windowSizeTipText());
      assertEquals("The number of neighbours to use.", iBk0.KNNTipText());
      assertEquals(0, iBk0.getWindowSize());
      assertEquals("If set to true, classifier may output additional info to the console.", iBk0.debugTipText());
      assertEquals("Whether the mean squared error is used rather than mean absolute error when doing cross-validation for regression problems.", iBk0.meanSquaredTipText());
      assertEquals("The nearest neighbour search algorithm to use (Default: weka.core.neighboursearch.LinearNNSearch).", iBk0.nearestNeighbourSearchAlgorithmTipText());
      assertFalse(iBk0.getCrossValidate());
      assertFalse(iBk0.getDebug());
      assertEquals("Gets the distance weighting method used.", iBk0.distanceWeightingTipText());
      assertFalse(iBk0.getMeanSquared());
      assertEquals(1, iBk0.getKNN());
      assertEquals("Whether hold-one-out cross-validation will be used to select the best k value.", iBk0.crossValidateTipText());
      assertEquals(4, instances1.classIndex());
      assertEquals("Testdata", instances1.relationName());
      assertEquals(4, instances1.numClasses());
      assertTrue(instances1.checkForStringAttributes());
      assertEquals(5, instances1.numAttributes());
      assertEquals(1, instances1.numInstances());
      assertEquals(1, instances1.size());
      assertEquals(1.0, instances1.sumOfWeights(), 0.01);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals(1, IBk.WEIGHT_NONE);
      assertEquals(2, IBk.WEIGHT_INVERSE);
      assertEquals(4, IBk.WEIGHT_SIMILARITY);
      assertEquals(5, doubleArray0.length);
      
      naiveBayesMultinomialText0.buildClassifier(instances1);
      assertNotSame(instances0, instances1);
      assertNotSame(instances1, instances0);
      assertArrayEquals(new double[] {(-1.0), 0.0, 4.0, 0.0, 4.0}, doubleArray0, 0.01);
      assertFalse(instances0.equals((Object)instances1));
      assertFalse(instances1.equals((Object)instances0));
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertTrue(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertFalse(inputMappedClassifier0.getDebug());
      assertEquals("Set the path from which to load a model. Loading occurs when the first test instance is received. Environment variables can be used in the supplied path.", inputMappedClassifier0.modelPathTipText());
      assertTrue(inputMappedClassifier0.getTrim());
      assertEquals("", inputMappedClassifier0.getModelPath());
      assertEquals("Don't output a report of model-to-input mappings.", inputMappedClassifier0.suppressMappingReportTipText());
      assertEquals(0, inputMappedClassifier0.graphType());
      assertEquals("Trim white space from each end of attribute names and nominal values before matching.", inputMappedClassifier0.trimTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", inputMappedClassifier0.debugTipText());
      assertTrue(inputMappedClassifier0.getIgnoreCaseForNames());
      assertEquals("Ignore case when matching attribute names and nomina values.", inputMappedClassifier0.ignoreCaseForNamesTipText());
      assertEquals("The base classifier to be used.", inputMappedClassifier0.classifierTipText());
      assertFalse(inputMappedClassifier0.getSuppressMappingReport());
      assertEquals("Wrapper classifier that addresses incompatible training and test data by building a mapping between the training data that a classifier has been built with and the incoming test instances' structure. Model attributes that are not found in the incoming instances receive missing values, so do incoming nominal attribute values that the classifier has not seen before. A new classifier can be trained or an existing one loaded from a file.", inputMappedClassifier0.globalInfo());
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      assertTrue(capabilities0.hasDependencies());
      assertEquals(1, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertFalse(testInstances0.getNoClass());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(1, testInstances0.getClassType());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(4, testInstances0.getNumClasses());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(5, testInstances0.getNumAttributes());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(1, testInstances0.getNumString());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(20, instances0.numInstances());
      assertEquals(5, instances0.numAttributes());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(20, instances0.size());
      assertEquals(4, instances0.classIndex());
      assertEquals(4, instances0.numClasses());
      assertEquals("Testdata", instances0.relationName());
      assertTrue(instances0.checkForStringAttributes());
      assertEquals("Gets the maximum number of instances allowed in the training pool. The addition of new instances above this value will result in old instances being removed. A value of 0 signifies no limit to the number of training instances.", iBk0.windowSizeTipText());
      assertEquals("The number of neighbours to use.", iBk0.KNNTipText());
      assertEquals(0, iBk0.getWindowSize());
      assertEquals("If set to true, classifier may output additional info to the console.", iBk0.debugTipText());
      assertEquals("Whether the mean squared error is used rather than mean absolute error when doing cross-validation for regression problems.", iBk0.meanSquaredTipText());
      assertEquals("The nearest neighbour search algorithm to use (Default: weka.core.neighboursearch.LinearNNSearch).", iBk0.nearestNeighbourSearchAlgorithmTipText());
      assertFalse(iBk0.getCrossValidate());
      assertFalse(iBk0.getDebug());
      assertEquals("Gets the distance weighting method used.", iBk0.distanceWeightingTipText());
      assertFalse(iBk0.getMeanSquared());
      assertEquals(1, iBk0.getKNN());
      assertEquals("Whether hold-one-out cross-validation will be used to select the best k value.", iBk0.crossValidateTipText());
      assertEquals(4, instances1.classIndex());
      assertEquals("Testdata", instances1.relationName());
      assertEquals(4, instances1.numClasses());
      assertTrue(instances1.checkForStringAttributes());
      assertEquals(5, instances1.numAttributes());
      assertEquals(1, instances1.numInstances());
      assertEquals(1, instances1.size());
      assertEquals(1.0, instances1.sumOfWeights(), 0.01);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals(1, IBk.WEIGHT_NONE);
      assertEquals(2, IBk.WEIGHT_INVERSE);
      assertEquals(4, IBk.WEIGHT_SIMILARITY);
      assertEquals(5, doubleArray0.length);
      
      String string0 = naiveBayesMultinomialText0.toString();
      assertNotNull(string0);
      assertEquals("The independent probability of a class\n--------------------------------------\nclass1\t2.0\nclass2\t1.0\nclass3\t1.0\nclass4\t1.0\n\nThe probability of a word given the class\n-----------------------------------------\n\tclass1\tclass2\tclass3\tclass4\t\njump\t22026.465794806718\t2.718281828459045\t2.718281828459045\t2.718281828459045\t\n", string0);
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertTrue(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      
      naiveBayesMultinomialText0.pruneDictionary();
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertTrue(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      
      String string1 = naiveBayesMultinomialText0.toString();
      assertNotNull(string1);
      assertEquals("The independent probability of a class\n--------------------------------------\nclass1\t2.0\nclass2\t1.0\nclass3\t1.0\nclass4\t1.0\n\nThe probability of a word given the class\n-----------------------------------------\n\tclass1\tclass2\tclass3\tclass4\t\njump\t22026.465794806718\t2.718281828459045\t2.718281828459045\t2.718281828459045\t\n", string1);
      assertTrue(string1.equals((Object)string0));
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertTrue(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      
      LinearNNSearch linearNNSearch0 = new LinearNNSearch(instances1);
      assertNotNull(linearNNSearch0);
      assertArrayEquals(new double[] {(-1.0), 0.0, 4.0, 0.0, 4.0}, doubleArray0, 0.01);
      assertFalse(instances0.equals((Object)instances1));
      assertFalse(instances1.equals((Object)instances0));
      assertEquals("Whether to skip identical instances (with distance 0 to the target)", linearNNSearch0.skipIdenticalTipText());
      assertEquals("Whether to calculate performance statistics for the NN search or not", linearNNSearch0.measurePerformanceTipText());
      assertEquals("Class implementing the brute force search algorithm for nearest neighbour search.", linearNNSearch0.globalInfo());
      assertEquals("The distance function to use for finding neighbours (default: weka.core.EuclideanDistance). ", linearNNSearch0.distanceFunctionTipText());
      assertFalse(linearNNSearch0.getMeasurePerformance());
      assertFalse(linearNNSearch0.getSkipIdentical());
      assertFalse(inputMappedClassifier0.getDebug());
      assertEquals("Set the path from which to load a model. Loading occurs when the first test instance is received. Environment variables can be used in the supplied path.", inputMappedClassifier0.modelPathTipText());
      assertTrue(inputMappedClassifier0.getTrim());
      assertEquals("", inputMappedClassifier0.getModelPath());
      assertEquals("Don't output a report of model-to-input mappings.", inputMappedClassifier0.suppressMappingReportTipText());
      assertEquals(0, inputMappedClassifier0.graphType());
      assertEquals("Trim white space from each end of attribute names and nominal values before matching.", inputMappedClassifier0.trimTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", inputMappedClassifier0.debugTipText());
      assertTrue(inputMappedClassifier0.getIgnoreCaseForNames());
      assertEquals("Ignore case when matching attribute names and nomina values.", inputMappedClassifier0.ignoreCaseForNamesTipText());
      assertEquals("The base classifier to be used.", inputMappedClassifier0.classifierTipText());
      assertFalse(inputMappedClassifier0.getSuppressMappingReport());
      assertEquals("Wrapper classifier that addresses incompatible training and test data by building a mapping between the training data that a classifier has been built with and the incoming test instances' structure. Model attributes that are not found in the incoming instances receive missing values, so do incoming nominal attribute values that the classifier has not seen before. A new classifier can be trained or an existing one loaded from a file.", inputMappedClassifier0.globalInfo());
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      assertTrue(capabilities0.hasDependencies());
      assertEquals(1, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertFalse(testInstances0.getNoClass());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(1, testInstances0.getClassType());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(4, testInstances0.getNumClasses());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(5, testInstances0.getNumAttributes());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(1, testInstances0.getNumString());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(20, instances0.numInstances());
      assertEquals(5, instances0.numAttributes());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(20, instances0.size());
      assertEquals(4, instances0.classIndex());
      assertEquals(4, instances0.numClasses());
      assertEquals("Testdata", instances0.relationName());
      assertTrue(instances0.checkForStringAttributes());
      assertEquals("Gets the maximum number of instances allowed in the training pool. The addition of new instances above this value will result in old instances being removed. A value of 0 signifies no limit to the number of training instances.", iBk0.windowSizeTipText());
      assertEquals("The number of neighbours to use.", iBk0.KNNTipText());
      assertEquals(0, iBk0.getWindowSize());
      assertEquals("If set to true, classifier may output additional info to the console.", iBk0.debugTipText());
      assertEquals("Whether the mean squared error is used rather than mean absolute error when doing cross-validation for regression problems.", iBk0.meanSquaredTipText());
      assertEquals("The nearest neighbour search algorithm to use (Default: weka.core.neighboursearch.LinearNNSearch).", iBk0.nearestNeighbourSearchAlgorithmTipText());
      assertFalse(iBk0.getCrossValidate());
      assertFalse(iBk0.getDebug());
      assertEquals("Gets the distance weighting method used.", iBk0.distanceWeightingTipText());
      assertFalse(iBk0.getMeanSquared());
      assertEquals(1, iBk0.getKNN());
      assertEquals("Whether hold-one-out cross-validation will be used to select the best k value.", iBk0.crossValidateTipText());
      assertEquals(4, instances1.classIndex());
      assertEquals("Testdata", instances1.relationName());
      assertEquals(4, instances1.numClasses());
      assertTrue(instances1.checkForStringAttributes());
      assertEquals(5, instances1.numAttributes());
      assertEquals(1, instances1.numInstances());
      assertEquals(1, instances1.size());
      assertEquals(1.0, instances1.sumOfWeights(), 0.01);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals(1, IBk.WEIGHT_NONE);
      assertEquals(2, IBk.WEIGHT_INVERSE);
      assertEquals(4, IBk.WEIGHT_SIMILARITY);
      assertEquals(5, doubleArray0.length);
      
      BinarySparseInstance binarySparseInstance0 = new BinarySparseInstance(4);
      assertNotNull(binarySparseInstance0);
      assertEquals(4, binarySparseInstance0.numValues());
      assertEquals(1.0, binarySparseInstance0.weight(), 0.01);
      assertEquals(4, binarySparseInstance0.numAttributes());
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      
      DenseInstance denseInstance0 = (DenseInstance)linearNNSearch0.nearestNeighbour(binarySparseInstance0);
      assertNotSame(instances0, instances1);
      assertNotSame(instances1, instances0);
      assertNotNull(denseInstance0);
      assertArrayEquals(new double[] {(-1.0), 0.0, 4.0, 0.0, 4.0}, doubleArray0, 0.01);
      assertFalse(instances0.equals((Object)instances1));
      assertFalse(instances1.equals((Object)instances0));
      assertEquals("Whether to skip identical instances (with distance 0 to the target)", linearNNSearch0.skipIdenticalTipText());
      assertEquals("Whether to calculate performance statistics for the NN search or not", linearNNSearch0.measurePerformanceTipText());
      assertEquals("Class implementing the brute force search algorithm for nearest neighbour search.", linearNNSearch0.globalInfo());
      assertEquals("The distance function to use for finding neighbours (default: weka.core.EuclideanDistance). ", linearNNSearch0.distanceFunctionTipText());
      assertFalse(linearNNSearch0.getMeasurePerformance());
      assertFalse(linearNNSearch0.getSkipIdentical());
      assertEquals(4, binarySparseInstance0.numValues());
      assertEquals(1.0, binarySparseInstance0.weight(), 0.01);
      assertEquals(4, binarySparseInstance0.numAttributes());
      assertEquals(4, denseInstance0.classIndex());
      assertEquals(5, denseInstance0.numAttributes());
      assertEquals(4, denseInstance0.numClasses());
      assertEquals(1.0, denseInstance0.weight(), 0.01);
      assertEquals(5, denseInstance0.numValues());
      assertFalse(inputMappedClassifier0.getDebug());
      assertEquals("Set the path from which to load a model. Loading occurs when the first test instance is received. Environment variables can be used in the supplied path.", inputMappedClassifier0.modelPathTipText());
      assertTrue(inputMappedClassifier0.getTrim());
      assertEquals("", inputMappedClassifier0.getModelPath());
      assertEquals("Don't output a report of model-to-input mappings.", inputMappedClassifier0.suppressMappingReportTipText());
      assertEquals(0, inputMappedClassifier0.graphType());
      assertEquals("Trim white space from each end of attribute names and nominal values before matching.", inputMappedClassifier0.trimTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", inputMappedClassifier0.debugTipText());
      assertTrue(inputMappedClassifier0.getIgnoreCaseForNames());
      assertEquals("Ignore case when matching attribute names and nomina values.", inputMappedClassifier0.ignoreCaseForNamesTipText());
      assertEquals("The base classifier to be used.", inputMappedClassifier0.classifierTipText());
      assertFalse(inputMappedClassifier0.getSuppressMappingReport());
      assertEquals("Wrapper classifier that addresses incompatible training and test data by building a mapping between the training data that a classifier has been built with and the incoming test instances' structure. Model attributes that are not found in the incoming instances receive missing values, so do incoming nominal attribute values that the classifier has not seen before. A new classifier can be trained or an existing one loaded from a file.", inputMappedClassifier0.globalInfo());
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      assertTrue(capabilities0.hasDependencies());
      assertEquals(1, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertFalse(testInstances0.getNoClass());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(1, testInstances0.getClassType());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(4, testInstances0.getNumClasses());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(5, testInstances0.getNumAttributes());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(1, testInstances0.getNumString());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(20, instances0.numInstances());
      assertEquals(5, instances0.numAttributes());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(20, instances0.size());
      assertEquals(4, instances0.classIndex());
      assertEquals(4, instances0.numClasses());
      assertEquals("Testdata", instances0.relationName());
      assertTrue(instances0.checkForStringAttributes());
      assertEquals("Gets the maximum number of instances allowed in the training pool. The addition of new instances above this value will result in old instances being removed. A value of 0 signifies no limit to the number of training instances.", iBk0.windowSizeTipText());
      assertEquals("The number of neighbours to use.", iBk0.KNNTipText());
      assertEquals(0, iBk0.getWindowSize());
      assertEquals("If set to true, classifier may output additional info to the console.", iBk0.debugTipText());
      assertEquals("Whether the mean squared error is used rather than mean absolute error when doing cross-validation for regression problems.", iBk0.meanSquaredTipText());
      assertEquals("The nearest neighbour search algorithm to use (Default: weka.core.neighboursearch.LinearNNSearch).", iBk0.nearestNeighbourSearchAlgorithmTipText());
      assertFalse(iBk0.getCrossValidate());
      assertFalse(iBk0.getDebug());
      assertEquals("Gets the distance weighting method used.", iBk0.distanceWeightingTipText());
      assertFalse(iBk0.getMeanSquared());
      assertEquals(1, iBk0.getKNN());
      assertEquals("Whether hold-one-out cross-validation will be used to select the best k value.", iBk0.crossValidateTipText());
      assertEquals(4, instances1.classIndex());
      assertEquals("Testdata", instances1.relationName());
      assertEquals(4, instances1.numClasses());
      assertTrue(instances1.checkForStringAttributes());
      assertEquals(5, instances1.numAttributes());
      assertEquals(1, instances1.numInstances());
      assertEquals(1, instances1.size());
      assertEquals(1.0, instances1.sumOfWeights(), 0.01);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals(1, IBk.WEIGHT_NONE);
      assertEquals(2, IBk.WEIGHT_INVERSE);
      assertEquals(4, IBk.WEIGHT_SIMILARITY);
      assertEquals(5, doubleArray0.length);
      
      double[] doubleArray1 = naiveBayesMultinomialText0.distributionForInstance(denseInstance0);
      assertNotSame(doubleArray1, doubleArray0);
      assertNotSame(instances0, instances1);
      assertNotSame(doubleArray0, doubleArray1);
      assertNotSame(instances1, instances0);
      assertNotNull(doubleArray1);
      assertArrayEquals(new double[] {0.3999999999999996, 0.20000000000000015, 0.20000000000000015, 0.20000000000000015}, doubleArray1, 0.01);
      assertArrayEquals(new double[] {(-1.0), 0.0, 4.0, 0.0, 4.0}, doubleArray0, 0.01);
      assertFalse(doubleArray1.equals((Object)doubleArray0));
      assertFalse(instances0.equals((Object)instances1));
      assertFalse(instances1.equals((Object)instances0));
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertTrue(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("Whether to skip identical instances (with distance 0 to the target)", linearNNSearch0.skipIdenticalTipText());
      assertEquals("Whether to calculate performance statistics for the NN search or not", linearNNSearch0.measurePerformanceTipText());
      assertEquals("Class implementing the brute force search algorithm for nearest neighbour search.", linearNNSearch0.globalInfo());
      assertEquals("The distance function to use for finding neighbours (default: weka.core.EuclideanDistance). ", linearNNSearch0.distanceFunctionTipText());
      assertFalse(linearNNSearch0.getMeasurePerformance());
      assertFalse(linearNNSearch0.getSkipIdentical());
      assertEquals(4, binarySparseInstance0.numValues());
      assertEquals(1.0, binarySparseInstance0.weight(), 0.01);
      assertEquals(4, binarySparseInstance0.numAttributes());
      assertEquals(4, denseInstance0.classIndex());
      assertEquals(5, denseInstance0.numAttributes());
      assertEquals(4, denseInstance0.numClasses());
      assertEquals(1.0, denseInstance0.weight(), 0.01);
      assertEquals(5, denseInstance0.numValues());
      assertFalse(inputMappedClassifier0.getDebug());
      assertEquals("Set the path from which to load a model. Loading occurs when the first test instance is received. Environment variables can be used in the supplied path.", inputMappedClassifier0.modelPathTipText());
      assertTrue(inputMappedClassifier0.getTrim());
      assertEquals("", inputMappedClassifier0.getModelPath());
      assertEquals("Don't output a report of model-to-input mappings.", inputMappedClassifier0.suppressMappingReportTipText());
      assertEquals(0, inputMappedClassifier0.graphType());
      assertEquals("Trim white space from each end of attribute names and nominal values before matching.", inputMappedClassifier0.trimTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", inputMappedClassifier0.debugTipText());
      assertTrue(inputMappedClassifier0.getIgnoreCaseForNames());
      assertEquals("Ignore case when matching attribute names and nomina values.", inputMappedClassifier0.ignoreCaseForNamesTipText());
      assertEquals("The base classifier to be used.", inputMappedClassifier0.classifierTipText());
      assertFalse(inputMappedClassifier0.getSuppressMappingReport());
      assertEquals("Wrapper classifier that addresses incompatible training and test data by building a mapping between the training data that a classifier has been built with and the incoming test instances' structure. Model attributes that are not found in the incoming instances receive missing values, so do incoming nominal attribute values that the classifier has not seen before. A new classifier can be trained or an existing one loaded from a file.", inputMappedClassifier0.globalInfo());
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      assertTrue(capabilities0.hasDependencies());
      assertEquals(1, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertFalse(testInstances0.getNoClass());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(1, testInstances0.getClassType());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(4, testInstances0.getNumClasses());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(5, testInstances0.getNumAttributes());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(1, testInstances0.getNumString());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(20, instances0.numInstances());
      assertEquals(5, instances0.numAttributes());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(20, instances0.size());
      assertEquals(4, instances0.classIndex());
      assertEquals(4, instances0.numClasses());
      assertEquals("Testdata", instances0.relationName());
      assertTrue(instances0.checkForStringAttributes());
      assertEquals("Gets the maximum number of instances allowed in the training pool. The addition of new instances above this value will result in old instances being removed. A value of 0 signifies no limit to the number of training instances.", iBk0.windowSizeTipText());
      assertEquals("The number of neighbours to use.", iBk0.KNNTipText());
      assertEquals(0, iBk0.getWindowSize());
      assertEquals("If set to true, classifier may output additional info to the console.", iBk0.debugTipText());
      assertEquals("Whether the mean squared error is used rather than mean absolute error when doing cross-validation for regression problems.", iBk0.meanSquaredTipText());
      assertEquals("The nearest neighbour search algorithm to use (Default: weka.core.neighboursearch.LinearNNSearch).", iBk0.nearestNeighbourSearchAlgorithmTipText());
      assertFalse(iBk0.getCrossValidate());
      assertFalse(iBk0.getDebug());
      assertEquals("Gets the distance weighting method used.", iBk0.distanceWeightingTipText());
      assertFalse(iBk0.getMeanSquared());
      assertEquals(1, iBk0.getKNN());
      assertEquals("Whether hold-one-out cross-validation will be used to select the best k value.", iBk0.crossValidateTipText());
      assertEquals(4, instances1.classIndex());
      assertEquals("Testdata", instances1.relationName());
      assertEquals(4, instances1.numClasses());
      assertTrue(instances1.checkForStringAttributes());
      assertEquals(5, instances1.numAttributes());
      assertEquals(1, instances1.numInstances());
      assertEquals(1, instances1.size());
      assertEquals(1.0, instances1.sumOfWeights(), 0.01);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals(1, IBk.WEIGHT_NONE);
      assertEquals(2, IBk.WEIGHT_INVERSE);
      assertEquals(4, IBk.WEIGHT_SIMILARITY);
      assertEquals(4, doubleArray1.length);
      assertEquals(5, doubleArray0.length);
  }

  /**
  //Test case number: 39
  /*Coverage entropy=3.1354942159291497
  */
  @Test(timeout = 4000)
  public void test39()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      assertNotNull(naiveBayesMultinomialText0);
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      
      naiveBayesMultinomialText0.setNormalizeDocLength(true);
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertTrue(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      
      naiveBayesMultinomialText0.m_useStopList = true;
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertTrue(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertTrue(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      
      IteratedLovinsStemmer iteratedLovinsStemmer0 = new IteratedLovinsStemmer();
      assertNotNull(iteratedLovinsStemmer0);
      
      InputMappedClassifier inputMappedClassifier0 = new InputMappedClassifier();
      assertNotNull(inputMappedClassifier0);
      assertEquals("Wrapper classifier that addresses incompatible training and test data by building a mapping between the training data that a classifier has been built with and the incoming test instances' structure. Model attributes that are not found in the incoming instances receive missing values, so do incoming nominal attribute values that the classifier has not seen before. A new classifier can be trained or an existing one loaded from a file.", inputMappedClassifier0.globalInfo());
      assertEquals("Ignore case when matching attribute names and nomina values.", inputMappedClassifier0.ignoreCaseForNamesTipText());
      assertEquals(0, inputMappedClassifier0.graphType());
      assertEquals("Trim white space from each end of attribute names and nominal values before matching.", inputMappedClassifier0.trimTipText());
      assertEquals("", inputMappedClassifier0.getModelPath());
      assertEquals("Set the path from which to load a model. Loading occurs when the first test instance is received. Environment variables can be used in the supplied path.", inputMappedClassifier0.modelPathTipText());
      assertFalse(inputMappedClassifier0.getDebug());
      assertFalse(inputMappedClassifier0.getSuppressMappingReport());
      assertTrue(inputMappedClassifier0.getTrim());
      assertEquals("The base classifier to be used.", inputMappedClassifier0.classifierTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", inputMappedClassifier0.debugTipText());
      assertEquals("Don't output a report of model-to-input mappings.", inputMappedClassifier0.suppressMappingReportTipText());
      assertTrue(inputMappedClassifier0.getIgnoreCaseForNames());
      
      Capabilities capabilities0 = inputMappedClassifier0.getCapabilities();
      assertNotNull(capabilities0);
      assertEquals("Wrapper classifier that addresses incompatible training and test data by building a mapping between the training data that a classifier has been built with and the incoming test instances' structure. Model attributes that are not found in the incoming instances receive missing values, so do incoming nominal attribute values that the classifier has not seen before. A new classifier can be trained or an existing one loaded from a file.", inputMappedClassifier0.globalInfo());
      assertEquals("Ignore case when matching attribute names and nomina values.", inputMappedClassifier0.ignoreCaseForNamesTipText());
      assertEquals(0, inputMappedClassifier0.graphType());
      assertEquals("Trim white space from each end of attribute names and nominal values before matching.", inputMappedClassifier0.trimTipText());
      assertEquals("", inputMappedClassifier0.getModelPath());
      assertEquals("Set the path from which to load a model. Loading occurs when the first test instance is received. Environment variables can be used in the supplied path.", inputMappedClassifier0.modelPathTipText());
      assertFalse(inputMappedClassifier0.getDebug());
      assertFalse(inputMappedClassifier0.getSuppressMappingReport());
      assertTrue(inputMappedClassifier0.getTrim());
      assertEquals("The base classifier to be used.", inputMappedClassifier0.classifierTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", inputMappedClassifier0.debugTipText());
      assertEquals("Don't output a report of model-to-input mappings.", inputMappedClassifier0.suppressMappingReportTipText());
      assertTrue(inputMappedClassifier0.getIgnoreCaseForNames());
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      assertTrue(capabilities0.hasDependencies());
      
      TestInstances testInstances0 = TestInstances.forCapabilities(capabilities0);
      assertNotNull(testInstances0);
      assertEquals("Wrapper classifier that addresses incompatible training and test data by building a mapping between the training data that a classifier has been built with and the incoming test instances' structure. Model attributes that are not found in the incoming instances receive missing values, so do incoming nominal attribute values that the classifier has not seen before. A new classifier can be trained or an existing one loaded from a file.", inputMappedClassifier0.globalInfo());
      assertEquals("Ignore case when matching attribute names and nomina values.", inputMappedClassifier0.ignoreCaseForNamesTipText());
      assertEquals(0, inputMappedClassifier0.graphType());
      assertEquals("Trim white space from each end of attribute names and nominal values before matching.", inputMappedClassifier0.trimTipText());
      assertEquals("", inputMappedClassifier0.getModelPath());
      assertEquals("Set the path from which to load a model. Loading occurs when the first test instance is received. Environment variables can be used in the supplied path.", inputMappedClassifier0.modelPathTipText());
      assertFalse(inputMappedClassifier0.getDebug());
      assertFalse(inputMappedClassifier0.getSuppressMappingReport());
      assertTrue(inputMappedClassifier0.getTrim());
      assertEquals("The base classifier to be used.", inputMappedClassifier0.classifierTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", inputMappedClassifier0.debugTipText());
      assertEquals("Don't output a report of model-to-input mappings.", inputMappedClassifier0.suppressMappingReportTipText());
      assertTrue(inputMappedClassifier0.getIgnoreCaseForNames());
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      assertTrue(capabilities0.hasDependencies());
      assertEquals(5, testInstances0.getNumAttributes());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(1, testInstances0.getNumString());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(1, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(4, testInstances0.getNumClasses());
      assertEquals((-1), testInstances0.getClassIndex());
      assertFalse(testInstances0.getNoClass());
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      
      Instances instances0 = testInstances0.generate();
      assertNotNull(instances0);
      assertEquals("Wrapper classifier that addresses incompatible training and test data by building a mapping between the training data that a classifier has been built with and the incoming test instances' structure. Model attributes that are not found in the incoming instances receive missing values, so do incoming nominal attribute values that the classifier has not seen before. A new classifier can be trained or an existing one loaded from a file.", inputMappedClassifier0.globalInfo());
      assertEquals("Ignore case when matching attribute names and nomina values.", inputMappedClassifier0.ignoreCaseForNamesTipText());
      assertEquals(0, inputMappedClassifier0.graphType());
      assertEquals("Trim white space from each end of attribute names and nominal values before matching.", inputMappedClassifier0.trimTipText());
      assertEquals("", inputMappedClassifier0.getModelPath());
      assertEquals("Set the path from which to load a model. Loading occurs when the first test instance is received. Environment variables can be used in the supplied path.", inputMappedClassifier0.modelPathTipText());
      assertFalse(inputMappedClassifier0.getDebug());
      assertFalse(inputMappedClassifier0.getSuppressMappingReport());
      assertTrue(inputMappedClassifier0.getTrim());
      assertEquals("The base classifier to be used.", inputMappedClassifier0.classifierTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", inputMappedClassifier0.debugTipText());
      assertEquals("Don't output a report of model-to-input mappings.", inputMappedClassifier0.suppressMappingReportTipText());
      assertTrue(inputMappedClassifier0.getIgnoreCaseForNames());
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      assertTrue(capabilities0.hasDependencies());
      assertEquals(5, testInstances0.getNumAttributes());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(1, testInstances0.getNumString());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(1, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(4, testInstances0.getNumClasses());
      assertEquals((-1), testInstances0.getClassIndex());
      assertFalse(testInstances0.getNoClass());
      assertEquals(4, instances0.classIndex());
      assertEquals(4, instances0.numClasses());
      assertEquals("Testdata", instances0.relationName());
      assertTrue(instances0.checkForStringAttributes());
      assertEquals(20, instances0.numInstances());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(20, instances0.size());
      assertEquals(5, instances0.numAttributes());
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      
      naiveBayesMultinomialText0.buildClassifier(instances0);
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertTrue(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertTrue(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("Wrapper classifier that addresses incompatible training and test data by building a mapping between the training data that a classifier has been built with and the incoming test instances' structure. Model attributes that are not found in the incoming instances receive missing values, so do incoming nominal attribute values that the classifier has not seen before. A new classifier can be trained or an existing one loaded from a file.", inputMappedClassifier0.globalInfo());
      assertEquals("Ignore case when matching attribute names and nomina values.", inputMappedClassifier0.ignoreCaseForNamesTipText());
      assertEquals(0, inputMappedClassifier0.graphType());
      assertEquals("Trim white space from each end of attribute names and nominal values before matching.", inputMappedClassifier0.trimTipText());
      assertEquals("", inputMappedClassifier0.getModelPath());
      assertEquals("Set the path from which to load a model. Loading occurs when the first test instance is received. Environment variables can be used in the supplied path.", inputMappedClassifier0.modelPathTipText());
      assertFalse(inputMappedClassifier0.getDebug());
      assertFalse(inputMappedClassifier0.getSuppressMappingReport());
      assertTrue(inputMappedClassifier0.getTrim());
      assertEquals("The base classifier to be used.", inputMappedClassifier0.classifierTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", inputMappedClassifier0.debugTipText());
      assertEquals("Don't output a report of model-to-input mappings.", inputMappedClassifier0.suppressMappingReportTipText());
      assertTrue(inputMappedClassifier0.getIgnoreCaseForNames());
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      assertTrue(capabilities0.hasDependencies());
      assertEquals(5, testInstances0.getNumAttributes());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(1, testInstances0.getNumString());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(1, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(4, testInstances0.getNumClasses());
      assertEquals((-1), testInstances0.getClassIndex());
      assertFalse(testInstances0.getNoClass());
      assertEquals(4, instances0.classIndex());
      assertEquals(4, instances0.numClasses());
      assertEquals("Testdata", instances0.relationName());
      assertTrue(instances0.checkForStringAttributes());
      assertEquals(20, instances0.numInstances());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(20, instances0.size());
      assertEquals(5, instances0.numAttributes());
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      
      System.setCurrentTimeMillis(1691L);
      String[] stringArray0 = Locale.getISOLanguages();
      assertNotNull(stringArray0);
      assertEquals(188, stringArray0.length);
      
      String[] stringArray1 = naiveBayesMultinomialText0.getOptions();
      assertNotSame(stringArray1, stringArray0);
      assertNotNull(stringArray1);
      assertFalse(stringArray1.equals((Object)stringArray0));
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertTrue(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertTrue(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals(14, stringArray1.length);
      
      InputMappedClassifier inputMappedClassifier1 = new InputMappedClassifier();
      assertNotNull(inputMappedClassifier1);
      assertFalse(inputMappedClassifier1.equals((Object)inputMappedClassifier0));
      assertEquals("Ignore case when matching attribute names and nomina values.", inputMappedClassifier1.ignoreCaseForNamesTipText());
      assertEquals("Trim white space from each end of attribute names and nominal values before matching.", inputMappedClassifier1.trimTipText());
      assertEquals(0, inputMappedClassifier1.graphType());
      assertTrue(inputMappedClassifier1.getIgnoreCaseForNames());
      assertEquals("If set to true, classifier may output additional info to the console.", inputMappedClassifier1.debugTipText());
      assertEquals("", inputMappedClassifier1.getModelPath());
      assertTrue(inputMappedClassifier1.getTrim());
      assertEquals("Don't output a report of model-to-input mappings.", inputMappedClassifier1.suppressMappingReportTipText());
      assertEquals("The base classifier to be used.", inputMappedClassifier1.classifierTipText());
      assertFalse(inputMappedClassifier1.getDebug());
      assertEquals("Set the path from which to load a model. Loading occurs when the first test instance is received. Environment variables can be used in the supplied path.", inputMappedClassifier1.modelPathTipText());
      assertEquals("Wrapper classifier that addresses incompatible training and test data by building a mapping between the training data that a classifier has been built with and the incoming test instances' structure. Model attributes that are not found in the incoming instances receive missing values, so do incoming nominal attribute values that the classifier has not seen before. A new classifier can be trained or an existing one loaded from a file.", inputMappedClassifier1.globalInfo());
      assertFalse(inputMappedClassifier1.getSuppressMappingReport());
      
      Capabilities capabilities1 = inputMappedClassifier1.getCapabilities();
      assertNotSame(inputMappedClassifier1, inputMappedClassifier0);
      assertNotSame(capabilities1, capabilities0);
      assertNotNull(capabilities1);
      assertFalse(inputMappedClassifier1.equals((Object)inputMappedClassifier0));
      assertFalse(capabilities1.equals((Object)capabilities0));
      assertEquals("Ignore case when matching attribute names and nomina values.", inputMappedClassifier1.ignoreCaseForNamesTipText());
      assertEquals("Trim white space from each end of attribute names and nominal values before matching.", inputMappedClassifier1.trimTipText());
      assertEquals(0, inputMappedClassifier1.graphType());
      assertTrue(inputMappedClassifier1.getIgnoreCaseForNames());
      assertEquals("If set to true, classifier may output additional info to the console.", inputMappedClassifier1.debugTipText());
      assertEquals("", inputMappedClassifier1.getModelPath());
      assertTrue(inputMappedClassifier1.getTrim());
      assertEquals("Don't output a report of model-to-input mappings.", inputMappedClassifier1.suppressMappingReportTipText());
      assertEquals("The base classifier to be used.", inputMappedClassifier1.classifierTipText());
      assertFalse(inputMappedClassifier1.getDebug());
      assertEquals("Set the path from which to load a model. Loading occurs when the first test instance is received. Environment variables can be used in the supplied path.", inputMappedClassifier1.modelPathTipText());
      assertEquals("Wrapper classifier that addresses incompatible training and test data by building a mapping between the training data that a classifier has been built with and the incoming test instances' structure. Model attributes that are not found in the incoming instances receive missing values, so do incoming nominal attribute values that the classifier has not seen before. A new classifier can be trained or an existing one loaded from a file.", inputMappedClassifier1.globalInfo());
      assertFalse(inputMappedClassifier1.getSuppressMappingReport());
      assertEquals(0, capabilities1.getMinimumNumberInstances());
      assertTrue(capabilities1.hasDependencies());
      
      LinkedHashMap<String, NaiveBayesMultinomialText.Count> linkedHashMap0 = new LinkedHashMap<String, NaiveBayesMultinomialText.Count>();
      assertNotNull(linkedHashMap0);
      assertEquals(0, linkedHashMap0.size());
      assertTrue(linkedHashMap0.isEmpty());
      
      TestInstances testInstances1 = TestInstances.forCapabilities(capabilities1);
      assertNotSame(inputMappedClassifier1, inputMappedClassifier0);
      assertNotSame(capabilities1, capabilities0);
      assertNotSame(testInstances1, testInstances0);
      assertNotNull(testInstances1);
      assertFalse(inputMappedClassifier1.equals((Object)inputMappedClassifier0));
      assertFalse(capabilities1.equals((Object)capabilities0));
      assertFalse(testInstances1.equals((Object)testInstances0));
      assertEquals("Ignore case when matching attribute names and nomina values.", inputMappedClassifier1.ignoreCaseForNamesTipText());
      assertEquals("Trim white space from each end of attribute names and nominal values before matching.", inputMappedClassifier1.trimTipText());
      assertEquals(0, inputMappedClassifier1.graphType());
      assertTrue(inputMappedClassifier1.getIgnoreCaseForNames());
      assertEquals("If set to true, classifier may output additional info to the console.", inputMappedClassifier1.debugTipText());
      assertEquals("", inputMappedClassifier1.getModelPath());
      assertTrue(inputMappedClassifier1.getTrim());
      assertEquals("Don't output a report of model-to-input mappings.", inputMappedClassifier1.suppressMappingReportTipText());
      assertEquals("The base classifier to be used.", inputMappedClassifier1.classifierTipText());
      assertFalse(inputMappedClassifier1.getDebug());
      assertEquals("Set the path from which to load a model. Loading occurs when the first test instance is received. Environment variables can be used in the supplied path.", inputMappedClassifier1.modelPathTipText());
      assertEquals("Wrapper classifier that addresses incompatible training and test data by building a mapping between the training data that a classifier has been built with and the incoming test instances' structure. Model attributes that are not found in the incoming instances receive missing values, so do incoming nominal attribute values that the classifier has not seen before. A new classifier can be trained or an existing one loaded from a file.", inputMappedClassifier1.globalInfo());
      assertFalse(inputMappedClassifier1.getSuppressMappingReport());
      assertEquals(0, capabilities1.getMinimumNumberInstances());
      assertTrue(capabilities1.hasDependencies());
      assertEquals(1, testInstances1.getNumRelationalString());
      assertEquals(1, testInstances1.getNumDate());
      assertEquals(2, testInstances1.getNumNominalValues());
      assertEquals(1, testInstances1.getNumNominal());
      assertEquals("Testdata", testInstances1.getRelation());
      assertEquals(" ", testInstances1.getWordSeparators());
      assertEquals(1, testInstances1.getClassType());
      assertEquals(1, testInstances1.getNumRelationalNumeric());
      assertEquals(20, testInstances1.getNumInstances());
      assertEquals(1, testInstances1.getNumRelationalNominal());
      assertEquals(10, testInstances1.getNumInstancesRelational());
      assertFalse(testInstances1.getMultiInstance());
      assertEquals(1, testInstances1.getSeed());
      assertEquals(1, testInstances1.getNumNumeric());
      assertEquals(0, testInstances1.getNumRelational());
      assertEquals(4, testInstances1.getNumClasses());
      assertEquals(1, testInstances1.getNumRelationalDate());
      assertEquals(1, testInstances1.getNumString());
      assertEquals((-1), testInstances1.getClassIndex());
      assertEquals(2, testInstances1.getNumRelationalNominalValues());
      assertEquals(5, testInstances1.getNumAttributes());
      assertFalse(testInstances1.getNoClass());
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      
      TestInstances testInstances2 = new TestInstances();
      assertNotNull(testInstances2);
      assertFalse(testInstances2.equals((Object)testInstances1));
      assertFalse(testInstances2.equals((Object)testInstances0));
      assertEquals(0, testInstances2.getNumString());
      assertEquals(1, testInstances2.getSeed());
      assertEquals((-1), testInstances2.getClassIndex());
      assertEquals(1, testInstances2.getClassType());
      assertEquals(2, testInstances2.getNumAttributes());
      assertEquals(1, testInstances2.getNumRelationalNominal());
      assertEquals(0, testInstances2.getNumNumeric());
      assertEquals(0, testInstances2.getNumRelationalString());
      assertEquals(0, testInstances2.getNumRelationalNumeric());
      assertEquals(2, testInstances2.getNumRelationalNominalValues());
      assertFalse(testInstances2.getMultiInstance());
      assertEquals(20, testInstances2.getNumInstances());
      assertFalse(testInstances2.getNoClass());
      assertEquals(0, testInstances2.getNumRelational());
      assertEquals(2, testInstances2.getNumClasses());
      assertEquals(0, testInstances2.getNumDate());
      assertEquals("Testdata", testInstances2.getRelation());
      assertEquals(" ", testInstances2.getWordSeparators());
      assertEquals(1, testInstances2.getNumNominal());
      assertEquals(0, testInstances2.getNumRelationalDate());
      assertEquals(2, testInstances2.getNumNominalValues());
      assertEquals(10, testInstances2.getNumInstancesRelational());
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      
      Instances instances1 = testInstances2.generate();
      assertNotSame(testInstances2, testInstances1);
      assertNotSame(testInstances2, testInstances0);
      assertNotSame(instances1, instances0);
      assertNotNull(instances1);
      assertFalse(testInstances2.equals((Object)testInstances1));
      assertFalse(testInstances2.equals((Object)testInstances0));
      assertFalse(instances1.equals((Object)instances0));
      assertEquals(0, testInstances2.getNumString());
      assertEquals(1, testInstances2.getSeed());
      assertEquals((-1), testInstances2.getClassIndex());
      assertEquals(1, testInstances2.getClassType());
      assertEquals(2, testInstances2.getNumAttributes());
      assertEquals(1, testInstances2.getNumRelationalNominal());
      assertEquals(0, testInstances2.getNumNumeric());
      assertEquals(0, testInstances2.getNumRelationalString());
      assertEquals(0, testInstances2.getNumRelationalNumeric());
      assertEquals(2, testInstances2.getNumRelationalNominalValues());
      assertFalse(testInstances2.getMultiInstance());
      assertEquals(20, testInstances2.getNumInstances());
      assertFalse(testInstances2.getNoClass());
      assertEquals(0, testInstances2.getNumRelational());
      assertEquals(2, testInstances2.getNumClasses());
      assertEquals(0, testInstances2.getNumDate());
      assertEquals("Testdata", testInstances2.getRelation());
      assertEquals(" ", testInstances2.getWordSeparators());
      assertEquals(1, testInstances2.getNumNominal());
      assertEquals(0, testInstances2.getNumRelationalDate());
      assertEquals(2, testInstances2.getNumNominalValues());
      assertEquals(10, testInstances2.getNumInstancesRelational());
      assertEquals(2, instances1.numAttributes());
      assertEquals(2, instances1.numClasses());
      assertEquals("Testdata", instances1.relationName());
      assertEquals(1, instances1.classIndex());
      assertEquals(20, instances1.size());
      assertEquals(20, instances1.numInstances());
      assertFalse(instances1.checkForStringAttributes());
      assertEquals(20.0, instances1.sumOfWeights(), 0.01);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      
      naiveBayesMultinomialText0.buildClassifier(instances1);
      assertNotSame(testInstances2, testInstances1);
      assertNotSame(testInstances2, testInstances0);
      assertNotSame(instances1, instances0);
      assertFalse(testInstances2.equals((Object)testInstances1));
      assertFalse(testInstances2.equals((Object)testInstances0));
      assertFalse(instances1.equals((Object)instances0));
      assertEquals(0, testInstances2.getNumString());
      assertEquals(1, testInstances2.getSeed());
      assertEquals((-1), testInstances2.getClassIndex());
      assertEquals(1, testInstances2.getClassType());
      assertEquals(2, testInstances2.getNumAttributes());
      assertEquals(1, testInstances2.getNumRelationalNominal());
      assertEquals(0, testInstances2.getNumNumeric());
      assertEquals(0, testInstances2.getNumRelationalString());
      assertEquals(0, testInstances2.getNumRelationalNumeric());
      assertEquals(2, testInstances2.getNumRelationalNominalValues());
      assertFalse(testInstances2.getMultiInstance());
      assertEquals(20, testInstances2.getNumInstances());
      assertFalse(testInstances2.getNoClass());
      assertEquals(0, testInstances2.getNumRelational());
      assertEquals(2, testInstances2.getNumClasses());
      assertEquals(0, testInstances2.getNumDate());
      assertEquals("Testdata", testInstances2.getRelation());
      assertEquals(" ", testInstances2.getWordSeparators());
      assertEquals(1, testInstances2.getNumNominal());
      assertEquals(0, testInstances2.getNumRelationalDate());
      assertEquals(2, testInstances2.getNumNominalValues());
      assertEquals(10, testInstances2.getNumInstancesRelational());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertTrue(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertTrue(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals(2, instances1.numAttributes());
      assertEquals(2, instances1.numClasses());
      assertEquals("Testdata", instances1.relationName());
      assertEquals(1, instances1.classIndex());
      assertEquals(20, instances1.size());
      assertEquals(20, instances1.numInstances());
      assertFalse(instances1.checkForStringAttributes());
      assertEquals(20.0, instances1.sumOfWeights(), 0.01);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      
      int[] intArray0 = new int[0];
      BinarySparseInstance binarySparseInstance0 = new BinarySparseInstance(0.0, intArray0, (-1209));
      assertNotNull(binarySparseInstance0);
      assertArrayEquals(new int[] {}, intArray0);
      assertEquals(0.0, binarySparseInstance0.weight(), 0.01);
      assertEquals(0, binarySparseInstance0.numValues());
      assertEquals((-1209), binarySparseInstance0.numAttributes());
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertEquals(0, intArray0.length);
      
      Capabilities capabilities2 = capabilities0.getOtherCapabilities();
      assertNotSame(inputMappedClassifier0, inputMappedClassifier1);
      assertNotSame(capabilities2, capabilities0);
      assertNotSame(capabilities2, capabilities1);
      assertNotSame(capabilities0, capabilities1);
      assertNotSame(capabilities0, capabilities2);
      assertNotNull(capabilities2);
      assertFalse(inputMappedClassifier0.equals((Object)inputMappedClassifier1));
      assertFalse(capabilities2.equals((Object)capabilities0));
      assertFalse(capabilities2.equals((Object)capabilities1));
      assertFalse(capabilities0.equals((Object)capabilities1));
      assertEquals("Wrapper classifier that addresses incompatible training and test data by building a mapping between the training data that a classifier has been built with and the incoming test instances' structure. Model attributes that are not found in the incoming instances receive missing values, so do incoming nominal attribute values that the classifier has not seen before. A new classifier can be trained or an existing one loaded from a file.", inputMappedClassifier0.globalInfo());
      assertEquals("Ignore case when matching attribute names and nomina values.", inputMappedClassifier0.ignoreCaseForNamesTipText());
      assertEquals(0, inputMappedClassifier0.graphType());
      assertEquals("Trim white space from each end of attribute names and nominal values before matching.", inputMappedClassifier0.trimTipText());
      assertEquals("", inputMappedClassifier0.getModelPath());
      assertEquals("Set the path from which to load a model. Loading occurs when the first test instance is received. Environment variables can be used in the supplied path.", inputMappedClassifier0.modelPathTipText());
      assertFalse(inputMappedClassifier0.getDebug());
      assertFalse(inputMappedClassifier0.getSuppressMappingReport());
      assertTrue(inputMappedClassifier0.getTrim());
      assertEquals("The base classifier to be used.", inputMappedClassifier0.classifierTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", inputMappedClassifier0.debugTipText());
      assertEquals("Don't output a report of model-to-input mappings.", inputMappedClassifier0.suppressMappingReportTipText());
      assertTrue(inputMappedClassifier0.getIgnoreCaseForNames());
      assertEquals(1, capabilities2.getMinimumNumberInstances());
      assertFalse(capabilities2.hasDependencies());
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      assertTrue(capabilities0.hasDependencies());
      
      double[] doubleArray0 = naiveBayesMultinomialText0.distributionForInstance(binarySparseInstance0);
      assertNotNull(doubleArray0);
      assertArrayEquals(new int[] {}, intArray0);
      assertArrayEquals(new double[] {0.5, 0.5}, doubleArray0, 0.01);
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertTrue(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertTrue(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals(0.0, binarySparseInstance0.weight(), 0.01);
      assertEquals(0, binarySparseInstance0.numValues());
      assertEquals((-1209), binarySparseInstance0.numAttributes());
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertEquals(0, intArray0.length);
      assertEquals(2, doubleArray0.length);
      
      naiveBayesMultinomialText0.pruneDictionary();
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertTrue(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertTrue(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      
      String[] stringArray2 = naiveBayesMultinomialText0.getOptions();
      assertNotSame(stringArray2, stringArray0);
      assertNotSame(stringArray2, stringArray1);
      assertNotNull(stringArray2);
      assertFalse(stringArray2.equals((Object)stringArray0));
      assertFalse(stringArray2.equals((Object)stringArray1));
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertTrue(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertTrue(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals(14, stringArray2.length);
      
      TestInstances testInstances3 = TestInstances.forCapabilities(capabilities1);
      assertNotSame(testInstances3, testInstances1);
      assertNotSame(testInstances3, testInstances0);
      assertNotSame(testInstances3, testInstances2);
      assertNotSame(inputMappedClassifier1, inputMappedClassifier0);
      assertNotSame(capabilities1, capabilities2);
      assertNotSame(capabilities1, capabilities0);
      assertNotNull(testInstances3);
      assertFalse(testInstances3.equals((Object)testInstances1));
      assertFalse(testInstances3.equals((Object)testInstances0));
      assertFalse(testInstances3.equals((Object)testInstances2));
      assertFalse(inputMappedClassifier1.equals((Object)inputMappedClassifier0));
      assertFalse(capabilities1.equals((Object)capabilities2));
      assertFalse(capabilities1.equals((Object)capabilities0));
      assertEquals(2, testInstances3.getNumRelationalNominalValues());
      assertEquals(10, testInstances3.getNumInstancesRelational());
      assertFalse(testInstances3.getNoClass());
      assertFalse(testInstances3.getMultiInstance());
      assertEquals(1, testInstances3.getNumNumeric());
      assertEquals(1, testInstances3.getNumRelationalString());
      assertEquals(1, testInstances3.getNumRelationalNumeric());
      assertEquals(1, testInstances3.getNumString());
      assertEquals(1, testInstances3.getNumRelationalNominal());
      assertEquals(1, testInstances3.getSeed());
      assertEquals(1, testInstances3.getClassType());
      assertEquals(5, testInstances3.getNumAttributes());
      assertEquals((-1), testInstances3.getClassIndex());
      assertEquals(0, testInstances3.getNumRelational());
      assertEquals(4, testInstances3.getNumClasses());
      assertEquals(1, testInstances3.getNumNominal());
      assertEquals("Testdata", testInstances3.getRelation());
      assertEquals(" ", testInstances3.getWordSeparators());
      assertEquals(1, testInstances3.getNumRelationalDate());
      assertEquals(20, testInstances3.getNumInstances());
      assertEquals(2, testInstances3.getNumNominalValues());
      assertEquals(1, testInstances3.getNumDate());
      assertEquals("Ignore case when matching attribute names and nomina values.", inputMappedClassifier1.ignoreCaseForNamesTipText());
      assertEquals("Trim white space from each end of attribute names and nominal values before matching.", inputMappedClassifier1.trimTipText());
      assertEquals(0, inputMappedClassifier1.graphType());
      assertTrue(inputMappedClassifier1.getIgnoreCaseForNames());
      assertEquals("If set to true, classifier may output additional info to the console.", inputMappedClassifier1.debugTipText());
      assertEquals("", inputMappedClassifier1.getModelPath());
      assertTrue(inputMappedClassifier1.getTrim());
      assertEquals("Don't output a report of model-to-input mappings.", inputMappedClassifier1.suppressMappingReportTipText());
      assertEquals("The base classifier to be used.", inputMappedClassifier1.classifierTipText());
      assertFalse(inputMappedClassifier1.getDebug());
      assertEquals("Set the path from which to load a model. Loading occurs when the first test instance is received. Environment variables can be used in the supplied path.", inputMappedClassifier1.modelPathTipText());
      assertEquals("Wrapper classifier that addresses incompatible training and test data by building a mapping between the training data that a classifier has been built with and the incoming test instances' structure. Model attributes that are not found in the incoming instances receive missing values, so do incoming nominal attribute values that the classifier has not seen before. A new classifier can be trained or an existing one loaded from a file.", inputMappedClassifier1.globalInfo());
      assertFalse(inputMappedClassifier1.getSuppressMappingReport());
      assertEquals(0, capabilities1.getMinimumNumberInstances());
      assertTrue(capabilities1.hasDependencies());
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      
      LinearNNSearch linearNNSearch0 = new LinearNNSearch(instances0);
      assertNotNull(linearNNSearch0);
      assertFalse(inputMappedClassifier0.equals((Object)inputMappedClassifier1));
      assertFalse(capabilities0.equals((Object)capabilities1));
      assertFalse(capabilities0.equals((Object)capabilities2));
      assertFalse(testInstances0.equals((Object)testInstances1));
      assertFalse(testInstances0.equals((Object)testInstances2));
      assertFalse(testInstances0.equals((Object)testInstances3));
      assertFalse(instances0.equals((Object)instances1));
      assertEquals("Wrapper classifier that addresses incompatible training and test data by building a mapping between the training data that a classifier has been built with and the incoming test instances' structure. Model attributes that are not found in the incoming instances receive missing values, so do incoming nominal attribute values that the classifier has not seen before. A new classifier can be trained or an existing one loaded from a file.", inputMappedClassifier0.globalInfo());
      assertEquals("Ignore case when matching attribute names and nomina values.", inputMappedClassifier0.ignoreCaseForNamesTipText());
      assertEquals(0, inputMappedClassifier0.graphType());
      assertEquals("Trim white space from each end of attribute names and nominal values before matching.", inputMappedClassifier0.trimTipText());
      assertEquals("", inputMappedClassifier0.getModelPath());
      assertEquals("Set the path from which to load a model. Loading occurs when the first test instance is received. Environment variables can be used in the supplied path.", inputMappedClassifier0.modelPathTipText());
      assertFalse(inputMappedClassifier0.getDebug());
      assertFalse(inputMappedClassifier0.getSuppressMappingReport());
      assertTrue(inputMappedClassifier0.getTrim());
      assertEquals("The base classifier to be used.", inputMappedClassifier0.classifierTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", inputMappedClassifier0.debugTipText());
      assertEquals("Don't output a report of model-to-input mappings.", inputMappedClassifier0.suppressMappingReportTipText());
      assertTrue(inputMappedClassifier0.getIgnoreCaseForNames());
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      assertTrue(capabilities0.hasDependencies());
      assertEquals(5, testInstances0.getNumAttributes());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(1, testInstances0.getNumString());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(1, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(4, testInstances0.getNumClasses());
      assertEquals((-1), testInstances0.getClassIndex());
      assertFalse(testInstances0.getNoClass());
      assertEquals(4, instances0.classIndex());
      assertEquals(4, instances0.numClasses());
      assertEquals("Testdata", instances0.relationName());
      assertTrue(instances0.checkForStringAttributes());
      assertEquals(20, instances0.numInstances());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(20, instances0.size());
      assertEquals(5, instances0.numAttributes());
      assertEquals("Whether to skip identical instances (with distance 0 to the target)", linearNNSearch0.skipIdenticalTipText());
      assertEquals("Whether to calculate performance statistics for the NN search or not", linearNNSearch0.measurePerformanceTipText());
      assertEquals("Class implementing the brute force search algorithm for nearest neighbour search.", linearNNSearch0.globalInfo());
      assertFalse(linearNNSearch0.getSkipIdentical());
      assertEquals("The distance function to use for finding neighbours (default: weka.core.EuclideanDistance). ", linearNNSearch0.distanceFunctionTipText());
      assertFalse(linearNNSearch0.getMeasurePerformance());
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      
      DenseInstance denseInstance0 = (DenseInstance)linearNNSearch0.nearestNeighbour(binarySparseInstance0);
      assertNotSame(inputMappedClassifier0, inputMappedClassifier1);
      assertNotSame(capabilities0, capabilities1);
      assertNotSame(capabilities0, capabilities2);
      assertNotSame(testInstances0, testInstances1);
      assertNotSame(testInstances0, testInstances2);
      assertNotSame(testInstances0, testInstances3);
      assertNotSame(instances0, instances1);
      assertNotNull(denseInstance0);
      assertArrayEquals(new int[] {}, intArray0);
      assertFalse(inputMappedClassifier0.equals((Object)inputMappedClassifier1));
      assertFalse(capabilities0.equals((Object)capabilities1));
      assertFalse(capabilities0.equals((Object)capabilities2));
      assertFalse(testInstances0.equals((Object)testInstances1));
      assertFalse(testInstances0.equals((Object)testInstances2));
      assertFalse(testInstances0.equals((Object)testInstances3));
      assertFalse(instances0.equals((Object)instances1));
      assertEquals(0.0, binarySparseInstance0.weight(), 0.01);
      assertEquals(0, binarySparseInstance0.numValues());
      assertEquals((-1209), binarySparseInstance0.numAttributes());
      assertEquals("Wrapper classifier that addresses incompatible training and test data by building a mapping between the training data that a classifier has been built with and the incoming test instances' structure. Model attributes that are not found in the incoming instances receive missing values, so do incoming nominal attribute values that the classifier has not seen before. A new classifier can be trained or an existing one loaded from a file.", inputMappedClassifier0.globalInfo());
      assertEquals("Ignore case when matching attribute names and nomina values.", inputMappedClassifier0.ignoreCaseForNamesTipText());
      assertEquals(0, inputMappedClassifier0.graphType());
      assertEquals("Trim white space from each end of attribute names and nominal values before matching.", inputMappedClassifier0.trimTipText());
      assertEquals("", inputMappedClassifier0.getModelPath());
      assertEquals("Set the path from which to load a model. Loading occurs when the first test instance is received. Environment variables can be used in the supplied path.", inputMappedClassifier0.modelPathTipText());
      assertFalse(inputMappedClassifier0.getDebug());
      assertFalse(inputMappedClassifier0.getSuppressMappingReport());
      assertTrue(inputMappedClassifier0.getTrim());
      assertEquals("The base classifier to be used.", inputMappedClassifier0.classifierTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", inputMappedClassifier0.debugTipText());
      assertEquals("Don't output a report of model-to-input mappings.", inputMappedClassifier0.suppressMappingReportTipText());
      assertTrue(inputMappedClassifier0.getIgnoreCaseForNames());
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      assertTrue(capabilities0.hasDependencies());
      assertEquals(5, testInstances0.getNumAttributes());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(1, testInstances0.getNumString());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(1, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(4, testInstances0.getNumClasses());
      assertEquals((-1), testInstances0.getClassIndex());
      assertFalse(testInstances0.getNoClass());
      assertEquals(4, instances0.classIndex());
      assertEquals(4, instances0.numClasses());
      assertEquals("Testdata", instances0.relationName());
      assertTrue(instances0.checkForStringAttributes());
      assertEquals(20, instances0.numInstances());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(20, instances0.size());
      assertEquals(5, instances0.numAttributes());
      assertEquals("Whether to skip identical instances (with distance 0 to the target)", linearNNSearch0.skipIdenticalTipText());
      assertEquals("Whether to calculate performance statistics for the NN search or not", linearNNSearch0.measurePerformanceTipText());
      assertEquals("Class implementing the brute force search algorithm for nearest neighbour search.", linearNNSearch0.globalInfo());
      assertFalse(linearNNSearch0.getSkipIdentical());
      assertEquals("The distance function to use for finding neighbours (default: weka.core.EuclideanDistance). ", linearNNSearch0.distanceFunctionTipText());
      assertFalse(linearNNSearch0.getMeasurePerformance());
      assertEquals(5, denseInstance0.numAttributes());
      assertEquals(5, denseInstance0.numValues());
      assertEquals(1.0, denseInstance0.weight(), 0.01);
      assertEquals(4, denseInstance0.numClasses());
      assertEquals(4, denseInstance0.classIndex());
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertEquals(0, intArray0.length);
      
      double[] doubleArray1 = naiveBayesMultinomialText0.distributionForInstance(denseInstance0);
      assertNotSame(inputMappedClassifier0, inputMappedClassifier1);
      assertNotSame(capabilities0, capabilities1);
      assertNotSame(capabilities0, capabilities2);
      assertNotSame(testInstances0, testInstances1);
      assertNotSame(testInstances0, testInstances2);
      assertNotSame(testInstances0, testInstances3);
      assertNotSame(instances0, instances1);
      assertNotSame(doubleArray1, doubleArray0);
      assertNotNull(doubleArray1);
      assertArrayEquals(new int[] {}, intArray0);
      assertArrayEquals(new double[] {0.5, 0.5}, doubleArray1, 0.01);
      assertFalse(inputMappedClassifier0.equals((Object)inputMappedClassifier1));
      assertFalse(capabilities0.equals((Object)capabilities1));
      assertFalse(capabilities0.equals((Object)capabilities2));
      assertFalse(testInstances0.equals((Object)testInstances1));
      assertFalse(testInstances0.equals((Object)testInstances2));
      assertFalse(testInstances0.equals((Object)testInstances3));
      assertFalse(instances0.equals((Object)instances1));
      assertFalse(doubleArray1.equals((Object)doubleArray0));
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertTrue(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertTrue(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals(0.0, binarySparseInstance0.weight(), 0.01);
      assertEquals(0, binarySparseInstance0.numValues());
      assertEquals((-1209), binarySparseInstance0.numAttributes());
      assertEquals("Wrapper classifier that addresses incompatible training and test data by building a mapping between the training data that a classifier has been built with and the incoming test instances' structure. Model attributes that are not found in the incoming instances receive missing values, so do incoming nominal attribute values that the classifier has not seen before. A new classifier can be trained or an existing one loaded from a file.", inputMappedClassifier0.globalInfo());
      assertEquals("Ignore case when matching attribute names and nomina values.", inputMappedClassifier0.ignoreCaseForNamesTipText());
      assertEquals(0, inputMappedClassifier0.graphType());
      assertEquals("Trim white space from each end of attribute names and nominal values before matching.", inputMappedClassifier0.trimTipText());
      assertEquals("", inputMappedClassifier0.getModelPath());
      assertEquals("Set the path from which to load a model. Loading occurs when the first test instance is received. Environment variables can be used in the supplied path.", inputMappedClassifier0.modelPathTipText());
      assertFalse(inputMappedClassifier0.getDebug());
      assertFalse(inputMappedClassifier0.getSuppressMappingReport());
      assertTrue(inputMappedClassifier0.getTrim());
      assertEquals("The base classifier to be used.", inputMappedClassifier0.classifierTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", inputMappedClassifier0.debugTipText());
      assertEquals("Don't output a report of model-to-input mappings.", inputMappedClassifier0.suppressMappingReportTipText());
      assertTrue(inputMappedClassifier0.getIgnoreCaseForNames());
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      assertTrue(capabilities0.hasDependencies());
      assertEquals(5, testInstances0.getNumAttributes());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(1, testInstances0.getNumString());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(1, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(4, testInstances0.getNumClasses());
      assertEquals((-1), testInstances0.getClassIndex());
      assertFalse(testInstances0.getNoClass());
      assertEquals(4, instances0.classIndex());
      assertEquals(4, instances0.numClasses());
      assertEquals("Testdata", instances0.relationName());
      assertTrue(instances0.checkForStringAttributes());
      assertEquals(20, instances0.numInstances());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(20, instances0.size());
      assertEquals(5, instances0.numAttributes());
      assertEquals("Whether to skip identical instances (with distance 0 to the target)", linearNNSearch0.skipIdenticalTipText());
      assertEquals("Whether to calculate performance statistics for the NN search or not", linearNNSearch0.measurePerformanceTipText());
      assertEquals("Class implementing the brute force search algorithm for nearest neighbour search.", linearNNSearch0.globalInfo());
      assertFalse(linearNNSearch0.getSkipIdentical());
      assertEquals("The distance function to use for finding neighbours (default: weka.core.EuclideanDistance). ", linearNNSearch0.distanceFunctionTipText());
      assertFalse(linearNNSearch0.getMeasurePerformance());
      assertEquals(5, denseInstance0.numAttributes());
      assertEquals(5, denseInstance0.numValues());
      assertEquals(1.0, denseInstance0.weight(), 0.01);
      assertEquals(4, denseInstance0.numClasses());
      assertEquals(4, denseInstance0.classIndex());
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertEquals(0, intArray0.length);
      assertEquals(2, doubleArray1.length);
      
      naiveBayesMultinomialText0.tokenizeInstance(binarySparseInstance0, false);
      assertArrayEquals(new int[] {}, intArray0);
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertTrue(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertTrue(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals(0.0, binarySparseInstance0.weight(), 0.01);
      assertEquals(0, binarySparseInstance0.numValues());
      assertEquals((-1209), binarySparseInstance0.numAttributes());
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertEquals(0, intArray0.length);
      
      String[] stringArray3 = naiveBayesMultinomialText0.getOptions();
      assertNotSame(stringArray3, stringArray0);
      assertNotSame(stringArray3, stringArray1);
      assertNotSame(stringArray3, stringArray2);
      assertNotNull(stringArray3);
      assertFalse(stringArray3.equals((Object)stringArray0));
      assertFalse(stringArray3.equals((Object)stringArray1));
      assertFalse(stringArray3.equals((Object)stringArray2));
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertTrue(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertTrue(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals(14, stringArray3.length);
      
      System.setCurrentTimeMillis(0);
  }

  /**
  //Test case number: 40
  /*Coverage entropy=3.5553480614894135
  */
  @Test(timeout = 4000)
  public void test40()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      assertNotNull(naiveBayesMultinomialText0);
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      
      naiveBayesMultinomialText0.setNormalizeDocLength(true);
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertTrue(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      
      naiveBayesMultinomialText0.m_useStopList = true;
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertTrue(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertTrue(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      
      naiveBayesMultinomialText0.setUseWordFrequencies(true);
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertTrue(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertTrue(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertTrue(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      
      IteratedLovinsStemmer iteratedLovinsStemmer0 = new IteratedLovinsStemmer();
      assertNotNull(iteratedLovinsStemmer0);
      
      InputMappedClassifier inputMappedClassifier0 = new InputMappedClassifier();
      assertNotNull(inputMappedClassifier0);
      assertEquals(0, inputMappedClassifier0.graphType());
      assertTrue(inputMappedClassifier0.getTrim());
      assertEquals("Ignore case when matching attribute names and nomina values.", inputMappedClassifier0.ignoreCaseForNamesTipText());
      assertEquals("Wrapper classifier that addresses incompatible training and test data by building a mapping between the training data that a classifier has been built with and the incoming test instances' structure. Model attributes that are not found in the incoming instances receive missing values, so do incoming nominal attribute values that the classifier has not seen before. A new classifier can be trained or an existing one loaded from a file.", inputMappedClassifier0.globalInfo());
      assertEquals("", inputMappedClassifier0.getModelPath());
      assertFalse(inputMappedClassifier0.getDebug());
      assertEquals("Don't output a report of model-to-input mappings.", inputMappedClassifier0.suppressMappingReportTipText());
      assertTrue(inputMappedClassifier0.getIgnoreCaseForNames());
      assertEquals("The base classifier to be used.", inputMappedClassifier0.classifierTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", inputMappedClassifier0.debugTipText());
      assertFalse(inputMappedClassifier0.getSuppressMappingReport());
      assertEquals("Set the path from which to load a model. Loading occurs when the first test instance is received. Environment variables can be used in the supplied path.", inputMappedClassifier0.modelPathTipText());
      assertEquals("Trim white space from each end of attribute names and nominal values before matching.", inputMappedClassifier0.trimTipText());
      
      Capabilities capabilities0 = inputMappedClassifier0.getCapabilities();
      assertNotNull(capabilities0);
      assertEquals(0, inputMappedClassifier0.graphType());
      assertTrue(inputMappedClassifier0.getTrim());
      assertEquals("Ignore case when matching attribute names and nomina values.", inputMappedClassifier0.ignoreCaseForNamesTipText());
      assertEquals("Wrapper classifier that addresses incompatible training and test data by building a mapping between the training data that a classifier has been built with and the incoming test instances' structure. Model attributes that are not found in the incoming instances receive missing values, so do incoming nominal attribute values that the classifier has not seen before. A new classifier can be trained or an existing one loaded from a file.", inputMappedClassifier0.globalInfo());
      assertEquals("", inputMappedClassifier0.getModelPath());
      assertFalse(inputMappedClassifier0.getDebug());
      assertEquals("Don't output a report of model-to-input mappings.", inputMappedClassifier0.suppressMappingReportTipText());
      assertTrue(inputMappedClassifier0.getIgnoreCaseForNames());
      assertEquals("The base classifier to be used.", inputMappedClassifier0.classifierTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", inputMappedClassifier0.debugTipText());
      assertFalse(inputMappedClassifier0.getSuppressMappingReport());
      assertEquals("Set the path from which to load a model. Loading occurs when the first test instance is received. Environment variables can be used in the supplied path.", inputMappedClassifier0.modelPathTipText());
      assertEquals("Trim white space from each end of attribute names and nominal values before matching.", inputMappedClassifier0.trimTipText());
      assertTrue(capabilities0.hasDependencies());
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      
      TestInstances testInstances0 = TestInstances.forCapabilities(capabilities0);
      assertNotNull(testInstances0);
      assertEquals(0, inputMappedClassifier0.graphType());
      assertTrue(inputMappedClassifier0.getTrim());
      assertEquals("Ignore case when matching attribute names and nomina values.", inputMappedClassifier0.ignoreCaseForNamesTipText());
      assertEquals("Wrapper classifier that addresses incompatible training and test data by building a mapping between the training data that a classifier has been built with and the incoming test instances' structure. Model attributes that are not found in the incoming instances receive missing values, so do incoming nominal attribute values that the classifier has not seen before. A new classifier can be trained or an existing one loaded from a file.", inputMappedClassifier0.globalInfo());
      assertEquals("", inputMappedClassifier0.getModelPath());
      assertFalse(inputMappedClassifier0.getDebug());
      assertEquals("Don't output a report of model-to-input mappings.", inputMappedClassifier0.suppressMappingReportTipText());
      assertTrue(inputMappedClassifier0.getIgnoreCaseForNames());
      assertEquals("The base classifier to be used.", inputMappedClassifier0.classifierTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", inputMappedClassifier0.debugTipText());
      assertFalse(inputMappedClassifier0.getSuppressMappingReport());
      assertEquals("Set the path from which to load a model. Loading occurs when the first test instance is received. Environment variables can be used in the supplied path.", inputMappedClassifier0.modelPathTipText());
      assertEquals("Trim white space from each end of attribute names and nominal values before matching.", inputMappedClassifier0.trimTipText());
      assertTrue(capabilities0.hasDependencies());
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(20, testInstances0.getNumInstances());
      assertFalse(testInstances0.getNoClass());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(5, testInstances0.getNumAttributes());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(4, testInstances0.getNumClasses());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(1, testInstances0.getSeed());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(1, testInstances0.getNumString());
      assertEquals(1, testInstances0.getNumRelationalString());
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      
      Instances instances0 = testInstances0.generate();
      assertNotNull(instances0);
      assertEquals(0, inputMappedClassifier0.graphType());
      assertTrue(inputMappedClassifier0.getTrim());
      assertEquals("Ignore case when matching attribute names and nomina values.", inputMappedClassifier0.ignoreCaseForNamesTipText());
      assertEquals("Wrapper classifier that addresses incompatible training and test data by building a mapping between the training data that a classifier has been built with and the incoming test instances' structure. Model attributes that are not found in the incoming instances receive missing values, so do incoming nominal attribute values that the classifier has not seen before. A new classifier can be trained or an existing one loaded from a file.", inputMappedClassifier0.globalInfo());
      assertEquals("", inputMappedClassifier0.getModelPath());
      assertFalse(inputMappedClassifier0.getDebug());
      assertEquals("Don't output a report of model-to-input mappings.", inputMappedClassifier0.suppressMappingReportTipText());
      assertTrue(inputMappedClassifier0.getIgnoreCaseForNames());
      assertEquals("The base classifier to be used.", inputMappedClassifier0.classifierTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", inputMappedClassifier0.debugTipText());
      assertFalse(inputMappedClassifier0.getSuppressMappingReport());
      assertEquals("Set the path from which to load a model. Loading occurs when the first test instance is received. Environment variables can be used in the supplied path.", inputMappedClassifier0.modelPathTipText());
      assertEquals("Trim white space from each end of attribute names and nominal values before matching.", inputMappedClassifier0.trimTipText());
      assertTrue(capabilities0.hasDependencies());
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(20, testInstances0.getNumInstances());
      assertFalse(testInstances0.getNoClass());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(5, testInstances0.getNumAttributes());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(4, testInstances0.getNumClasses());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(1, testInstances0.getSeed());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(1, testInstances0.getNumString());
      assertEquals(1, testInstances0.getNumRelationalString());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(4, instances0.numClasses());
      assertTrue(instances0.checkForStringAttributes());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(20, instances0.size());
      assertEquals(4, instances0.classIndex());
      assertEquals(5, instances0.numAttributes());
      assertEquals(20, instances0.numInstances());
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      
      naiveBayesMultinomialText0.buildClassifier(instances0);
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertTrue(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertTrue(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertTrue(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals(0, inputMappedClassifier0.graphType());
      assertTrue(inputMappedClassifier0.getTrim());
      assertEquals("Ignore case when matching attribute names and nomina values.", inputMappedClassifier0.ignoreCaseForNamesTipText());
      assertEquals("Wrapper classifier that addresses incompatible training and test data by building a mapping between the training data that a classifier has been built with and the incoming test instances' structure. Model attributes that are not found in the incoming instances receive missing values, so do incoming nominal attribute values that the classifier has not seen before. A new classifier can be trained or an existing one loaded from a file.", inputMappedClassifier0.globalInfo());
      assertEquals("", inputMappedClassifier0.getModelPath());
      assertFalse(inputMappedClassifier0.getDebug());
      assertEquals("Don't output a report of model-to-input mappings.", inputMappedClassifier0.suppressMappingReportTipText());
      assertTrue(inputMappedClassifier0.getIgnoreCaseForNames());
      assertEquals("The base classifier to be used.", inputMappedClassifier0.classifierTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", inputMappedClassifier0.debugTipText());
      assertFalse(inputMappedClassifier0.getSuppressMappingReport());
      assertEquals("Set the path from which to load a model. Loading occurs when the first test instance is received. Environment variables can be used in the supplied path.", inputMappedClassifier0.modelPathTipText());
      assertEquals("Trim white space from each end of attribute names and nominal values before matching.", inputMappedClassifier0.trimTipText());
      assertTrue(capabilities0.hasDependencies());
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(20, testInstances0.getNumInstances());
      assertFalse(testInstances0.getNoClass());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(5, testInstances0.getNumAttributes());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(4, testInstances0.getNumClasses());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(1, testInstances0.getSeed());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(1, testInstances0.getNumString());
      assertEquals(1, testInstances0.getNumRelationalString());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(4, instances0.numClasses());
      assertTrue(instances0.checkForStringAttributes());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(20, instances0.size());
      assertEquals(4, instances0.classIndex());
      assertEquals(5, instances0.numAttributes());
      assertEquals(20, instances0.numInstances());
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      
      System.setCurrentTimeMillis(1691L);
      String[] stringArray0 = Locale.getISOLanguages();
      assertNotNull(stringArray0);
      assertEquals(188, stringArray0.length);
      
      TestInstances testInstances1 = (TestInstances)testInstances0.clone();
      assertNotSame(testInstances0, testInstances1);
      assertNotSame(testInstances1, testInstances0);
      assertNotNull(testInstances1);
      assertFalse(testInstances1.equals((Object)testInstances0));
      assertEquals(0, inputMappedClassifier0.graphType());
      assertTrue(inputMappedClassifier0.getTrim());
      assertEquals("Ignore case when matching attribute names and nomina values.", inputMappedClassifier0.ignoreCaseForNamesTipText());
      assertEquals("Wrapper classifier that addresses incompatible training and test data by building a mapping between the training data that a classifier has been built with and the incoming test instances' structure. Model attributes that are not found in the incoming instances receive missing values, so do incoming nominal attribute values that the classifier has not seen before. A new classifier can be trained or an existing one loaded from a file.", inputMappedClassifier0.globalInfo());
      assertEquals("", inputMappedClassifier0.getModelPath());
      assertFalse(inputMappedClassifier0.getDebug());
      assertEquals("Don't output a report of model-to-input mappings.", inputMappedClassifier0.suppressMappingReportTipText());
      assertTrue(inputMappedClassifier0.getIgnoreCaseForNames());
      assertEquals("The base classifier to be used.", inputMappedClassifier0.classifierTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", inputMappedClassifier0.debugTipText());
      assertFalse(inputMappedClassifier0.getSuppressMappingReport());
      assertEquals("Set the path from which to load a model. Loading occurs when the first test instance is received. Environment variables can be used in the supplied path.", inputMappedClassifier0.modelPathTipText());
      assertEquals("Trim white space from each end of attribute names and nominal values before matching.", inputMappedClassifier0.trimTipText());
      assertTrue(capabilities0.hasDependencies());
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(20, testInstances0.getNumInstances());
      assertFalse(testInstances0.getNoClass());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(5, testInstances0.getNumAttributes());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(4, testInstances0.getNumClasses());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(1, testInstances0.getSeed());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(1, testInstances0.getNumString());
      assertEquals(1, testInstances0.getNumRelationalString());
      assertEquals(2, testInstances1.getNumRelationalNominalValues());
      assertEquals(0, testInstances1.getNumRelational());
      assertEquals(2, testInstances1.getNumNominalValues());
      assertEquals(20, testInstances1.getNumInstances());
      assertEquals(1, testInstances1.getNumRelationalDate());
      assertEquals(1, testInstances1.getNumDate());
      assertEquals(1, testInstances1.getNumRelationalString());
      assertEquals(10, testInstances1.getNumInstancesRelational());
      assertFalse(testInstances1.getMultiInstance());
      assertEquals(1, testInstances1.getSeed());
      assertEquals(1, testInstances1.getNumRelationalNominal());
      assertEquals((-1), testInstances1.getClassIndex());
      assertEquals(1, testInstances1.getClassType());
      assertEquals(1, testInstances1.getNumString());
      assertEquals(1, testInstances1.getNumRelationalNumeric());
      assertEquals(1, testInstances1.getNumNominal());
      assertEquals("Testdata", testInstances1.getRelation());
      assertEquals(" ", testInstances1.getWordSeparators());
      assertEquals(4, testInstances1.getNumClasses());
      assertEquals(1, testInstances1.getNumNumeric());
      assertEquals(5, testInstances1.getNumAttributes());
      assertFalse(testInstances1.getNoClass());
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      
      String string0 = naiveBayesMultinomialText0.toString();
      assertNotNull(string0);
      assertEquals("The independent probability of a class\n--------------------------------------\nclass1\t9.0\nclass2\t4.0\nclass3\t7.0\nclass4\t4.0\n\nThe probability of a word given the class\n-----------------------------------------\n\tclass1\tclass2\tclass3\tclass4\t\nquick\t7.38905609893065\t2.718281828459045\t7.38905609893065\t7.38905609893065\t\nlazy\t7.38905609893065\t2.718281828459045\t7.38905609893065\t2.718281828459045\t\njumps\t20.085536923187668\t7.38905609893065\t2.718281828459045\t2.718281828459045\t\nbrown\t7.38905609893065\t7.38905609893065\t2.718281828459045\t2.718281828459045\t\ndog\t7.38905609893065\t2.718281828459045\t7.38905609893065\t2.718281828459045\t\nfox\t2.718281828459045\t2.718281828459045\t7.38905609893065\t7.38905609893065\t\n", string0);
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertTrue(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertTrue(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertTrue(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      
      NaiveBayesMultinomialText naiveBayesMultinomialText1 = new NaiveBayesMultinomialText();
      assertNotNull(naiveBayesMultinomialText1);
      assertFalse(naiveBayesMultinomialText1.equals((Object)naiveBayesMultinomialText0));
      assertEquals(2.0, naiveBayesMultinomialText1.getLNorm(), 0.01);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText1.normalizeDocLengthTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText1.useWordFrequenciesTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText1.tokenizerTipText());
      assertFalse(naiveBayesMultinomialText1.getNormalizeDocLength());
      assertEquals(0, naiveBayesMultinomialText1.getPeriodicPruning());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText1.minWordFrequencyTipText());
      assertEquals(3.0, naiveBayesMultinomialText1.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText1.LNormTipText());
      assertFalse(naiveBayesMultinomialText1.getLowercaseTokens());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText1.useStopListTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText1.stemmerTipText());
      assertFalse(naiveBayesMultinomialText1.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText1.globalInfo());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText1.normTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText1.periodicPruningTipText());
      assertFalse(naiveBayesMultinomialText1.getUseStopList());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText1.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText1.getDebug());
      assertEquals(1.0, naiveBayesMultinomialText1.getNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText1.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText1.stopwordsTipText());
      
      int[] intArray0 = new int[0];
      BinarySparseInstance binarySparseInstance0 = new BinarySparseInstance((-267.77445), intArray0, (-2));
      assertNotNull(binarySparseInstance0);
      assertArrayEquals(new int[] {}, intArray0);
      assertEquals(0, binarySparseInstance0.numValues());
      assertEquals((-267.77445), binarySparseInstance0.weight(), 0.01);
      assertEquals((-2), binarySparseInstance0.numAttributes());
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertEquals(0, intArray0.length);
      
      boolean boolean0 = FileSystemHandling.appendStringToFile((EvoSuiteFile) null, "");
      assertFalse(boolean0);
      
      InputMappedClassifier inputMappedClassifier1 = new InputMappedClassifier();
      assertNotNull(inputMappedClassifier1);
      assertFalse(inputMappedClassifier1.equals((Object)inputMappedClassifier0));
      assertTrue(inputMappedClassifier1.getIgnoreCaseForNames());
      assertEquals("If set to true, classifier may output additional info to the console.", inputMappedClassifier1.debugTipText());
      assertEquals("Ignore case when matching attribute names and nomina values.", inputMappedClassifier1.ignoreCaseForNamesTipText());
      assertEquals("The base classifier to be used.", inputMappedClassifier1.classifierTipText());
      assertFalse(inputMappedClassifier1.getSuppressMappingReport());
      assertEquals("Wrapper classifier that addresses incompatible training and test data by building a mapping between the training data that a classifier has been built with and the incoming test instances' structure. Model attributes that are not found in the incoming instances receive missing values, so do incoming nominal attribute values that the classifier has not seen before. A new classifier can be trained or an existing one loaded from a file.", inputMappedClassifier1.globalInfo());
      assertFalse(inputMappedClassifier1.getDebug());
      assertEquals("", inputMappedClassifier1.getModelPath());
      assertTrue(inputMappedClassifier1.getTrim());
      assertEquals("Set the path from which to load a model. Loading occurs when the first test instance is received. Environment variables can be used in the supplied path.", inputMappedClassifier1.modelPathTipText());
      assertEquals("Don't output a report of model-to-input mappings.", inputMappedClassifier1.suppressMappingReportTipText());
      assertEquals("Trim white space from each end of attribute names and nominal values before matching.", inputMappedClassifier1.trimTipText());
      assertEquals(0, inputMappedClassifier1.graphType());
      
      Capabilities capabilities1 = capabilities0.getAttributeCapabilities();
      assertNotSame(capabilities1, capabilities0);
      assertNotSame(inputMappedClassifier0, inputMappedClassifier1);
      assertNotSame(capabilities0, capabilities1);
      assertNotNull(capabilities1);
      assertFalse(capabilities1.equals((Object)capabilities0));
      assertFalse(inputMappedClassifier0.equals((Object)inputMappedClassifier1));
      assertFalse(capabilities1.hasDependencies());
      assertEquals(1, capabilities1.getMinimumNumberInstances());
      assertEquals(0, inputMappedClassifier0.graphType());
      assertTrue(inputMappedClassifier0.getTrim());
      assertEquals("Ignore case when matching attribute names and nomina values.", inputMappedClassifier0.ignoreCaseForNamesTipText());
      assertEquals("Wrapper classifier that addresses incompatible training and test data by building a mapping between the training data that a classifier has been built with and the incoming test instances' structure. Model attributes that are not found in the incoming instances receive missing values, so do incoming nominal attribute values that the classifier has not seen before. A new classifier can be trained or an existing one loaded from a file.", inputMappedClassifier0.globalInfo());
      assertEquals("", inputMappedClassifier0.getModelPath());
      assertFalse(inputMappedClassifier0.getDebug());
      assertEquals("Don't output a report of model-to-input mappings.", inputMappedClassifier0.suppressMappingReportTipText());
      assertTrue(inputMappedClassifier0.getIgnoreCaseForNames());
      assertEquals("The base classifier to be used.", inputMappedClassifier0.classifierTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", inputMappedClassifier0.debugTipText());
      assertFalse(inputMappedClassifier0.getSuppressMappingReport());
      assertEquals("Set the path from which to load a model. Loading occurs when the first test instance is received. Environment variables can be used in the supplied path.", inputMappedClassifier0.modelPathTipText());
      assertEquals("Trim white space from each end of attribute names and nominal values before matching.", inputMappedClassifier0.trimTipText());
      assertTrue(capabilities0.hasDependencies());
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      
      double[] doubleArray0 = naiveBayesMultinomialText0.distributionForInstance(binarySparseInstance0);
      assertNotSame(naiveBayesMultinomialText0, naiveBayesMultinomialText1);
      assertNotNull(doubleArray0);
      assertArrayEquals(new double[] {0.3750000000000001, 0.16666666666666669, 0.29166666666666663, 0.16666666666666669}, doubleArray0, 0.01);
      assertArrayEquals(new int[] {}, intArray0);
      assertFalse(naiveBayesMultinomialText0.equals((Object)naiveBayesMultinomialText1));
      assertEquals(0, binarySparseInstance0.numValues());
      assertEquals((-267.77445), binarySparseInstance0.weight(), 0.01);
      assertEquals((-2), binarySparseInstance0.numAttributes());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertTrue(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertTrue(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertTrue(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertEquals(4, doubleArray0.length);
      assertEquals(0, intArray0.length);
      
      naiveBayesMultinomialText1.pruneDictionary();
      assertNotSame(naiveBayesMultinomialText1, naiveBayesMultinomialText0);
      assertFalse(naiveBayesMultinomialText1.equals((Object)naiveBayesMultinomialText0));
      assertEquals(2.0, naiveBayesMultinomialText1.getLNorm(), 0.01);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText1.normalizeDocLengthTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText1.useWordFrequenciesTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText1.tokenizerTipText());
      assertFalse(naiveBayesMultinomialText1.getNormalizeDocLength());
      assertEquals(0, naiveBayesMultinomialText1.getPeriodicPruning());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText1.minWordFrequencyTipText());
      assertEquals(3.0, naiveBayesMultinomialText1.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText1.LNormTipText());
      assertFalse(naiveBayesMultinomialText1.getLowercaseTokens());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText1.useStopListTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText1.stemmerTipText());
      assertFalse(naiveBayesMultinomialText1.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText1.globalInfo());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText1.normTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText1.periodicPruningTipText());
      assertFalse(naiveBayesMultinomialText1.getUseStopList());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText1.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText1.getDebug());
      assertEquals(1.0, naiveBayesMultinomialText1.getNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText1.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText1.stopwordsTipText());
      
      NaiveBayesMultinomialText naiveBayesMultinomialText2 = new NaiveBayesMultinomialText();
      assertNotNull(naiveBayesMultinomialText2);
      assertFalse(naiveBayesMultinomialText2.equals((Object)naiveBayesMultinomialText0));
      assertFalse(naiveBayesMultinomialText2.equals((Object)naiveBayesMultinomialText1));
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText2.lowercaseTokensTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText2.tokenizerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText2.minWordFrequencyTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText2.stopwordsTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText2.normTipText());
      assertEquals(2.0, naiveBayesMultinomialText2.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText2.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText2.globalInfo());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText2.debugTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText2.normalizeDocLengthTipText());
      assertEquals(1.0, naiveBayesMultinomialText2.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText2.getDebug());
      assertFalse(naiveBayesMultinomialText2.getUseStopList());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText2.useWordFrequenciesTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText2.periodicPruningTipText());
      assertEquals(0, naiveBayesMultinomialText2.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText2.getNormalizeDocLength());
      assertEquals(3.0, naiveBayesMultinomialText2.getMinWordFrequency(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText2.useStopListTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText2.stemmerTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText2.LNormTipText());
      assertFalse(naiveBayesMultinomialText2.getLowercaseTokens());
      
      String[] stringArray1 = naiveBayesMultinomialText2.getOptions();
      assertNotSame(naiveBayesMultinomialText2, naiveBayesMultinomialText0);
      assertNotSame(naiveBayesMultinomialText2, naiveBayesMultinomialText1);
      assertNotSame(stringArray1, stringArray0);
      assertNotNull(stringArray1);
      assertFalse(naiveBayesMultinomialText2.equals((Object)naiveBayesMultinomialText0));
      assertFalse(naiveBayesMultinomialText2.equals((Object)naiveBayesMultinomialText1));
      assertFalse(stringArray1.equals((Object)stringArray0));
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText2.lowercaseTokensTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText2.tokenizerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText2.minWordFrequencyTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText2.stopwordsTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText2.normTipText());
      assertEquals(2.0, naiveBayesMultinomialText2.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText2.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText2.globalInfo());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText2.debugTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText2.normalizeDocLengthTipText());
      assertEquals(1.0, naiveBayesMultinomialText2.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText2.getDebug());
      assertFalse(naiveBayesMultinomialText2.getUseStopList());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText2.useWordFrequenciesTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText2.periodicPruningTipText());
      assertEquals(0, naiveBayesMultinomialText2.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText2.getNormalizeDocLength());
      assertEquals(3.0, naiveBayesMultinomialText2.getMinWordFrequency(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText2.useStopListTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText2.stemmerTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText2.LNormTipText());
      assertFalse(naiveBayesMultinomialText2.getLowercaseTokens());
      assertEquals(12, stringArray1.length);
      
      LinearNNSearch linearNNSearch0 = new LinearNNSearch(instances0);
      assertNotNull(linearNNSearch0);
      assertFalse(inputMappedClassifier0.equals((Object)inputMappedClassifier1));
      assertFalse(capabilities0.equals((Object)capabilities1));
      assertFalse(testInstances0.equals((Object)testInstances1));
      assertEquals(0, inputMappedClassifier0.graphType());
      assertTrue(inputMappedClassifier0.getTrim());
      assertEquals("Ignore case when matching attribute names and nomina values.", inputMappedClassifier0.ignoreCaseForNamesTipText());
      assertEquals("Wrapper classifier that addresses incompatible training and test data by building a mapping between the training data that a classifier has been built with and the incoming test instances' structure. Model attributes that are not found in the incoming instances receive missing values, so do incoming nominal attribute values that the classifier has not seen before. A new classifier can be trained or an existing one loaded from a file.", inputMappedClassifier0.globalInfo());
      assertEquals("", inputMappedClassifier0.getModelPath());
      assertFalse(inputMappedClassifier0.getDebug());
      assertEquals("Don't output a report of model-to-input mappings.", inputMappedClassifier0.suppressMappingReportTipText());
      assertTrue(inputMappedClassifier0.getIgnoreCaseForNames());
      assertEquals("The base classifier to be used.", inputMappedClassifier0.classifierTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", inputMappedClassifier0.debugTipText());
      assertFalse(inputMappedClassifier0.getSuppressMappingReport());
      assertEquals("Set the path from which to load a model. Loading occurs when the first test instance is received. Environment variables can be used in the supplied path.", inputMappedClassifier0.modelPathTipText());
      assertEquals("Trim white space from each end of attribute names and nominal values before matching.", inputMappedClassifier0.trimTipText());
      assertTrue(capabilities0.hasDependencies());
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(20, testInstances0.getNumInstances());
      assertFalse(testInstances0.getNoClass());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(5, testInstances0.getNumAttributes());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(4, testInstances0.getNumClasses());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(1, testInstances0.getSeed());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(1, testInstances0.getNumString());
      assertEquals(1, testInstances0.getNumRelationalString());
      assertFalse(linearNNSearch0.getMeasurePerformance());
      assertFalse(linearNNSearch0.getSkipIdentical());
      assertEquals("Whether to skip identical instances (with distance 0 to the target)", linearNNSearch0.skipIdenticalTipText());
      assertEquals("Class implementing the brute force search algorithm for nearest neighbour search.", linearNNSearch0.globalInfo());
      assertEquals("The distance function to use for finding neighbours (default: weka.core.EuclideanDistance). ", linearNNSearch0.distanceFunctionTipText());
      assertEquals("Whether to calculate performance statistics for the NN search or not", linearNNSearch0.measurePerformanceTipText());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(4, instances0.numClasses());
      assertTrue(instances0.checkForStringAttributes());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(20, instances0.size());
      assertEquals(4, instances0.classIndex());
      assertEquals(5, instances0.numAttributes());
      assertEquals(20, instances0.numInstances());
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      
      DenseInstance denseInstance0 = (DenseInstance)linearNNSearch0.nearestNeighbour(binarySparseInstance0);
      assertNotSame(inputMappedClassifier0, inputMappedClassifier1);
      assertNotSame(capabilities0, capabilities1);
      assertNotSame(testInstances0, testInstances1);
      assertNotNull(denseInstance0);
      assertArrayEquals(new int[] {}, intArray0);
      assertFalse(inputMappedClassifier0.equals((Object)inputMappedClassifier1));
      assertFalse(capabilities0.equals((Object)capabilities1));
      assertFalse(testInstances0.equals((Object)testInstances1));
      assertEquals(0, binarySparseInstance0.numValues());
      assertEquals((-267.77445), binarySparseInstance0.weight(), 0.01);
      assertEquals((-2), binarySparseInstance0.numAttributes());
      assertEquals(0, inputMappedClassifier0.graphType());
      assertTrue(inputMappedClassifier0.getTrim());
      assertEquals("Ignore case when matching attribute names and nomina values.", inputMappedClassifier0.ignoreCaseForNamesTipText());
      assertEquals("Wrapper classifier that addresses incompatible training and test data by building a mapping between the training data that a classifier has been built with and the incoming test instances' structure. Model attributes that are not found in the incoming instances receive missing values, so do incoming nominal attribute values that the classifier has not seen before. A new classifier can be trained or an existing one loaded from a file.", inputMappedClassifier0.globalInfo());
      assertEquals("", inputMappedClassifier0.getModelPath());
      assertFalse(inputMappedClassifier0.getDebug());
      assertEquals("Don't output a report of model-to-input mappings.", inputMappedClassifier0.suppressMappingReportTipText());
      assertTrue(inputMappedClassifier0.getIgnoreCaseForNames());
      assertEquals("The base classifier to be used.", inputMappedClassifier0.classifierTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", inputMappedClassifier0.debugTipText());
      assertFalse(inputMappedClassifier0.getSuppressMappingReport());
      assertEquals("Set the path from which to load a model. Loading occurs when the first test instance is received. Environment variables can be used in the supplied path.", inputMappedClassifier0.modelPathTipText());
      assertEquals("Trim white space from each end of attribute names and nominal values before matching.", inputMappedClassifier0.trimTipText());
      assertTrue(capabilities0.hasDependencies());
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(20, testInstances0.getNumInstances());
      assertFalse(testInstances0.getNoClass());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(5, testInstances0.getNumAttributes());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(4, testInstances0.getNumClasses());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(1, testInstances0.getSeed());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(1, testInstances0.getNumString());
      assertEquals(1, testInstances0.getNumRelationalString());
      assertFalse(linearNNSearch0.getMeasurePerformance());
      assertFalse(linearNNSearch0.getSkipIdentical());
      assertEquals("Whether to skip identical instances (with distance 0 to the target)", linearNNSearch0.skipIdenticalTipText());
      assertEquals("Class implementing the brute force search algorithm for nearest neighbour search.", linearNNSearch0.globalInfo());
      assertEquals("The distance function to use for finding neighbours (default: weka.core.EuclideanDistance). ", linearNNSearch0.distanceFunctionTipText());
      assertEquals("Whether to calculate performance statistics for the NN search or not", linearNNSearch0.measurePerformanceTipText());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(4, instances0.numClasses());
      assertTrue(instances0.checkForStringAttributes());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(20, instances0.size());
      assertEquals(4, instances0.classIndex());
      assertEquals(5, instances0.numAttributes());
      assertEquals(20, instances0.numInstances());
      assertEquals(4, denseInstance0.numClasses());
      assertEquals(5, denseInstance0.numValues());
      assertEquals(1.0, denseInstance0.weight(), 0.01);
      assertEquals(4, denseInstance0.classIndex());
      assertEquals(5, denseInstance0.numAttributes());
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertEquals(0, intArray0.length);
      
      double[] doubleArray1 = naiveBayesMultinomialText0.distributionForInstance(denseInstance0);
      assertNotSame(naiveBayesMultinomialText0, naiveBayesMultinomialText1);
      assertNotSame(naiveBayesMultinomialText0, naiveBayesMultinomialText2);
      assertNotSame(inputMappedClassifier0, inputMappedClassifier1);
      assertNotSame(capabilities0, capabilities1);
      assertNotSame(testInstances0, testInstances1);
      assertNotSame(doubleArray1, doubleArray0);
      assertNotNull(doubleArray1);
      assertArrayEquals(new double[] {0.38461538461538475, 0.12820512820512825, 0.3589743589743588, 0.12820512820512825}, doubleArray1, 0.01);
      assertArrayEquals(new int[] {}, intArray0);
      assertFalse(naiveBayesMultinomialText0.equals((Object)naiveBayesMultinomialText1));
      assertFalse(naiveBayesMultinomialText0.equals((Object)naiveBayesMultinomialText2));
      assertFalse(inputMappedClassifier0.equals((Object)inputMappedClassifier1));
      assertFalse(capabilities0.equals((Object)capabilities1));
      assertFalse(testInstances0.equals((Object)testInstances1));
      assertFalse(doubleArray1.equals((Object)doubleArray0));
      assertEquals(0, binarySparseInstance0.numValues());
      assertEquals((-267.77445), binarySparseInstance0.weight(), 0.01);
      assertEquals((-2), binarySparseInstance0.numAttributes());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertTrue(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertTrue(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertTrue(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals(0, inputMappedClassifier0.graphType());
      assertTrue(inputMappedClassifier0.getTrim());
      assertEquals("Ignore case when matching attribute names and nomina values.", inputMappedClassifier0.ignoreCaseForNamesTipText());
      assertEquals("Wrapper classifier that addresses incompatible training and test data by building a mapping between the training data that a classifier has been built with and the incoming test instances' structure. Model attributes that are not found in the incoming instances receive missing values, so do incoming nominal attribute values that the classifier has not seen before. A new classifier can be trained or an existing one loaded from a file.", inputMappedClassifier0.globalInfo());
      assertEquals("", inputMappedClassifier0.getModelPath());
      assertFalse(inputMappedClassifier0.getDebug());
      assertEquals("Don't output a report of model-to-input mappings.", inputMappedClassifier0.suppressMappingReportTipText());
      assertTrue(inputMappedClassifier0.getIgnoreCaseForNames());
      assertEquals("The base classifier to be used.", inputMappedClassifier0.classifierTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", inputMappedClassifier0.debugTipText());
      assertFalse(inputMappedClassifier0.getSuppressMappingReport());
      assertEquals("Set the path from which to load a model. Loading occurs when the first test instance is received. Environment variables can be used in the supplied path.", inputMappedClassifier0.modelPathTipText());
      assertEquals("Trim white space from each end of attribute names and nominal values before matching.", inputMappedClassifier0.trimTipText());
      assertTrue(capabilities0.hasDependencies());
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(20, testInstances0.getNumInstances());
      assertFalse(testInstances0.getNoClass());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(5, testInstances0.getNumAttributes());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(4, testInstances0.getNumClasses());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(1, testInstances0.getSeed());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(1, testInstances0.getNumString());
      assertEquals(1, testInstances0.getNumRelationalString());
      assertFalse(linearNNSearch0.getMeasurePerformance());
      assertFalse(linearNNSearch0.getSkipIdentical());
      assertEquals("Whether to skip identical instances (with distance 0 to the target)", linearNNSearch0.skipIdenticalTipText());
      assertEquals("Class implementing the brute force search algorithm for nearest neighbour search.", linearNNSearch0.globalInfo());
      assertEquals("The distance function to use for finding neighbours (default: weka.core.EuclideanDistance). ", linearNNSearch0.distanceFunctionTipText());
      assertEquals("Whether to calculate performance statistics for the NN search or not", linearNNSearch0.measurePerformanceTipText());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(4, instances0.numClasses());
      assertTrue(instances0.checkForStringAttributes());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(20, instances0.size());
      assertEquals(4, instances0.classIndex());
      assertEquals(5, instances0.numAttributes());
      assertEquals(20, instances0.numInstances());
      assertEquals(4, denseInstance0.numClasses());
      assertEquals(5, denseInstance0.numValues());
      assertEquals(1.0, denseInstance0.weight(), 0.01);
      assertEquals(4, denseInstance0.classIndex());
      assertEquals(5, denseInstance0.numAttributes());
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertEquals(4, doubleArray1.length);
      assertEquals(0, intArray0.length);
      
      capabilities0.enableAllClasses();
      assertNotSame(inputMappedClassifier0, inputMappedClassifier1);
      assertNotSame(capabilities0, capabilities1);
      assertFalse(inputMappedClassifier0.equals((Object)inputMappedClassifier1));
      assertFalse(capabilities0.equals((Object)capabilities1));
      assertEquals(0, inputMappedClassifier0.graphType());
      assertTrue(inputMappedClassifier0.getTrim());
      assertEquals("Ignore case when matching attribute names and nomina values.", inputMappedClassifier0.ignoreCaseForNamesTipText());
      assertEquals("Wrapper classifier that addresses incompatible training and test data by building a mapping between the training data that a classifier has been built with and the incoming test instances' structure. Model attributes that are not found in the incoming instances receive missing values, so do incoming nominal attribute values that the classifier has not seen before. A new classifier can be trained or an existing one loaded from a file.", inputMappedClassifier0.globalInfo());
      assertEquals("", inputMappedClassifier0.getModelPath());
      assertFalse(inputMappedClassifier0.getDebug());
      assertEquals("Don't output a report of model-to-input mappings.", inputMappedClassifier0.suppressMappingReportTipText());
      assertTrue(inputMappedClassifier0.getIgnoreCaseForNames());
      assertEquals("The base classifier to be used.", inputMappedClassifier0.classifierTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", inputMappedClassifier0.debugTipText());
      assertFalse(inputMappedClassifier0.getSuppressMappingReport());
      assertEquals("Set the path from which to load a model. Loading occurs when the first test instance is received. Environment variables can be used in the supplied path.", inputMappedClassifier0.modelPathTipText());
      assertEquals("Trim white space from each end of attribute names and nominal values before matching.", inputMappedClassifier0.trimTipText());
      assertTrue(capabilities0.hasDependencies());
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      
      naiveBayesMultinomialText1.setOptions(stringArray0);
      assertNotSame(stringArray0, stringArray1);
      assertNotSame(naiveBayesMultinomialText1, naiveBayesMultinomialText2);
      assertNotSame(naiveBayesMultinomialText1, naiveBayesMultinomialText0);
      assertFalse(stringArray0.equals((Object)stringArray1));
      assertFalse(naiveBayesMultinomialText1.equals((Object)naiveBayesMultinomialText2));
      assertFalse(naiveBayesMultinomialText1.equals((Object)naiveBayesMultinomialText0));
      assertEquals(2.0, naiveBayesMultinomialText1.getLNorm(), 0.01);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText1.normalizeDocLengthTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText1.useWordFrequenciesTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText1.tokenizerTipText());
      assertFalse(naiveBayesMultinomialText1.getNormalizeDocLength());
      assertEquals(0, naiveBayesMultinomialText1.getPeriodicPruning());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText1.minWordFrequencyTipText());
      assertEquals(3.0, naiveBayesMultinomialText1.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText1.LNormTipText());
      assertFalse(naiveBayesMultinomialText1.getLowercaseTokens());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText1.useStopListTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText1.stemmerTipText());
      assertFalse(naiveBayesMultinomialText1.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText1.globalInfo());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText1.normTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText1.periodicPruningTipText());
      assertFalse(naiveBayesMultinomialText1.getUseStopList());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText1.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText1.getDebug());
      assertEquals(1.0, naiveBayesMultinomialText1.getNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText1.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText1.stopwordsTipText());
      assertEquals(188, stringArray0.length);
      
      String[] stringArray2 = naiveBayesMultinomialText1.getOptions();
      assertNotSame(stringArray2, stringArray1);
      assertNotSame(stringArray2, stringArray0);
      assertNotSame(naiveBayesMultinomialText1, naiveBayesMultinomialText2);
      assertNotSame(naiveBayesMultinomialText1, naiveBayesMultinomialText0);
      assertNotNull(stringArray2);
      assertFalse(stringArray2.equals((Object)stringArray1));
      assertFalse(stringArray2.equals((Object)stringArray0));
      assertFalse(naiveBayesMultinomialText1.equals((Object)naiveBayesMultinomialText2));
      assertFalse(naiveBayesMultinomialText1.equals((Object)naiveBayesMultinomialText0));
      assertEquals(2.0, naiveBayesMultinomialText1.getLNorm(), 0.01);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText1.normalizeDocLengthTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText1.useWordFrequenciesTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText1.tokenizerTipText());
      assertFalse(naiveBayesMultinomialText1.getNormalizeDocLength());
      assertEquals(0, naiveBayesMultinomialText1.getPeriodicPruning());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText1.minWordFrequencyTipText());
      assertEquals(3.0, naiveBayesMultinomialText1.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText1.LNormTipText());
      assertFalse(naiveBayesMultinomialText1.getLowercaseTokens());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText1.useStopListTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText1.stemmerTipText());
      assertFalse(naiveBayesMultinomialText1.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText1.globalInfo());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText1.normTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText1.periodicPruningTipText());
      assertFalse(naiveBayesMultinomialText1.getUseStopList());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText1.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText1.getDebug());
      assertEquals(1.0, naiveBayesMultinomialText1.getNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText1.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText1.stopwordsTipText());
      assertEquals(12, stringArray2.length);
      
      naiveBayesMultinomialText2.setOptions(stringArray1);
      assertNotSame(naiveBayesMultinomialText2, naiveBayesMultinomialText0);
      assertNotSame(naiveBayesMultinomialText2, naiveBayesMultinomialText1);
      assertNotSame(stringArray1, stringArray2);
      assertNotSame(stringArray1, stringArray0);
      assertFalse(naiveBayesMultinomialText2.equals((Object)naiveBayesMultinomialText0));
      assertFalse(naiveBayesMultinomialText2.equals((Object)naiveBayesMultinomialText1));
      assertFalse(stringArray1.equals((Object)stringArray2));
      assertFalse(stringArray1.equals((Object)stringArray0));
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText2.lowercaseTokensTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText2.tokenizerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText2.minWordFrequencyTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText2.stopwordsTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText2.normTipText());
      assertEquals(2.0, naiveBayesMultinomialText2.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText2.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText2.globalInfo());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText2.debugTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText2.normalizeDocLengthTipText());
      assertEquals(1.0, naiveBayesMultinomialText2.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText2.getDebug());
      assertFalse(naiveBayesMultinomialText2.getUseStopList());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText2.useWordFrequenciesTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText2.periodicPruningTipText());
      assertEquals(0, naiveBayesMultinomialText2.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText2.getNormalizeDocLength());
      assertEquals(3.0, naiveBayesMultinomialText2.getMinWordFrequency(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText2.useStopListTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText2.stemmerTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText2.LNormTipText());
      assertFalse(naiveBayesMultinomialText2.getLowercaseTokens());
      assertEquals(12, stringArray1.length);
      
      Random.setNextRandom((-2663));
      double[] doubleArray2 = naiveBayesMultinomialText0.distributionForInstance(denseInstance0);
      assertNotSame(doubleArray2, doubleArray0);
      assertNotSame(doubleArray2, doubleArray1);
      assertNotSame(naiveBayesMultinomialText0, naiveBayesMultinomialText1);
      assertNotSame(naiveBayesMultinomialText0, naiveBayesMultinomialText2);
      assertNotSame(inputMappedClassifier0, inputMappedClassifier1);
      assertNotSame(capabilities0, capabilities1);
      assertNotSame(testInstances0, testInstances1);
      assertNotNull(doubleArray2);
      assertArrayEquals(new double[] {0.38461538461538475, 0.12820512820512825, 0.3589743589743588, 0.12820512820512825}, doubleArray2, 0.01);
      assertArrayEquals(new int[] {}, intArray0);
      assertFalse(doubleArray2.equals((Object)doubleArray0));
      assertFalse(doubleArray2.equals((Object)doubleArray1));
      assertFalse(naiveBayesMultinomialText0.equals((Object)naiveBayesMultinomialText1));
      assertFalse(naiveBayesMultinomialText0.equals((Object)naiveBayesMultinomialText2));
      assertFalse(inputMappedClassifier0.equals((Object)inputMappedClassifier1));
      assertFalse(capabilities0.equals((Object)capabilities1));
      assertFalse(testInstances0.equals((Object)testInstances1));
      assertEquals(0, binarySparseInstance0.numValues());
      assertEquals((-267.77445), binarySparseInstance0.weight(), 0.01);
      assertEquals((-2), binarySparseInstance0.numAttributes());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertTrue(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertTrue(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertTrue(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals(0, inputMappedClassifier0.graphType());
      assertTrue(inputMappedClassifier0.getTrim());
      assertEquals("Ignore case when matching attribute names and nomina values.", inputMappedClassifier0.ignoreCaseForNamesTipText());
      assertEquals("Wrapper classifier that addresses incompatible training and test data by building a mapping between the training data that a classifier has been built with and the incoming test instances' structure. Model attributes that are not found in the incoming instances receive missing values, so do incoming nominal attribute values that the classifier has not seen before. A new classifier can be trained or an existing one loaded from a file.", inputMappedClassifier0.globalInfo());
      assertEquals("", inputMappedClassifier0.getModelPath());
      assertFalse(inputMappedClassifier0.getDebug());
      assertEquals("Don't output a report of model-to-input mappings.", inputMappedClassifier0.suppressMappingReportTipText());
      assertTrue(inputMappedClassifier0.getIgnoreCaseForNames());
      assertEquals("The base classifier to be used.", inputMappedClassifier0.classifierTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", inputMappedClassifier0.debugTipText());
      assertFalse(inputMappedClassifier0.getSuppressMappingReport());
      assertEquals("Set the path from which to load a model. Loading occurs when the first test instance is received. Environment variables can be used in the supplied path.", inputMappedClassifier0.modelPathTipText());
      assertEquals("Trim white space from each end of attribute names and nominal values before matching.", inputMappedClassifier0.trimTipText());
      assertTrue(capabilities0.hasDependencies());
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(20, testInstances0.getNumInstances());
      assertFalse(testInstances0.getNoClass());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(5, testInstances0.getNumAttributes());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(4, testInstances0.getNumClasses());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(1, testInstances0.getSeed());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(1, testInstances0.getNumString());
      assertEquals(1, testInstances0.getNumRelationalString());
      assertFalse(linearNNSearch0.getMeasurePerformance());
      assertFalse(linearNNSearch0.getSkipIdentical());
      assertEquals("Whether to skip identical instances (with distance 0 to the target)", linearNNSearch0.skipIdenticalTipText());
      assertEquals("Class implementing the brute force search algorithm for nearest neighbour search.", linearNNSearch0.globalInfo());
      assertEquals("The distance function to use for finding neighbours (default: weka.core.EuclideanDistance). ", linearNNSearch0.distanceFunctionTipText());
      assertEquals("Whether to calculate performance statistics for the NN search or not", linearNNSearch0.measurePerformanceTipText());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(4, instances0.numClasses());
      assertTrue(instances0.checkForStringAttributes());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(20, instances0.size());
      assertEquals(4, instances0.classIndex());
      assertEquals(5, instances0.numAttributes());
      assertEquals(20, instances0.numInstances());
      assertEquals(4, denseInstance0.numClasses());
      assertEquals(5, denseInstance0.numValues());
      assertEquals(1.0, denseInstance0.weight(), 0.01);
      assertEquals(4, denseInstance0.classIndex());
      assertEquals(5, denseInstance0.numAttributes());
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertEquals(4, doubleArray2.length);
      assertEquals(0, intArray0.length);
  }
}
