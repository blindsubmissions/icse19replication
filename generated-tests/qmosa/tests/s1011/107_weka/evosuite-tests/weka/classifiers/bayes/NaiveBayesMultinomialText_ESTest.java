/*
 * This file was automatically generated by EvoSuite
 * Fri Aug 24 11:20:06 GMT 2018
 */

package weka.classifiers.bayes;

import org.junit.Test;
import static org.junit.Assert.*;
import static org.evosuite.runtime.EvoAssertions.*;
import java.io.File;
import java.net.URI;
import java.util.ArrayList;
import java.util.Enumeration;
import java.util.LinkedHashMap;
import java.util.Map;
import java.util.Vector;
import org.evosuite.runtime.EvoRunner;
import org.evosuite.runtime.EvoRunnerParameters;
import org.evosuite.runtime.Random;
import org.evosuite.runtime.System;
import org.evosuite.runtime.mock.java.io.MockFile;
import org.evosuite.runtime.testdata.EvoSuiteFile;
import org.evosuite.runtime.testdata.FileSystemHandling;
import org.junit.runner.RunWith;
import weka.attributeSelection.PrincipalComponents;
import weka.classifiers.AbstractClassifier;
import weka.classifiers.bayes.NaiveBayesMultinomialText;
import weka.classifiers.functions.SGDText;
import weka.classifiers.functions.supportVector.PrecomputedKernelMatrixKernel;
import weka.classifiers.lazy.IBk;
import weka.core.AbstractInstance;
import weka.core.Attribute;
import weka.core.BinarySparseInstance;
import weka.core.Capabilities;
import weka.core.DenseInstance;
import weka.core.FindWithCapabilities;
import weka.core.Instance;
import weka.core.Instances;
import weka.core.Option;
import weka.core.SparseInstance;
import weka.core.TestInstances;
import weka.core.neighboursearch.CoverTree;
import weka.core.neighboursearch.balltrees.BallNode;
import weka.core.stemmers.IteratedLovinsStemmer;
import weka.core.stemmers.LovinsStemmer;
import weka.core.stemmers.NullStemmer;
import weka.core.stemmers.SnowballStemmer;
import weka.core.stemmers.Stemmer;
import weka.core.tokenizers.NGramTokenizer;
import weka.core.tokenizers.Tokenizer;
import weka.core.tokenizers.WordTokenizer;
import weka.estimators.DiscreteEstimator;

@RunWith(EvoRunner.class) @EvoRunnerParameters(mockJVMNonDeterminism = true, useVFS = true, useVNET = true, resetStaticState = true, separateClassLoader = true, useJEE = true) 
public class NaiveBayesMultinomialText_ESTest extends NaiveBayesMultinomialText_ESTest_scaffolding {

  /**
  //Test case number: 0
  /*Coverage entropy=1.6094379124341005
  */
  @Test(timeout = 4000)
  public void test00()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.pruneDictionary();
      naiveBayesMultinomialText0.setNormalizeDocLength(true);
      naiveBayesMultinomialText0.setUseWordFrequencies(true);
      naiveBayesMultinomialText0.getNormalizeDocLength();
      String[] stringArray0 = null;
      try { 
        AbstractClassifier.forName("-stopwords", (String[]) null);
        fail("Expecting exception: Exception");
      
      } catch(Exception e) {
         //
         // Can't find class called: -stopwords
         //
         verifyException("weka.core.Utils", e);
      }
  }

  /**
  //Test case number: 1
  /*Coverage entropy=1.945910149055313
  */
  @Test(timeout = 4000)
  public void test01()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.toString();
      naiveBayesMultinomialText0.setNorm(1222.8);
      naiveBayesMultinomialText0.setPeriodicPruning(10000);
      naiveBayesMultinomialText0.getCapabilities();
      String[] stringArray0 = new String[0];
      NaiveBayesMultinomialText.main(stringArray0);
      NaiveBayesMultinomialText.main(stringArray0);
      naiveBayesMultinomialText0.toString();
      assertEquals(1222.8, naiveBayesMultinomialText0.getNorm(), 0.01);
  }

  /**
  //Test case number: 2
  /*Coverage entropy=2.3978952727983707
  */
  @Test(timeout = 4000)
  public void test02()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      String[] stringArray0 = new String[6];
      stringArray0[0] = "";
      stringArray0[1] = "-string <num>";
      stringArray0[2] = "JO";
      stringArray0[3] = "(<x,x>^";
      stringArray0[4] = "";
      stringArray0[5] = "ty";
      naiveBayesMultinomialText0.setOptions(stringArray0);
      String string0 = naiveBayesMultinomialText0.lowercaseTokensTipText();
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("Whether to convert all tokens to lowercase", string0);
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
  }

  /**
  //Test case number: 3
  /*Coverage entropy=0.6931471805599453
  */
  @Test(timeout = 4000)
  public void test03()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.globalInfo();
      SGDText sGDText0 = new SGDText();
      sGDText0.getStopwords();
      String[] stringArray0 = new String[1];
      stringArray0[0] = "Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification";
      try { 
        AbstractClassifier.forName("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", stringArray0);
        fail("Expecting exception: Exception");
      
      } catch(Exception e) {
         //
         // Can't find class called: Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification
         //
         verifyException("weka.core.Utils", e);
      }
  }

  /**
  //Test case number: 4
  /*Coverage entropy=0.6931471805599453
  */
  @Test(timeout = 4000)
  public void test04()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      String string0 = naiveBayesMultinomialText0.stopwordsTipText();
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", string0);
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
  }

  /**
  //Test case number: 5
  /*Coverage entropy=1.6094379124341005
  */
  @Test(timeout = 4000)
  public void test05()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.setDebug(false);
      Capabilities capabilities0 = naiveBayesMultinomialText0.getCapabilities();
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      
      naiveBayesMultinomialText0.reset();
      String string0 = naiveBayesMultinomialText0.tokenizerTipText();
      assertEquals("The tokenizing algorithm to use on the strings.", string0);
      
      String string1 = naiveBayesMultinomialText0.toString();
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("NaiveBayesMultinomialText: No model built yet.\n", string1);
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
  }

  /**
  //Test case number: 6
  /*Coverage entropy=1.6094379124341005
  */
  @Test(timeout = 4000)
  public void test06()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      boolean boolean0 = false;
      PrincipalComponents principalComponents0 = new PrincipalComponents();
      NaiveBayesMultinomialText naiveBayesMultinomialText1 = new NaiveBayesMultinomialText();
      File file0 = naiveBayesMultinomialText1.getStopwords();
      naiveBayesMultinomialText0.setStopwords(file0);
      SparseInstance sparseInstance0 = new SparseInstance(1110);
      int int0 = 500;
      try { 
        naiveBayesMultinomialText0.distributionForInstance(sparseInstance0);
        fail("Expecting exception: RuntimeException");
      
      } catch(RuntimeException e) {
         //
         // DenseInstance doesn't have access to a dataset!
         //
         verifyException("weka.core.AbstractInstance", e);
      }
  }

  /**
  //Test case number: 7
  /*Coverage entropy=2.70805020110221
  */
  @Test(timeout = 4000)
  public void test07()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.m_stopwordsFile = null;
      AbstractClassifier.makeCopies(naiveBayesMultinomialText0, 1555);
      naiveBayesMultinomialText0.m_minWordP = 0.0;
      IteratedLovinsStemmer iteratedLovinsStemmer0 = new IteratedLovinsStemmer();
      naiveBayesMultinomialText0.setStemmer(iteratedLovinsStemmer0);
      naiveBayesMultinomialText0.setLowercaseTokens(true);
      boolean boolean0 = false;
      naiveBayesMultinomialText0.m_lowercaseTokens = false;
      naiveBayesMultinomialText0.listOptions();
      naiveBayesMultinomialText0.setDebug(false);
      naiveBayesMultinomialText0.setNorm(0.0);
      // Undeclared exception!
      try { 
        naiveBayesMultinomialText0.getOptions();
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("weka.classifiers.bayes.NaiveBayesMultinomialText", e);
      }
  }

  /**
  //Test case number: 8
  /*Coverage entropy=2.890371757896165
  */
  @Test(timeout = 4000)
  public void test08()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      String[] stringArray0 = new String[9];
      stringArray0[0] = "F%~~/|1v)X@";
      stringArray0[1] = "rn&axH+)i";
      stringArray0[2] = "tokenizer";
      stringArray0[3] = "s-l~n)w~v,R?>v)";
      stringArray0[4] = "14th International Joint Conference on Articial Intelligence";
      stringArray0[5] = "";
      stringArray0[6] = "[bG 2eovE  $>kKLZ";
      stringArray0[7] = "of a differnet structure than the incoming training data";
      stringArray0[8] = "cc/)";
      AbstractClassifier.runClassifier(naiveBayesMultinomialText0, stringArray0);
      naiveBayesMultinomialText0.m_probOfWordGivenClass = null;
      naiveBayesMultinomialText0.getOptions();
      naiveBayesMultinomialText0.m_normalize = true;
      SnowballStemmer snowballStemmer0 = new SnowballStemmer();
      snowballStemmer0.stem("");
      naiveBayesMultinomialText0.setStemmer(snowballStemmer0);
      naiveBayesMultinomialText0.listOptions();
      naiveBayesMultinomialText0.setNorm(0.1);
      naiveBayesMultinomialText0.reset();
      naiveBayesMultinomialText0.getMinWordFrequency();
      naiveBayesMultinomialText0.useStopListTipText();
      assertEquals(0.1, naiveBayesMultinomialText0.getNorm(), 0.01);
  }

  /**
  //Test case number: 9
  /*Coverage entropy=1.6094379124341005
  */
  @Test(timeout = 4000)
  public void test09()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      String string0 = naiveBayesMultinomialText0.periodicPruningTipText();
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", string0);
      
      naiveBayesMultinomialText0.getLowercaseTokens();
      boolean boolean0 = naiveBayesMultinomialText0.getNormalizeDocLength();
      assertFalse(boolean0);
      
      naiveBayesMultinomialText0.getStopwords();
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
  }

  /**
  //Test case number: 10
  /*Coverage entropy=0.6931471805599453
  */
  @Test(timeout = 4000)
  public void test10()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      String string0 = naiveBayesMultinomialText0.normalizeDocLengthTipText();
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", string0);
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
  }

  /**
  //Test case number: 11
  /*Coverage entropy=2.833213344056216
  */
  @Test(timeout = 4000)
  public void test11()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.setLNorm((-755.0049937));
      naiveBayesMultinomialText0.m_norm = (-755.0049937);
      naiveBayesMultinomialText0.m_wordFrequencies = false;
      naiveBayesMultinomialText0.m_stemmer = null;
      naiveBayesMultinomialText0.getTokenizer();
      naiveBayesMultinomialText0.m_stemmer = null;
      naiveBayesMultinomialText0.setPeriodicPruning(1195);
      naiveBayesMultinomialText0.minWordFrequencyTipText();
      naiveBayesMultinomialText0.m_leplace = 166.6739689172;
      naiveBayesMultinomialText0.setUseStopList(true);
      naiveBayesMultinomialText0.getUseWordFrequencies();
      naiveBayesMultinomialText0.setNorm((-755.0049937));
      naiveBayesMultinomialText0.stemmerTipText();
      String[] stringArray0 = new String[9];
      stringArray0[0] = "";
      stringArray0[1] = "The stemming algorithm to use on the words.";
      stringArray0[2] = "The stemming algorithm to use on the words.";
      stringArray0[3] = "Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value";
      stringArray0[4] = "Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value";
      stringArray0[5] = "Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value";
      stringArray0[6] = "The stemming algorithm to use on the words.";
      stringArray0[7] = "Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value";
      stringArray0[8] = "The stemming algorithm to use on the words.";
      naiveBayesMultinomialText0.setOptions(stringArray0);
      assertEquals((-755.0049937), naiveBayesMultinomialText0.getNorm(), 0.01);
  }

  /**
  //Test case number: 12
  /*Coverage entropy=2.3025850929940455
  */
  @Test(timeout = 4000)
  public void test12()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      assertNotNull(naiveBayesMultinomialText0);
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      
      naiveBayesMultinomialText0.m_wordsPerClass = null;
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      
      naiveBayesMultinomialText0.setNormalizeDocLength(false);
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      
      Capabilities capabilities0 = naiveBayesMultinomialText0.getCapabilities();
      assertNotNull(capabilities0);
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      assertFalse(capabilities0.hasDependencies());
      
      boolean boolean0 = naiveBayesMultinomialText0.getNormalizeDocLength();
      assertFalse(boolean0);
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      
      WordTokenizer wordTokenizer0 = (WordTokenizer)naiveBayesMultinomialText0.getTokenizer();
      assertNotNull(wordTokenizer0);
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals(" \r\n\t.,;:'\"()?!", wordTokenizer0.getDelimiters());
      assertEquals("A simple tokenizer that is using the java.util.StringTokenizer class to tokenize the strings.", wordTokenizer0.globalInfo());
      assertEquals("Set of delimiter characters to use in tokenizing (\\r, \\n and \\t can be used for carriage-return, line-feed and tab)", wordTokenizer0.delimitersTipText());
      
      String string0 = naiveBayesMultinomialText0.periodicPruningTipText();
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", string0);
      assertNotNull(string0);
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      
      naiveBayesMultinomialText0.setPeriodicPruning((-291));
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals((-291), naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      
      String string1 = naiveBayesMultinomialText0.stemmerTipText();
      assertEquals("The stemming algorithm to use on the words.", string1);
      assertFalse(string1.equals((Object)string0));
      assertNotNull(string1);
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals((-291), naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      
      try { 
        naiveBayesMultinomialText0.updateClassifier((Instance) null);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("weka.classifiers.bayes.NaiveBayesMultinomialText", e);
      }
  }

  /**
  //Test case number: 13
  /*Coverage entropy=1.8380834670331858
  */
  @Test(timeout = 4000)
  public void test13()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      assertNotNull(naiveBayesMultinomialText0);
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      
      String[] stringArray0 = new String[8];
      stringArray0[0] = "-stemmer";
      stringArray0[1] = "hA`|";
      stringArray0[2] = "&{{BkF\"o1.<c";
      stringArray0[3] = "0)";
      stringArray0[4] = "stopwords";
      stringArray0[5] = "/FN^?JkuV";
      stringArray0[6] = "  Precision  Recall";
      stringArray0[7] = "";
      try { 
        naiveBayesMultinomialText0.setOptions(stringArray0);
        fail("Expecting exception: ClassNotFoundException");
      
      } catch(ClassNotFoundException e) {
      }
  }

  /**
  //Test case number: 14
  /*Coverage entropy=1.0986122886681096
  */
  @Test(timeout = 4000)
  public void test14()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      assertNotNull(naiveBayesMultinomialText0);
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      
      NGramTokenizer nGramTokenizer0 = new NGramTokenizer();
      assertNotNull(nGramTokenizer0);
      assertEquals("The min N of the NGram.", nGramTokenizer0.NGramMinSizeTipText());
      assertEquals(1, nGramTokenizer0.getNGramMinSize());
      assertEquals("Splits a string into an n-gram with min and max grams.", nGramTokenizer0.globalInfo());
      assertFalse(nGramTokenizer0.hasMoreElements());
      assertEquals("Set of delimiter characters to use in tokenizing (\\r, \\n and \\t can be used for carriage-return, line-feed and tab)", nGramTokenizer0.delimitersTipText());
      assertEquals(" \r\n\t.,;:'\"()?!", nGramTokenizer0.getDelimiters());
      assertEquals(3, nGramTokenizer0.getNGramMaxSize());
      assertEquals("The max N of the NGram.", nGramTokenizer0.NGramMaxSizeTipText());
      
      Enumeration enumeration0 = nGramTokenizer0.listOptions();
      assertNotNull(enumeration0);
      assertEquals("The min N of the NGram.", nGramTokenizer0.NGramMinSizeTipText());
      assertEquals(1, nGramTokenizer0.getNGramMinSize());
      assertEquals("Splits a string into an n-gram with min and max grams.", nGramTokenizer0.globalInfo());
      assertFalse(nGramTokenizer0.hasMoreElements());
      assertEquals("Set of delimiter characters to use in tokenizing (\\r, \\n and \\t can be used for carriage-return, line-feed and tab)", nGramTokenizer0.delimitersTipText());
      assertEquals(" \r\n\t.,;:'\"()?!", nGramTokenizer0.getDelimiters());
      assertEquals(3, nGramTokenizer0.getNGramMaxSize());
      assertEquals("The max N of the NGram.", nGramTokenizer0.NGramMaxSizeTipText());
      
      nGramTokenizer0.setDelimiters("- not allowed: ");
      assertEquals("The min N of the NGram.", nGramTokenizer0.NGramMinSizeTipText());
      assertEquals(1, nGramTokenizer0.getNGramMinSize());
      assertEquals("Splits a string into an n-gram with min and max grams.", nGramTokenizer0.globalInfo());
      assertFalse(nGramTokenizer0.hasMoreElements());
      assertEquals("Set of delimiter characters to use in tokenizing (\\r, \\n and \\t can be used for carriage-return, line-feed and tab)", nGramTokenizer0.delimitersTipText());
      assertEquals(3, nGramTokenizer0.getNGramMaxSize());
      assertEquals("The max N of the NGram.", nGramTokenizer0.NGramMaxSizeTipText());
      assertEquals("- not allowed: ", nGramTokenizer0.getDelimiters());
      
      naiveBayesMultinomialText0.setTokenizer(nGramTokenizer0);
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The min N of the NGram.", nGramTokenizer0.NGramMinSizeTipText());
      assertEquals(1, nGramTokenizer0.getNGramMinSize());
      assertEquals("Splits a string into an n-gram with min and max grams.", nGramTokenizer0.globalInfo());
      assertFalse(nGramTokenizer0.hasMoreElements());
      assertEquals("Set of delimiter characters to use in tokenizing (\\r, \\n and \\t can be used for carriage-return, line-feed and tab)", nGramTokenizer0.delimitersTipText());
      assertEquals(3, nGramTokenizer0.getNGramMaxSize());
      assertEquals("The max N of the NGram.", nGramTokenizer0.NGramMaxSizeTipText());
      assertEquals("- not allowed: ", nGramTokenizer0.getDelimiters());
      
      String string0 = naiveBayesMultinomialText0.getRevision();
      assertEquals("9122", string0);
      assertNotNull(string0);
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
  }

  /**
  //Test case number: 15
  /*Coverage entropy=1.6094379124341005
  */
  @Test(timeout = 4000)
  public void test15()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      assertNotNull(naiveBayesMultinomialText0);
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      
      naiveBayesMultinomialText0.setDebug(true);
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertTrue(naiveBayesMultinomialText0.getDebug());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      
      MockFile mockFile0 = new MockFile("hV:lcw'T", "hV:lcw'T");
      assertNotNull(mockFile0);
      
      URI uRI0 = mockFile0.toURI();
      assertNotNull(uRI0);
      assertEquals("hV:lcw'T", mockFile0.getName());
      assertEquals(0L, mockFile0.getFreeSpace());
      assertEquals(0L, mockFile0.length());
      assertFalse(mockFile0.isDirectory());
      assertEquals("/home/ubuntu/evosuite_readability_gen/projects/107_weka/hV:lcw'T/hV:lcw'T", mockFile0.toString());
      assertEquals(0L, mockFile0.getTotalSpace());
      assertEquals("/home/ubuntu/evosuite_readability_gen/projects/107_weka/hV:lcw'T", mockFile0.getParent());
      assertFalse(mockFile0.canExecute());
      assertFalse(mockFile0.canRead());
      assertFalse(mockFile0.exists());
      assertEquals(0L, mockFile0.lastModified());
      assertEquals(0L, mockFile0.getUsableSpace());
      assertFalse(mockFile0.isHidden());
      assertTrue(mockFile0.isFile());
      assertTrue(mockFile0.isAbsolute());
      assertFalse(mockFile0.canWrite());
      assertFalse(uRI0.isOpaque());
      assertNull(uRI0.getRawUserInfo());
      assertNull(uRI0.getRawAuthority());
      assertTrue(uRI0.isAbsolute());
      assertNull(uRI0.getRawQuery());
      assertNull(uRI0.getHost());
      assertEquals("file", uRI0.getScheme());
      assertEquals((-1), uRI0.getPort());
      assertEquals("file:/home/ubuntu/evosuite_readability_gen/projects/107_weka/hV:lcw'T%7F/hV:lcw'T%7F", uRI0.toString());
      assertNull(uRI0.getRawFragment());
      assertEquals("/home/ubuntu/evosuite_readability_gen/projects/107_weka/hV:lcw'T%7F/hV:lcw'T%7F", uRI0.getRawPath());
      
      boolean boolean0 = mockFile0.delete();
      assertFalse(boolean0);
      assertEquals("hV:lcw'T", mockFile0.getName());
      assertEquals(0L, mockFile0.getFreeSpace());
      assertEquals(0L, mockFile0.length());
      assertFalse(mockFile0.isDirectory());
      assertEquals("/home/ubuntu/evosuite_readability_gen/projects/107_weka/hV:lcw'T/hV:lcw'T", mockFile0.toString());
      assertEquals(0L, mockFile0.getTotalSpace());
      assertEquals("/home/ubuntu/evosuite_readability_gen/projects/107_weka/hV:lcw'T", mockFile0.getParent());
      assertFalse(mockFile0.canExecute());
      assertFalse(mockFile0.canRead());
      assertFalse(mockFile0.exists());
      assertEquals(0L, mockFile0.lastModified());
      assertEquals(0L, mockFile0.getUsableSpace());
      assertFalse(mockFile0.isHidden());
      assertTrue(mockFile0.isFile());
      assertTrue(mockFile0.isAbsolute());
      assertFalse(mockFile0.canWrite());
      
      naiveBayesMultinomialText0.setStopwords(mockFile0);
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertTrue(naiveBayesMultinomialText0.getDebug());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("hV:lcw'T", mockFile0.getName());
      assertEquals(0L, mockFile0.getFreeSpace());
      assertEquals(0L, mockFile0.length());
      assertFalse(mockFile0.isDirectory());
      assertEquals("/home/ubuntu/evosuite_readability_gen/projects/107_weka/hV:lcw'T/hV:lcw'T", mockFile0.toString());
      assertEquals(0L, mockFile0.getTotalSpace());
      assertEquals("/home/ubuntu/evosuite_readability_gen/projects/107_weka/hV:lcw'T", mockFile0.getParent());
      assertFalse(mockFile0.canExecute());
      assertFalse(mockFile0.canRead());
      assertFalse(mockFile0.exists());
      assertEquals(0L, mockFile0.lastModified());
      assertEquals(0L, mockFile0.getUsableSpace());
      assertFalse(mockFile0.isHidden());
      assertTrue(mockFile0.isFile());
      assertTrue(mockFile0.isAbsolute());
      assertFalse(mockFile0.canWrite());
      
      String string0 = naiveBayesMultinomialText0.minWordFrequencyTipText();
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", string0);
      assertNotNull(string0);
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertTrue(naiveBayesMultinomialText0.getDebug());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      
      naiveBayesMultinomialText0.m_probOfClass = null;
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertTrue(naiveBayesMultinomialText0.getDebug());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      
      double double0 = naiveBayesMultinomialText0.getNorm();
      assertEquals(1.0, double0, 0.01);
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertTrue(naiveBayesMultinomialText0.getDebug());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      
      String string1 = naiveBayesMultinomialText0.LNormTipText();
      assertEquals("The LNorm to use for document length normalization.", string1);
      assertFalse(string1.equals((Object)string0));
      assertNotNull(string1);
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertTrue(naiveBayesMultinomialText0.getDebug());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
  }

  /**
  //Test case number: 16
  /*Coverage entropy=2.7184734013300127
  */
  @Test(timeout = 4000)
  public void test16()  throws Throwable  {
      boolean boolean0 = FileSystemHandling.shouldThrowIOException((EvoSuiteFile) null);
      assertFalse(boolean0);
      
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      assertNotNull(naiveBayesMultinomialText0);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      
      naiveBayesMultinomialText0.m_lowercaseTokens = false;
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      
      naiveBayesMultinomialText0.setUseStopList(false);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      
      boolean boolean1 = FileSystemHandling.shouldThrowIOException((EvoSuiteFile) null);
      assertFalse(boolean1);
      assertTrue(boolean1 == boolean0);
      
      String string0 = naiveBayesMultinomialText0.getRevision();
      assertEquals("9122", string0);
      assertNotNull(string0);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      
      String string1 = naiveBayesMultinomialText0.getRevision();
      assertEquals("9122", string1);
      assertTrue(string1.equals((Object)string0));
      assertNotNull(string1);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      
      double[] doubleArray0 = new double[9];
      doubleArray0[0] = 2.0;
      doubleArray0[1] = 2.0;
      doubleArray0[2] = 2.0;
      doubleArray0[3] = 2.0;
      doubleArray0[4] = 2.0;
      doubleArray0[5] = 2.0;
      doubleArray0[6] = 2.0;
      doubleArray0[7] = 2.0;
      doubleArray0[8] = 2.0;
      naiveBayesMultinomialText0.m_wordsPerClass = doubleArray0;
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      
      naiveBayesMultinomialText0.m_useStopList = true;
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertTrue(naiveBayesMultinomialText0.getUseStopList());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      
      SnowballStemmer snowballStemmer0 = new SnowballStemmer();
      assertNotNull(snowballStemmer0);
      assertEquals("A wrapper class for the Snowball stemmers. Only available if the Snowball classes are in the classpath.\nIf the class discovery is not dynamic, i.e., the property 'UseDynamic' in the props file 'weka/gui/GenericPropertiesCreator.props' is 'false', then the property 'org.tartarus.snowball.SnowballProgram' in the 'weka/gui/GenericObjectEditor.props' file has to be uncommented as well. If necessary you have to discover and fill in the snowball stemmers manually. You can use the 'weka.core.ClassDiscovery' for this:\n  java weka.core.ClassDiscovery org.tartarus.snowball.SnowballProgram org.tartarus.snowball.ext\n\nFor more information visit these web sites:\n  http://weka.wikispaces.com/Stemmers\n  http://snowball.tartarus.org/\n", snowballStemmer0.globalInfo());
      assertFalse(snowballStemmer0.isPresent());
      
      String[] stringArray0 = new String[0];
      Enumeration enumeration0 = SnowballStemmer.listStemmers();
      assertNotNull(enumeration0);
      
      Enumeration enumeration1 = snowballStemmer0.listOptions();
      assertFalse(enumeration1.equals((Object)enumeration0));
      assertNotNull(enumeration1);
      assertNotSame(enumeration1, enumeration0);
      assertEquals("A wrapper class for the Snowball stemmers. Only available if the Snowball classes are in the classpath.\nIf the class discovery is not dynamic, i.e., the property 'UseDynamic' in the props file 'weka/gui/GenericPropertiesCreator.props' is 'false', then the property 'org.tartarus.snowball.SnowballProgram' in the 'weka/gui/GenericObjectEditor.props' file has to be uncommented as well. If necessary you have to discover and fill in the snowball stemmers manually. You can use the 'weka.core.ClassDiscovery' for this:\n  java weka.core.ClassDiscovery org.tartarus.snowball.SnowballProgram org.tartarus.snowball.ext\n\nFor more information visit these web sites:\n  http://weka.wikispaces.com/Stemmers\n  http://snowball.tartarus.org/\n", snowballStemmer0.globalInfo());
      assertFalse(snowballStemmer0.isPresent());
      
      SnowballStemmer.main(stringArray0);
      assertEquals(0, stringArray0.length);
      
      naiveBayesMultinomialText0.setStemmer(snowballStemmer0);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertTrue(naiveBayesMultinomialText0.getUseStopList());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("A wrapper class for the Snowball stemmers. Only available if the Snowball classes are in the classpath.\nIf the class discovery is not dynamic, i.e., the property 'UseDynamic' in the props file 'weka/gui/GenericPropertiesCreator.props' is 'false', then the property 'org.tartarus.snowball.SnowballProgram' in the 'weka/gui/GenericObjectEditor.props' file has to be uncommented as well. If necessary you have to discover and fill in the snowball stemmers manually. You can use the 'weka.core.ClassDiscovery' for this:\n  java weka.core.ClassDiscovery org.tartarus.snowball.SnowballProgram org.tartarus.snowball.ext\n\nFor more information visit these web sites:\n  http://weka.wikispaces.com/Stemmers\n  http://snowball.tartarus.org/\n", snowballStemmer0.globalInfo());
      assertFalse(snowballStemmer0.isPresent());
      
      naiveBayesMultinomialText0.setLNorm(2.0);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertTrue(naiveBayesMultinomialText0.getUseStopList());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      
      String[] stringArray1 = naiveBayesMultinomialText0.getOptions();
      assertFalse(stringArray1.equals((Object)stringArray0));
      assertEquals(13, stringArray1.length);
      assertNotNull(stringArray1);
      assertNotSame(stringArray1, stringArray0);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertTrue(naiveBayesMultinomialText0.getUseStopList());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      
      String[] stringArray2 = naiveBayesMultinomialText0.getOptions();
      assertFalse(stringArray2.equals((Object)stringArray1));
      assertFalse(stringArray2.equals((Object)stringArray0));
      assertEquals(13, stringArray2.length);
      assertNotNull(stringArray2);
      assertNotSame(stringArray2, stringArray1);
      assertNotSame(stringArray2, stringArray0);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertTrue(naiveBayesMultinomialText0.getUseStopList());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      
      Random.setNextRandom(4);
      BinarySparseInstance binarySparseInstance0 = new BinarySparseInstance(4);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertNotNull(binarySparseInstance0);
      assertEquals(1.0, binarySparseInstance0.weight(), 0.01);
      assertEquals(4, binarySparseInstance0.numValues());
      assertEquals(4, binarySparseInstance0.numAttributes());
      
      // Undeclared exception!
      try { 
        binarySparseInstance0.relationalValue(4);
        fail("Expecting exception: RuntimeException");
      
      } catch(RuntimeException e) {
         //
         // DenseInstance doesn't have access to a dataset!
         //
         verifyException("weka.core.AbstractInstance", e);
      }
  }

  /**
  //Test case number: 17
  /*Coverage entropy=3.2058430290511546
  */
  @Test(timeout = 4000)
  public void test17()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      assertNotNull(naiveBayesMultinomialText0);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      
      SGDText sGDText0 = new SGDText();
      assertEquals(1, SGDText.LOGLOSS);
      assertEquals(0, SGDText.HINGE);
      assertNotNull(sGDText0);
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText0.globalInfo());
      assertEquals("The norm of the instances after normalization.", sGDText0.normTipText());
      assertEquals(1.0, sGDText0.getNorm(), 0.01);
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText0.tokenizerTipText());
      assertEquals(1, sGDText0.getSeed());
      assertEquals("The LNorm to use for document length normalization.", sGDText0.LNormTipText());
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText0.lossFunctionTipText());
      assertFalse(sGDText0.getUseWordFrequencies());
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText0.useWordFrequenciesTipText());
      assertEquals("The regularization constant. (default = 0.0001)", sGDText0.lambdaTipText());
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText0.outputProbsForSVMTipText());
      assertFalse(sGDText0.getOutputProbsForSVM());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText0.epochsTipText());
      assertEquals("Whether to convert all tokens to lowercase", sGDText0.lowercaseTokensTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText0.normalizeDocLengthTipText());
      assertEquals(2.0, sGDText0.getLNorm(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText0.stopwordsTipText());
      assertEquals(500, sGDText0.getEpochs());
      assertEquals(3.0, sGDText0.getMinWordFrequency(), 0.01);
      assertEquals(0, sGDText0.getPeriodicPruning());
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText0.debugTipText());
      assertFalse(sGDText0.getLowercaseTokens());
      assertFalse(sGDText0.getUseStopList());
      assertEquals(1.0E-4, sGDText0.getLambda(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText0.useStopListTipText());
      assertEquals("The random number seed to be used.", sGDText0.seedTipText());
      assertEquals(0.01, sGDText0.getLearningRate(), 0.01);
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText0.minWordFrequencyTipText());
      assertEquals("The stemming algorithm to use on the words.", sGDText0.stemmerTipText());
      assertFalse(sGDText0.getDebug());
      assertFalse(sGDText0.getNormalizeDocLength());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText0.periodicPruningTipText());
      assertEquals("The learning rate.", sGDText0.learningRateTipText());
      
      NullStemmer nullStemmer0 = (NullStemmer)sGDText0.getStemmer();
      assertEquals(1, SGDText.LOGLOSS);
      assertEquals(0, SGDText.HINGE);
      assertNotNull(nullStemmer0);
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText0.globalInfo());
      assertEquals("The norm of the instances after normalization.", sGDText0.normTipText());
      assertEquals(1.0, sGDText0.getNorm(), 0.01);
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText0.tokenizerTipText());
      assertEquals(1, sGDText0.getSeed());
      assertEquals("The LNorm to use for document length normalization.", sGDText0.LNormTipText());
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText0.lossFunctionTipText());
      assertFalse(sGDText0.getUseWordFrequencies());
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText0.useWordFrequenciesTipText());
      assertEquals("The regularization constant. (default = 0.0001)", sGDText0.lambdaTipText());
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText0.outputProbsForSVMTipText());
      assertFalse(sGDText0.getOutputProbsForSVM());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText0.epochsTipText());
      assertEquals("Whether to convert all tokens to lowercase", sGDText0.lowercaseTokensTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText0.normalizeDocLengthTipText());
      assertEquals(2.0, sGDText0.getLNorm(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText0.stopwordsTipText());
      assertEquals(500, sGDText0.getEpochs());
      assertEquals(3.0, sGDText0.getMinWordFrequency(), 0.01);
      assertEquals(0, sGDText0.getPeriodicPruning());
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText0.debugTipText());
      assertFalse(sGDText0.getLowercaseTokens());
      assertFalse(sGDText0.getUseStopList());
      assertEquals(1.0E-4, sGDText0.getLambda(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText0.useStopListTipText());
      assertEquals("The random number seed to be used.", sGDText0.seedTipText());
      assertEquals(0.01, sGDText0.getLearningRate(), 0.01);
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText0.minWordFrequencyTipText());
      assertEquals("The stemming algorithm to use on the words.", sGDText0.stemmerTipText());
      assertFalse(sGDText0.getDebug());
      assertFalse(sGDText0.getNormalizeDocLength());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText0.periodicPruningTipText());
      assertEquals("The learning rate.", sGDText0.learningRateTipText());
      assertEquals("A dummy stemmer that performs no stemming at all.", nullStemmer0.globalInfo());
      
      naiveBayesMultinomialText0.m_stemmer = (Stemmer) nullStemmer0;
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText0.globalInfo());
      assertEquals("The norm of the instances after normalization.", sGDText0.normTipText());
      assertEquals(1.0, sGDText0.getNorm(), 0.01);
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText0.tokenizerTipText());
      assertEquals(1, sGDText0.getSeed());
      assertEquals("The LNorm to use for document length normalization.", sGDText0.LNormTipText());
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText0.lossFunctionTipText());
      assertFalse(sGDText0.getUseWordFrequencies());
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText0.useWordFrequenciesTipText());
      assertEquals("The regularization constant. (default = 0.0001)", sGDText0.lambdaTipText());
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText0.outputProbsForSVMTipText());
      assertFalse(sGDText0.getOutputProbsForSVM());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText0.epochsTipText());
      assertEquals("Whether to convert all tokens to lowercase", sGDText0.lowercaseTokensTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText0.normalizeDocLengthTipText());
      assertEquals(2.0, sGDText0.getLNorm(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText0.stopwordsTipText());
      assertEquals(500, sGDText0.getEpochs());
      assertEquals(3.0, sGDText0.getMinWordFrequency(), 0.01);
      assertEquals(0, sGDText0.getPeriodicPruning());
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText0.debugTipText());
      assertFalse(sGDText0.getLowercaseTokens());
      assertFalse(sGDText0.getUseStopList());
      assertEquals(1.0E-4, sGDText0.getLambda(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText0.useStopListTipText());
      assertEquals("The random number seed to be used.", sGDText0.seedTipText());
      assertEquals(0.01, sGDText0.getLearningRate(), 0.01);
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText0.minWordFrequencyTipText());
      assertEquals("The stemming algorithm to use on the words.", sGDText0.stemmerTipText());
      assertFalse(sGDText0.getDebug());
      assertFalse(sGDText0.getNormalizeDocLength());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText0.periodicPruningTipText());
      assertEquals("The learning rate.", sGDText0.learningRateTipText());
      assertEquals("A dummy stemmer that performs no stemming at all.", nullStemmer0.globalInfo());
      
      naiveBayesMultinomialText0.setNorm(3803.12);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(3803.12, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      
      NullStemmer nullStemmer1 = (NullStemmer)naiveBayesMultinomialText0.m_stemmer;
      assertNotNull(nullStemmer1);
      assertSame(nullStemmer1, nullStemmer0);
      assertEquals("A dummy stemmer that performs no stemming at all.", nullStemmer1.globalInfo());
      
      naiveBayesMultinomialText0.setStemmer(nullStemmer1);
      assertSame(nullStemmer1, nullStemmer0);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(3803.12, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("A dummy stemmer that performs no stemming at all.", nullStemmer1.globalInfo());
      
      naiveBayesMultinomialText0.setNorm(3803.12);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(3803.12, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      
      Capabilities capabilities0 = naiveBayesMultinomialText0.getCapabilities();
      assertNotNull(capabilities0);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(3803.12, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      assertFalse(capabilities0.hasDependencies());
      
      naiveBayesMultinomialText0.setPeriodicPruning(0);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(3803.12, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      
      Enumeration<Option> enumeration0 = naiveBayesMultinomialText0.listOptions();
      assertNotNull(enumeration0);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(3803.12, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      
      naiveBayesMultinomialText0.setLNorm(4027.85446189);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(3803.12, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals(4027.85446189, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      
      String string0 = naiveBayesMultinomialText0.useWordFrequenciesTipText();
      assertEquals("Use word frequencies rather than binary bag of words representation", string0);
      assertNotNull(string0);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(3803.12, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals(4027.85446189, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      
      String string1 = naiveBayesMultinomialText0.stemmerTipText();
      assertEquals("The stemming algorithm to use on the words.", string1);
      assertFalse(string1.equals((Object)string0));
      assertNotNull(string1);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(3803.12, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals(4027.85446189, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      
      double double0 = naiveBayesMultinomialText0.getNorm();
      assertEquals(3803.12, double0, 0.01);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(3803.12, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals(4027.85446189, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      
      WordTokenizer wordTokenizer0 = (WordTokenizer)naiveBayesMultinomialText0.getTokenizer();
      assertNotNull(wordTokenizer0);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(3803.12, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals(4027.85446189, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(" \r\n\t.,;:'\"()?!", wordTokenizer0.getDelimiters());
      assertEquals("Set of delimiter characters to use in tokenizing (\\r, \\n and \\t can be used for carriage-return, line-feed and tab)", wordTokenizer0.delimitersTipText());
      assertEquals("A simple tokenizer that is using the java.util.StringTokenizer class to tokenize the strings.", wordTokenizer0.globalInfo());
      
      boolean boolean0 = naiveBayesMultinomialText0.getUseWordFrequencies();
      assertFalse(boolean0);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(3803.12, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals(4027.85446189, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      
      LovinsStemmer lovinsStemmer0 = new LovinsStemmer();
      assertNotNull(lovinsStemmer0);
      
      NaiveBayesMultinomialText naiveBayesMultinomialText1 = new NaiveBayesMultinomialText();
      assertFalse(naiveBayesMultinomialText1.equals((Object)naiveBayesMultinomialText0));
      assertNotNull(naiveBayesMultinomialText1);
      assertFalse(naiveBayesMultinomialText1.getNormalizeDocLength());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText1.tokenizerTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText1.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText1.getPeriodicPruning());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText1.normalizeDocLengthTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText1.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText1.getDebug());
      assertFalse(naiveBayesMultinomialText1.getUseStopList());
      assertEquals(1.0, naiveBayesMultinomialText1.getNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText1.debugTipText());
      assertEquals(2.0, naiveBayesMultinomialText1.getLNorm(), 0.01);
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText1.normTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText1.stopwordsTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText1.globalInfo());
      assertFalse(naiveBayesMultinomialText1.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText1.getLowercaseTokens());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText1.LNormTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText1.minWordFrequencyTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText1.periodicPruningTipText());
      assertEquals(3.0, naiveBayesMultinomialText1.getMinWordFrequency(), 0.01);
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText1.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText1.useStopListTipText());
      
      String[] stringArray0 = new String[6];
      stringArray0[0] = "The stemming algorithm to use on the words.";
      stringArray0[1] = "The stemming algorithm to use on the words.";
      stringArray0[2] = "The stemming algorithm to use on the words.";
      stringArray0[3] = "The stemming algorithm to use on the words.";
      stringArray0[4] = "The stemming algorithm to use on the words.";
      stringArray0[5] = "The stemming algorithm to use on the words.";
      naiveBayesMultinomialText1.setOptions(stringArray0);
      assertFalse(naiveBayesMultinomialText1.equals((Object)naiveBayesMultinomialText0));
      assertEquals(6, stringArray0.length);
      assertNotSame(naiveBayesMultinomialText1, naiveBayesMultinomialText0);
      assertFalse(naiveBayesMultinomialText1.getNormalizeDocLength());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText1.tokenizerTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText1.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText1.getPeriodicPruning());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText1.normalizeDocLengthTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText1.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText1.getDebug());
      assertFalse(naiveBayesMultinomialText1.getUseStopList());
      assertEquals(1.0, naiveBayesMultinomialText1.getNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText1.debugTipText());
      assertEquals(2.0, naiveBayesMultinomialText1.getLNorm(), 0.01);
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText1.normTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText1.stopwordsTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText1.globalInfo());
      assertFalse(naiveBayesMultinomialText1.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText1.getLowercaseTokens());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText1.LNormTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText1.minWordFrequencyTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText1.periodicPruningTipText());
      assertEquals(3.0, naiveBayesMultinomialText1.getMinWordFrequency(), 0.01);
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText1.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText1.useStopListTipText());
      
      naiveBayesMultinomialText1.setStemmer(nullStemmer1);
      assertFalse(naiveBayesMultinomialText0.equals((Object)naiveBayesMultinomialText1));
      assertFalse(naiveBayesMultinomialText1.equals((Object)naiveBayesMultinomialText0));
      assertNotSame(naiveBayesMultinomialText0, naiveBayesMultinomialText1);
      assertNotSame(naiveBayesMultinomialText1, naiveBayesMultinomialText0);
      assertSame(nullStemmer1, nullStemmer0);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(3803.12, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals(4027.85446189, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText1.getNormalizeDocLength());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText1.tokenizerTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText1.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText1.getPeriodicPruning());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText1.normalizeDocLengthTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText1.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText1.getDebug());
      assertFalse(naiveBayesMultinomialText1.getUseStopList());
      assertEquals(1.0, naiveBayesMultinomialText1.getNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText1.debugTipText());
      assertEquals(2.0, naiveBayesMultinomialText1.getLNorm(), 0.01);
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText1.normTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText1.stopwordsTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText1.globalInfo());
      assertFalse(naiveBayesMultinomialText1.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText1.getLowercaseTokens());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText1.LNormTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText1.minWordFrequencyTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText1.periodicPruningTipText());
      assertEquals(3.0, naiveBayesMultinomialText1.getMinWordFrequency(), 0.01);
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText1.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText1.useStopListTipText());
      assertEquals("A dummy stemmer that performs no stemming at all.", nullStemmer1.globalInfo());
      
      TestInstances testInstances0 = TestInstances.forCapabilities(capabilities0);
      assertFalse(naiveBayesMultinomialText0.equals((Object)naiveBayesMultinomialText1));
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertNotNull(testInstances0);
      assertNotSame(naiveBayesMultinomialText0, naiveBayesMultinomialText1);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(3803.12, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals(4027.85446189, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      assertFalse(capabilities0.hasDependencies());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(1, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getSeed());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(5, testInstances0.getNumAttributes());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(4, testInstances0.getNumClasses());
      assertFalse(testInstances0.getNoClass());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(1, testInstances0.getNumString());
      
      Instances instances0 = testInstances0.generate("The stemming algorithm to use on the words.");
      assertFalse(naiveBayesMultinomialText0.equals((Object)naiveBayesMultinomialText1));
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertNotNull(instances0);
      assertNotSame(naiveBayesMultinomialText0, naiveBayesMultinomialText1);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(3803.12, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals(4027.85446189, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      assertFalse(capabilities0.hasDependencies());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(1, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getSeed());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(5, testInstances0.getNumAttributes());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(4, testInstances0.getNumClasses());
      assertFalse(testInstances0.getNoClass());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(1, testInstances0.getNumString());
      assertEquals(4, instances0.numClasses());
      assertEquals(4, instances0.classIndex());
      assertTrue(instances0.checkForStringAttributes());
      assertEquals(5, instances0.numAttributes());
      assertEquals(20, instances0.numInstances());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(20, instances0.size());
      
      naiveBayesMultinomialText1.buildClassifier(instances0);
      assertFalse(naiveBayesMultinomialText0.equals((Object)naiveBayesMultinomialText1));
      assertFalse(naiveBayesMultinomialText1.equals((Object)naiveBayesMultinomialText0));
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertNotSame(naiveBayesMultinomialText0, naiveBayesMultinomialText1);
      assertNotSame(naiveBayesMultinomialText1, naiveBayesMultinomialText0);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(3803.12, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals(4027.85446189, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText1.getNormalizeDocLength());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText1.tokenizerTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText1.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText1.getPeriodicPruning());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText1.normalizeDocLengthTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText1.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText1.getDebug());
      assertFalse(naiveBayesMultinomialText1.getUseStopList());
      assertEquals(1.0, naiveBayesMultinomialText1.getNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText1.debugTipText());
      assertEquals(2.0, naiveBayesMultinomialText1.getLNorm(), 0.01);
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText1.normTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText1.stopwordsTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText1.globalInfo());
      assertFalse(naiveBayesMultinomialText1.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText1.getLowercaseTokens());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText1.LNormTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText1.minWordFrequencyTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText1.periodicPruningTipText());
      assertEquals(3.0, naiveBayesMultinomialText1.getMinWordFrequency(), 0.01);
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText1.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText1.useStopListTipText());
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      assertFalse(capabilities0.hasDependencies());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(1, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getSeed());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(5, testInstances0.getNumAttributes());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(4, testInstances0.getNumClasses());
      assertFalse(testInstances0.getNoClass());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(1, testInstances0.getNumString());
      assertEquals(4, instances0.numClasses());
      assertEquals(4, instances0.classIndex());
      assertTrue(instances0.checkForStringAttributes());
      assertEquals(5, instances0.numAttributes());
      assertEquals(20, instances0.numInstances());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(20, instances0.size());
  }

  /**
  //Test case number: 18
  /*Coverage entropy=2.0982737395252498
  */
  @Test(timeout = 4000)
  public void test18()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      assertNotNull(naiveBayesMultinomialText0);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      
      naiveBayesMultinomialText0.setNormalizeDocLength(false);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      
      Capabilities capabilities0 = naiveBayesMultinomialText0.getCapabilities();
      assertNotNull(capabilities0);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      assertFalse(capabilities0.hasDependencies());
      
      boolean boolean0 = naiveBayesMultinomialText0.getNormalizeDocLength();
      assertFalse(boolean0);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      
      WordTokenizer wordTokenizer0 = (WordTokenizer)naiveBayesMultinomialText0.getTokenizer();
      assertNotNull(wordTokenizer0);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("A simple tokenizer that is using the java.util.StringTokenizer class to tokenize the strings.", wordTokenizer0.globalInfo());
      assertEquals("Set of delimiter characters to use in tokenizing (\\r, \\n and \\t can be used for carriage-return, line-feed and tab)", wordTokenizer0.delimitersTipText());
      assertEquals(" \r\n\t.,;:'\"()?!", wordTokenizer0.getDelimiters());
      
      String string0 = naiveBayesMultinomialText0.periodicPruningTipText();
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", string0);
      assertNotNull(string0);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      
      LovinsStemmer lovinsStemmer0 = new LovinsStemmer();
      assertNotNull(lovinsStemmer0);
      
      naiveBayesMultinomialText0.setStemmer(lovinsStemmer0);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      
      MockFile mockFile0 = new MockFile("w[qIbt", "weka/core/Capabilities.props");
      assertNotNull(mockFile0);
      
      MockFile mockFile1 = (MockFile)MockFile.createTempFile("weka/core/Capabilities.props", "az86ghg", (File) mockFile0);
      assertFalse(mockFile1.equals((Object)mockFile0));
      assertNotNull(mockFile1);
      assertNotSame(mockFile0, mockFile1);
      assertNotSame(mockFile1, mockFile0);
      assertEquals("Capabilities.props", mockFile0.getName());
      assertEquals(0L, mockFile0.getFreeSpace());
      assertEquals(0L, mockFile0.length());
      assertTrue(mockFile0.exists());
      assertFalse(mockFile0.isFile());
      assertEquals("/home/ubuntu/evosuite_readability_gen/projects/107_weka/w[qIbt/weka/core", mockFile0.getParent());
      assertTrue(mockFile0.isAbsolute());
      assertTrue(mockFile0.isDirectory());
      assertEquals(0L, mockFile0.getUsableSpace());
      assertFalse(mockFile0.isHidden());
      assertEquals("/home/ubuntu/evosuite_readability_gen/projects/107_weka/w[qIbt/weka/core/Capabilities.props", mockFile0.toString());
      assertTrue(mockFile0.canWrite());
      assertTrue(mockFile0.canRead());
      assertTrue(mockFile0.canExecute());
      assertEquals(1392409281320L, mockFile0.lastModified());
      assertEquals(0L, mockFile0.getTotalSpace());
      assertTrue(mockFile1.isAbsolute());
      assertEquals(0L, mockFile1.getTotalSpace());
      assertTrue(mockFile1.canExecute());
      assertTrue(mockFile1.exists());
      assertEquals("/home/ubuntu/evosuite_readability_gen/projects/107_weka/w[qIbt/weka/core/Capabilities.props/weka/core", mockFile1.getParent());
      assertEquals(1392409281320L, mockFile1.lastModified());
      assertTrue(mockFile1.canRead());
      assertTrue(mockFile1.isFile());
      assertEquals("Capabilities.props0az86ghg", mockFile1.getName());
      assertEquals(0L, mockFile1.getUsableSpace());
      assertFalse(mockFile1.isHidden());
      assertEquals(0L, mockFile1.length());
      assertTrue(mockFile1.canWrite());
      assertEquals("/home/ubuntu/evosuite_readability_gen/projects/107_weka/w[qIbt/weka/core/Capabilities.props/weka/core/Capabilities.props0az86ghg", mockFile1.toString());
      assertFalse(mockFile1.isDirectory());
      assertEquals(0L, mockFile1.getFreeSpace());
      
      naiveBayesMultinomialText0.setStopwords(mockFile1);
      assertFalse(mockFile0.equals((Object)mockFile1));
      assertFalse(mockFile1.equals((Object)mockFile0));
      assertNotSame(mockFile0, mockFile1);
      assertNotSame(mockFile1, mockFile0);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertTrue(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Capabilities.props", mockFile0.getName());
      assertEquals(0L, mockFile0.getFreeSpace());
      assertEquals(0L, mockFile0.length());
      assertTrue(mockFile0.exists());
      assertFalse(mockFile0.isFile());
      assertEquals("/home/ubuntu/evosuite_readability_gen/projects/107_weka/w[qIbt/weka/core", mockFile0.getParent());
      assertTrue(mockFile0.isAbsolute());
      assertTrue(mockFile0.isDirectory());
      assertEquals(0L, mockFile0.getUsableSpace());
      assertFalse(mockFile0.isHidden());
      assertEquals("/home/ubuntu/evosuite_readability_gen/projects/107_weka/w[qIbt/weka/core/Capabilities.props", mockFile0.toString());
      assertTrue(mockFile0.canWrite());
      assertTrue(mockFile0.canRead());
      assertTrue(mockFile0.canExecute());
      assertEquals(1392409281320L, mockFile0.lastModified());
      assertEquals(0L, mockFile0.getTotalSpace());
      assertTrue(mockFile1.isAbsolute());
      assertEquals(0L, mockFile1.getTotalSpace());
      assertTrue(mockFile1.canExecute());
      assertTrue(mockFile1.exists());
      assertEquals("/home/ubuntu/evosuite_readability_gen/projects/107_weka/w[qIbt/weka/core/Capabilities.props/weka/core", mockFile1.getParent());
      assertEquals(1392409281320L, mockFile1.lastModified());
      assertTrue(mockFile1.canRead());
      assertTrue(mockFile1.isFile());
      assertEquals("Capabilities.props0az86ghg", mockFile1.getName());
      assertEquals(0L, mockFile1.getUsableSpace());
      assertFalse(mockFile1.isHidden());
      assertEquals(0L, mockFile1.length());
      assertTrue(mockFile1.canWrite());
      assertEquals("/home/ubuntu/evosuite_readability_gen/projects/107_weka/w[qIbt/weka/core/Capabilities.props/weka/core/Capabilities.props0az86ghg", mockFile1.toString());
      assertFalse(mockFile1.isDirectory());
      assertEquals(0L, mockFile1.getFreeSpace());
  }

  /**
  //Test case number: 19
  /*Coverage entropy=3.2536462828828747
  */
  @Test(timeout = 4000)
  public void test19()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      assertNotNull(naiveBayesMultinomialText0);
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      
      String string0 = naiveBayesMultinomialText0.toString();
      assertEquals("NaiveBayesMultinomialText: No model built yet.\n", string0);
      assertNotNull(string0);
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      
      double double0 = naiveBayesMultinomialText0.getNorm();
      assertEquals(1.0, double0, 0.01);
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      
      String[] stringArray0 = naiveBayesMultinomialText0.getOptions();
      assertEquals(12, stringArray0.length);
      assertNotNull(stringArray0);
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      
      naiveBayesMultinomialText0.setOptions(stringArray0);
      assertEquals(12, stringArray0.length);
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
  }

  /**
  //Test case number: 20
  /*Coverage entropy=2.833213344056216
  */
  @Test(timeout = 4000)
  public void test20()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      assertNotNull(naiveBayesMultinomialText0);
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      
      SGDText sGDText0 = new SGDText();
      assertEquals(1, SGDText.LOGLOSS);
      assertEquals(0, SGDText.HINGE);
      assertNotNull(sGDText0);
      assertEquals("Whether to convert all tokens to lowercase", sGDText0.lowercaseTokensTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText0.normalizeDocLengthTipText());
      assertFalse(sGDText0.getDebug());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText0.epochsTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText0.stopwordsTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText0.periodicPruningTipText());
      assertEquals("The learning rate.", sGDText0.learningRateTipText());
      assertEquals("The stemming algorithm to use on the words.", sGDText0.stemmerTipText());
      assertEquals(500, sGDText0.getEpochs());
      assertEquals(2.0, sGDText0.getLNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText0.debugTipText());
      assertFalse(sGDText0.getLowercaseTokens());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText0.minWordFrequencyTipText());
      assertEquals(3.0, sGDText0.getMinWordFrequency(), 0.01);
      assertEquals("The random number seed to be used.", sGDText0.seedTipText());
      assertEquals(0, sGDText0.getPeriodicPruning());
      assertFalse(sGDText0.getUseStopList());
      assertEquals(1, sGDText0.getSeed());
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText0.tokenizerTipText());
      assertFalse(sGDText0.getNormalizeDocLength());
      assertEquals(0.01, sGDText0.getLearningRate(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", sGDText0.LNormTipText());
      assertEquals("The norm of the instances after normalization.", sGDText0.normTipText());
      assertEquals("The regularization constant. (default = 0.0001)", sGDText0.lambdaTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText0.useWordFrequenciesTipText());
      assertFalse(sGDText0.getUseWordFrequencies());
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText0.useStopListTipText());
      assertEquals(1.0E-4, sGDText0.getLambda(), 0.01);
      assertFalse(sGDText0.getOutputProbsForSVM());
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText0.lossFunctionTipText());
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText0.outputProbsForSVMTipText());
      assertEquals(1.0, sGDText0.getNorm(), 0.01);
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText0.globalInfo());
      
      NullStemmer nullStemmer0 = (NullStemmer)sGDText0.getStemmer();
      assertEquals(1, SGDText.LOGLOSS);
      assertEquals(0, SGDText.HINGE);
      assertNotNull(nullStemmer0);
      assertEquals("Whether to convert all tokens to lowercase", sGDText0.lowercaseTokensTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText0.normalizeDocLengthTipText());
      assertFalse(sGDText0.getDebug());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText0.epochsTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText0.stopwordsTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText0.periodicPruningTipText());
      assertEquals("The learning rate.", sGDText0.learningRateTipText());
      assertEquals("The stemming algorithm to use on the words.", sGDText0.stemmerTipText());
      assertEquals(500, sGDText0.getEpochs());
      assertEquals(2.0, sGDText0.getLNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText0.debugTipText());
      assertFalse(sGDText0.getLowercaseTokens());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText0.minWordFrequencyTipText());
      assertEquals(3.0, sGDText0.getMinWordFrequency(), 0.01);
      assertEquals("The random number seed to be used.", sGDText0.seedTipText());
      assertEquals(0, sGDText0.getPeriodicPruning());
      assertFalse(sGDText0.getUseStopList());
      assertEquals(1, sGDText0.getSeed());
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText0.tokenizerTipText());
      assertFalse(sGDText0.getNormalizeDocLength());
      assertEquals(0.01, sGDText0.getLearningRate(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", sGDText0.LNormTipText());
      assertEquals("The norm of the instances after normalization.", sGDText0.normTipText());
      assertEquals("The regularization constant. (default = 0.0001)", sGDText0.lambdaTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText0.useWordFrequenciesTipText());
      assertFalse(sGDText0.getUseWordFrequencies());
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText0.useStopListTipText());
      assertEquals(1.0E-4, sGDText0.getLambda(), 0.01);
      assertFalse(sGDText0.getOutputProbsForSVM());
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText0.lossFunctionTipText());
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText0.outputProbsForSVMTipText());
      assertEquals(1.0, sGDText0.getNorm(), 0.01);
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText0.globalInfo());
      assertEquals("A dummy stemmer that performs no stemming at all.", nullStemmer0.globalInfo());
      
      naiveBayesMultinomialText0.m_stemmer = (Stemmer) nullStemmer0;
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("Whether to convert all tokens to lowercase", sGDText0.lowercaseTokensTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText0.normalizeDocLengthTipText());
      assertFalse(sGDText0.getDebug());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText0.epochsTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText0.stopwordsTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText0.periodicPruningTipText());
      assertEquals("The learning rate.", sGDText0.learningRateTipText());
      assertEquals("The stemming algorithm to use on the words.", sGDText0.stemmerTipText());
      assertEquals(500, sGDText0.getEpochs());
      assertEquals(2.0, sGDText0.getLNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText0.debugTipText());
      assertFalse(sGDText0.getLowercaseTokens());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText0.minWordFrequencyTipText());
      assertEquals(3.0, sGDText0.getMinWordFrequency(), 0.01);
      assertEquals("The random number seed to be used.", sGDText0.seedTipText());
      assertEquals(0, sGDText0.getPeriodicPruning());
      assertFalse(sGDText0.getUseStopList());
      assertEquals(1, sGDText0.getSeed());
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText0.tokenizerTipText());
      assertFalse(sGDText0.getNormalizeDocLength());
      assertEquals(0.01, sGDText0.getLearningRate(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", sGDText0.LNormTipText());
      assertEquals("The norm of the instances after normalization.", sGDText0.normTipText());
      assertEquals("The regularization constant. (default = 0.0001)", sGDText0.lambdaTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText0.useWordFrequenciesTipText());
      assertFalse(sGDText0.getUseWordFrequencies());
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText0.useStopListTipText());
      assertEquals(1.0E-4, sGDText0.getLambda(), 0.01);
      assertFalse(sGDText0.getOutputProbsForSVM());
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText0.lossFunctionTipText());
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText0.outputProbsForSVMTipText());
      assertEquals(1.0, sGDText0.getNorm(), 0.01);
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText0.globalInfo());
      assertEquals("A dummy stemmer that performs no stemming at all.", nullStemmer0.globalInfo());
      
      naiveBayesMultinomialText0.setNorm(3803.12);
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals(3803.12, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      
      NullStemmer nullStemmer1 = (NullStemmer)naiveBayesMultinomialText0.m_stemmer;
      assertNotNull(nullStemmer1);
      assertSame(nullStemmer1, nullStemmer0);
      assertEquals("A dummy stemmer that performs no stemming at all.", nullStemmer1.globalInfo());
      
      naiveBayesMultinomialText0.setStemmer(nullStemmer1);
      assertSame(nullStemmer1, nullStemmer0);
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals(3803.12, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("A dummy stemmer that performs no stemming at all.", nullStemmer1.globalInfo());
      
      naiveBayesMultinomialText0.setNorm(3803.12);
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals(3803.12, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      
      Capabilities capabilities0 = naiveBayesMultinomialText0.getCapabilities();
      assertNotNull(capabilities0);
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals(3803.12, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      assertFalse(capabilities0.hasDependencies());
      
      naiveBayesMultinomialText0.setPeriodicPruning(0);
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals(3803.12, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      
      Enumeration<Option> enumeration0 = naiveBayesMultinomialText0.listOptions();
      assertNotNull(enumeration0);
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals(3803.12, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      
      naiveBayesMultinomialText0.setLNorm(4027.85446189);
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(4027.85446189, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals(3803.12, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      
      String string0 = naiveBayesMultinomialText0.useWordFrequenciesTipText();
      assertEquals("Use word frequencies rather than binary bag of words representation", string0);
      assertNotNull(string0);
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(4027.85446189, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals(3803.12, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      
      String[] stringArray0 = new String[7];
      stringArray0[0] = "";
      stringArray0[1] = "  Precision  Recall";
      stringArray0[2] = "Use word frequencies rather than binary bag of words representation";
      stringArray0[3] = "Use word frequencies rather than binary bag of words representation";
      stringArray0[4] = "/FN^?JkuV";
      stringArray0[5] = "LG#b";
      stringArray0[6] = "  Precision  Recall";
      naiveBayesMultinomialText0.setOptions(stringArray0);
      assertEquals(7, stringArray0.length);
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(4027.85446189, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals(3803.12, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      
      String string1 = naiveBayesMultinomialText0.normTipText();
      assertEquals("The norm of the instances after normalization.", string1);
      assertFalse(string1.equals((Object)string0));
      assertNotNull(string1);
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(4027.85446189, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals(3803.12, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
  }

  /**
  //Test case number: 21
  /*Coverage entropy=2.966943486189354
  */
  @Test(timeout = 4000)
  public void test21()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      assertNotNull(naiveBayesMultinomialText0);
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      
      NaiveBayesMultinomialText naiveBayesMultinomialText1 = new NaiveBayesMultinomialText();
      assertFalse(naiveBayesMultinomialText1.equals((Object)naiveBayesMultinomialText0));
      assertNotNull(naiveBayesMultinomialText1);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText1.tokenizerTipText());
      assertFalse(naiveBayesMultinomialText1.getNormalizeDocLength());
      assertEquals(0, naiveBayesMultinomialText1.getPeriodicPruning());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText1.lowercaseTokensTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText1.normalizeDocLengthTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText1.useWordFrequenciesTipText());
      assertFalse(naiveBayesMultinomialText1.getUseStopList());
      assertEquals(1.0, naiveBayesMultinomialText1.getNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText1.debugTipText());
      assertEquals(2.0, naiveBayesMultinomialText1.getLNorm(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText1.stopwordsTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText1.normTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText1.globalInfo());
      assertFalse(naiveBayesMultinomialText1.getUseWordFrequencies());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText1.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText1.getDebug());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText1.stemmerTipText());
      assertFalse(naiveBayesMultinomialText1.getLowercaseTokens());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText1.LNormTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText1.periodicPruningTipText());
      assertEquals(3.0, naiveBayesMultinomialText1.getMinWordFrequency(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText1.useStopListTipText());
      
      LovinsStemmer lovinsStemmer0 = new LovinsStemmer();
      assertNotNull(lovinsStemmer0);
      
      naiveBayesMultinomialText1.setStemmer(lovinsStemmer0);
      assertFalse(naiveBayesMultinomialText1.equals((Object)naiveBayesMultinomialText0));
      assertNotSame(naiveBayesMultinomialText1, naiveBayesMultinomialText0);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText1.tokenizerTipText());
      assertFalse(naiveBayesMultinomialText1.getNormalizeDocLength());
      assertEquals(0, naiveBayesMultinomialText1.getPeriodicPruning());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText1.lowercaseTokensTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText1.normalizeDocLengthTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText1.useWordFrequenciesTipText());
      assertFalse(naiveBayesMultinomialText1.getUseStopList());
      assertEquals(1.0, naiveBayesMultinomialText1.getNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText1.debugTipText());
      assertEquals(2.0, naiveBayesMultinomialText1.getLNorm(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText1.stopwordsTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText1.normTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText1.globalInfo());
      assertFalse(naiveBayesMultinomialText1.getUseWordFrequencies());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText1.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText1.getDebug());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText1.stemmerTipText());
      assertFalse(naiveBayesMultinomialText1.getLowercaseTokens());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText1.LNormTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText1.periodicPruningTipText());
      assertEquals(3.0, naiveBayesMultinomialText1.getMinWordFrequency(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText1.useStopListTipText());
      
      naiveBayesMultinomialText1.setNorm(2142.4921741384946);
      assertFalse(naiveBayesMultinomialText1.equals((Object)naiveBayesMultinomialText0));
      assertNotSame(naiveBayesMultinomialText1, naiveBayesMultinomialText0);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText1.tokenizerTipText());
      assertFalse(naiveBayesMultinomialText1.getNormalizeDocLength());
      assertEquals(0, naiveBayesMultinomialText1.getPeriodicPruning());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText1.lowercaseTokensTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText1.normalizeDocLengthTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText1.useWordFrequenciesTipText());
      assertFalse(naiveBayesMultinomialText1.getUseStopList());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText1.debugTipText());
      assertEquals(2.0, naiveBayesMultinomialText1.getLNorm(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText1.stopwordsTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText1.normTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText1.globalInfo());
      assertEquals(2142.4921741384946, naiveBayesMultinomialText1.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText1.getUseWordFrequencies());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText1.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText1.getDebug());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText1.stemmerTipText());
      assertFalse(naiveBayesMultinomialText1.getLowercaseTokens());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText1.LNormTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText1.periodicPruningTipText());
      assertEquals(3.0, naiveBayesMultinomialText1.getMinWordFrequency(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText1.useStopListTipText());
      
      Capabilities capabilities0 = naiveBayesMultinomialText0.getCapabilities();
      assertFalse(naiveBayesMultinomialText0.equals((Object)naiveBayesMultinomialText1));
      assertNotNull(capabilities0);
      assertNotSame(naiveBayesMultinomialText0, naiveBayesMultinomialText1);
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      assertFalse(capabilities0.hasDependencies());
      
      naiveBayesMultinomialText0.setPeriodicPruning(7);
      assertFalse(naiveBayesMultinomialText0.equals((Object)naiveBayesMultinomialText1));
      assertNotSame(naiveBayesMultinomialText0, naiveBayesMultinomialText1);
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals(7, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      
      Enumeration<Option> enumeration0 = naiveBayesMultinomialText1.listOptions();
      assertFalse(naiveBayesMultinomialText1.equals((Object)naiveBayesMultinomialText0));
      assertNotNull(enumeration0);
      assertNotSame(naiveBayesMultinomialText1, naiveBayesMultinomialText0);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText1.tokenizerTipText());
      assertFalse(naiveBayesMultinomialText1.getNormalizeDocLength());
      assertEquals(0, naiveBayesMultinomialText1.getPeriodicPruning());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText1.lowercaseTokensTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText1.normalizeDocLengthTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText1.useWordFrequenciesTipText());
      assertFalse(naiveBayesMultinomialText1.getUseStopList());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText1.debugTipText());
      assertEquals(2.0, naiveBayesMultinomialText1.getLNorm(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText1.stopwordsTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText1.normTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText1.globalInfo());
      assertEquals(2142.4921741384946, naiveBayesMultinomialText1.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText1.getUseWordFrequencies());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText1.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText1.getDebug());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText1.stemmerTipText());
      assertFalse(naiveBayesMultinomialText1.getLowercaseTokens());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText1.LNormTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText1.periodicPruningTipText());
      assertEquals(3.0, naiveBayesMultinomialText1.getMinWordFrequency(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText1.useStopListTipText());
      
      naiveBayesMultinomialText0.setLNorm(7);
      assertFalse(naiveBayesMultinomialText0.equals((Object)naiveBayesMultinomialText1));
      assertNotSame(naiveBayesMultinomialText0, naiveBayesMultinomialText1);
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals(7, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals(7.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      
      String string0 = naiveBayesMultinomialText0.useWordFrequenciesTipText();
      assertEquals("Use word frequencies rather than binary bag of words representation", string0);
      assertFalse(naiveBayesMultinomialText0.equals((Object)naiveBayesMultinomialText1));
      assertNotNull(string0);
      assertNotSame(naiveBayesMultinomialText0, naiveBayesMultinomialText1);
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals(7, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals(7.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      
      String string1 = naiveBayesMultinomialText0.stemmerTipText();
      assertEquals("The stemming algorithm to use on the words.", string1);
      assertFalse(naiveBayesMultinomialText0.equals((Object)naiveBayesMultinomialText1));
      assertFalse(string1.equals((Object)string0));
      assertNotNull(string1);
      assertNotSame(naiveBayesMultinomialText0, naiveBayesMultinomialText1);
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals(7, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals(7.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      
      NaiveBayesMultinomialText naiveBayesMultinomialText2 = new NaiveBayesMultinomialText();
      assertFalse(naiveBayesMultinomialText2.equals((Object)naiveBayesMultinomialText1));
      assertFalse(naiveBayesMultinomialText2.equals((Object)naiveBayesMultinomialText0));
      assertNotNull(naiveBayesMultinomialText2);
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText2.minWordFrequencyTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText2.lowercaseTokensTipText());
      assertEquals(3.0, naiveBayesMultinomialText2.getMinWordFrequency(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText2.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText2.stemmerTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText2.LNormTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText2.normalizeDocLengthTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText2.normTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText2.globalInfo());
      assertFalse(naiveBayesMultinomialText2.getUseWordFrequencies());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText2.periodicPruningTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText2.tokenizerTipText());
      assertFalse(naiveBayesMultinomialText2.getNormalizeDocLength());
      assertEquals(1.0, naiveBayesMultinomialText2.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText2.getDebug());
      assertFalse(naiveBayesMultinomialText2.getUseStopList());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText2.useWordFrequenciesTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText2.useStopListTipText());
      assertFalse(naiveBayesMultinomialText2.getLowercaseTokens());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText2.stopwordsTipText());
      assertEquals(2.0, naiveBayesMultinomialText2.getLNorm(), 0.01);
      assertEquals(0, naiveBayesMultinomialText2.getPeriodicPruning());
      
      double double0 = naiveBayesMultinomialText2.getNorm();
      assertEquals(1.0, double0, 0.01);
      assertFalse(naiveBayesMultinomialText2.equals((Object)naiveBayesMultinomialText1));
      assertFalse(naiveBayesMultinomialText2.equals((Object)naiveBayesMultinomialText0));
      assertNotSame(naiveBayesMultinomialText2, naiveBayesMultinomialText1);
      assertNotSame(naiveBayesMultinomialText2, naiveBayesMultinomialText0);
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText2.minWordFrequencyTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText2.lowercaseTokensTipText());
      assertEquals(3.0, naiveBayesMultinomialText2.getMinWordFrequency(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText2.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText2.stemmerTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText2.LNormTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText2.normalizeDocLengthTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText2.normTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText2.globalInfo());
      assertFalse(naiveBayesMultinomialText2.getUseWordFrequencies());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText2.periodicPruningTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText2.tokenizerTipText());
      assertFalse(naiveBayesMultinomialText2.getNormalizeDocLength());
      assertEquals(1.0, naiveBayesMultinomialText2.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText2.getDebug());
      assertFalse(naiveBayesMultinomialText2.getUseStopList());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText2.useWordFrequenciesTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText2.useStopListTipText());
      assertFalse(naiveBayesMultinomialText2.getLowercaseTokens());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText2.stopwordsTipText());
      assertEquals(2.0, naiveBayesMultinomialText2.getLNorm(), 0.01);
      assertEquals(0, naiveBayesMultinomialText2.getPeriodicPruning());
      
      NaiveBayesMultinomialText naiveBayesMultinomialText3 = new NaiveBayesMultinomialText();
      assertFalse(naiveBayesMultinomialText3.equals((Object)naiveBayesMultinomialText1));
      assertFalse(naiveBayesMultinomialText3.equals((Object)naiveBayesMultinomialText0));
      assertFalse(naiveBayesMultinomialText3.equals((Object)naiveBayesMultinomialText2));
      assertNotNull(naiveBayesMultinomialText3);
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText3.normTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText3.stopwordsTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText3.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText3.stemmerTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText3.periodicPruningTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText3.lowercaseTokensTipText());
      assertEquals(3.0, naiveBayesMultinomialText3.getMinWordFrequency(), 0.01);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText3.normalizeDocLengthTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText3.globalInfo());
      assertFalse(naiveBayesMultinomialText3.getUseWordFrequencies());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText3.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText3.getDebug());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText3.tokenizerTipText());
      assertFalse(naiveBayesMultinomialText3.getNormalizeDocLength());
      assertEquals(2.0, naiveBayesMultinomialText3.getLNorm(), 0.01);
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText3.useWordFrequenciesTipText());
      assertEquals(1.0, naiveBayesMultinomialText3.getNorm(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText3.useStopListTipText());
      assertFalse(naiveBayesMultinomialText3.getLowercaseTokens());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText3.LNormTipText());
      assertFalse(naiveBayesMultinomialText3.getUseStopList());
      assertEquals(0, naiveBayesMultinomialText3.getPeriodicPruning());
      
      WordTokenizer wordTokenizer0 = (WordTokenizer)naiveBayesMultinomialText3.getTokenizer();
      assertFalse(naiveBayesMultinomialText3.equals((Object)naiveBayesMultinomialText1));
      assertFalse(naiveBayesMultinomialText3.equals((Object)naiveBayesMultinomialText0));
      assertFalse(naiveBayesMultinomialText3.equals((Object)naiveBayesMultinomialText2));
      assertNotNull(wordTokenizer0);
      assertNotSame(naiveBayesMultinomialText3, naiveBayesMultinomialText1);
      assertNotSame(naiveBayesMultinomialText3, naiveBayesMultinomialText0);
      assertNotSame(naiveBayesMultinomialText3, naiveBayesMultinomialText2);
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText3.normTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText3.stopwordsTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText3.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText3.stemmerTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText3.periodicPruningTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText3.lowercaseTokensTipText());
      assertEquals(3.0, naiveBayesMultinomialText3.getMinWordFrequency(), 0.01);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText3.normalizeDocLengthTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText3.globalInfo());
      assertFalse(naiveBayesMultinomialText3.getUseWordFrequencies());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText3.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText3.getDebug());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText3.tokenizerTipText());
      assertFalse(naiveBayesMultinomialText3.getNormalizeDocLength());
      assertEquals(2.0, naiveBayesMultinomialText3.getLNorm(), 0.01);
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText3.useWordFrequenciesTipText());
      assertEquals(1.0, naiveBayesMultinomialText3.getNorm(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText3.useStopListTipText());
      assertFalse(naiveBayesMultinomialText3.getLowercaseTokens());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText3.LNormTipText());
      assertFalse(naiveBayesMultinomialText3.getUseStopList());
      assertEquals(0, naiveBayesMultinomialText3.getPeriodicPruning());
      assertEquals(" \r\n\t.,;:'\"()?!", wordTokenizer0.getDelimiters());
      assertEquals("Set of delimiter characters to use in tokenizing (\\r, \\n and \\t can be used for carriage-return, line-feed and tab)", wordTokenizer0.delimitersTipText());
      assertEquals("A simple tokenizer that is using the java.util.StringTokenizer class to tokenize the strings.", wordTokenizer0.globalInfo());
      
      boolean boolean0 = naiveBayesMultinomialText3.getUseWordFrequencies();
      assertFalse(boolean0);
      assertFalse(naiveBayesMultinomialText3.equals((Object)naiveBayesMultinomialText1));
      assertFalse(naiveBayesMultinomialText3.equals((Object)naiveBayesMultinomialText0));
      assertFalse(naiveBayesMultinomialText3.equals((Object)naiveBayesMultinomialText2));
      assertNotSame(naiveBayesMultinomialText3, naiveBayesMultinomialText1);
      assertNotSame(naiveBayesMultinomialText3, naiveBayesMultinomialText0);
      assertNotSame(naiveBayesMultinomialText3, naiveBayesMultinomialText2);
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText3.normTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText3.stopwordsTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText3.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText3.stemmerTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText3.periodicPruningTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText3.lowercaseTokensTipText());
      assertEquals(3.0, naiveBayesMultinomialText3.getMinWordFrequency(), 0.01);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText3.normalizeDocLengthTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText3.globalInfo());
      assertFalse(naiveBayesMultinomialText3.getUseWordFrequencies());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText3.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText3.getDebug());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText3.tokenizerTipText());
      assertFalse(naiveBayesMultinomialText3.getNormalizeDocLength());
      assertEquals(2.0, naiveBayesMultinomialText3.getLNorm(), 0.01);
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText3.useWordFrequenciesTipText());
      assertEquals(1.0, naiveBayesMultinomialText3.getNorm(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText3.useStopListTipText());
      assertFalse(naiveBayesMultinomialText3.getLowercaseTokens());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText3.LNormTipText());
      assertFalse(naiveBayesMultinomialText3.getUseStopList());
      assertEquals(0, naiveBayesMultinomialText3.getPeriodicPruning());
      
      LovinsStemmer lovinsStemmer1 = new LovinsStemmer();
      assertFalse(lovinsStemmer1.equals((Object)lovinsStemmer0));
      assertNotNull(lovinsStemmer1);
      
      NaiveBayesMultinomialText naiveBayesMultinomialText4 = new NaiveBayesMultinomialText();
      assertFalse(naiveBayesMultinomialText4.equals((Object)naiveBayesMultinomialText0));
      assertFalse(naiveBayesMultinomialText4.equals((Object)naiveBayesMultinomialText1));
      assertFalse(naiveBayesMultinomialText4.equals((Object)naiveBayesMultinomialText2));
      assertFalse(naiveBayesMultinomialText4.equals((Object)naiveBayesMultinomialText3));
      assertNotNull(naiveBayesMultinomialText4);
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText4.lowercaseTokensTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText4.minWordFrequencyTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText4.tokenizerTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText4.globalInfo());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText4.normalizeDocLengthTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText4.normTipText());
      assertFalse(naiveBayesMultinomialText4.getUseWordFrequencies());
      assertEquals(2.0, naiveBayesMultinomialText4.getLNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText4.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText4.stopwordsTipText());
      assertFalse(naiveBayesMultinomialText4.getUseStopList());
      assertFalse(naiveBayesMultinomialText4.getDebug());
      assertEquals(1.0, naiveBayesMultinomialText4.getNorm(), 0.01);
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText4.periodicPruningTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText4.useWordFrequenciesTipText());
      assertFalse(naiveBayesMultinomialText4.getNormalizeDocLength());
      assertEquals(0, naiveBayesMultinomialText4.getPeriodicPruning());
      assertEquals(3.0, naiveBayesMultinomialText4.getMinWordFrequency(), 0.01);
      assertFalse(naiveBayesMultinomialText4.getLowercaseTokens());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText4.LNormTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText4.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText4.useStopListTipText());
      
      String[] stringArray0 = new String[9];
      stringArray0[0] = "Qi,JucQ{e=pvJL7Z";
      stringArray0[1] = "Use word frequencies rather than binary bag of words representation";
      stringArray0[2] = "The stemming algorithm to use on the words.";
      stringArray0[3] = "The stemming algorithm to use on the words.";
      stringArray0[4] = "Gj/NN/t+WeqHTF]w]";
      stringArray0[5] = "\tMinimum word frequency. Words with less than this frequence are ignored.\n\tIf periodic pruning is turned on then this is also used to determine which\n\twords to remove from the dictionary (default = 3).";
      stringArray0[6] = "The stemming algorithm to use on the words.";
      stringArray0[7] = "Use word frequencies rather than binary bag of words representation";
      stringArray0[8] = "Use word frequencies rather than binary bag of words representation";
      naiveBayesMultinomialText1.setOptions(stringArray0);
      assertFalse(naiveBayesMultinomialText1.equals((Object)naiveBayesMultinomialText2));
      assertFalse(naiveBayesMultinomialText1.equals((Object)naiveBayesMultinomialText3));
      assertFalse(naiveBayesMultinomialText1.equals((Object)naiveBayesMultinomialText4));
      assertFalse(naiveBayesMultinomialText1.equals((Object)naiveBayesMultinomialText0));
      assertEquals(9, stringArray0.length);
      assertNotSame(naiveBayesMultinomialText1, naiveBayesMultinomialText2);
      assertNotSame(naiveBayesMultinomialText1, naiveBayesMultinomialText3);
      assertNotSame(naiveBayesMultinomialText1, naiveBayesMultinomialText4);
      assertNotSame(naiveBayesMultinomialText1, naiveBayesMultinomialText0);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText1.tokenizerTipText());
      assertFalse(naiveBayesMultinomialText1.getNormalizeDocLength());
      assertEquals(0, naiveBayesMultinomialText1.getPeriodicPruning());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText1.lowercaseTokensTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText1.normalizeDocLengthTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText1.useWordFrequenciesTipText());
      assertFalse(naiveBayesMultinomialText1.getUseStopList());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText1.debugTipText());
      assertEquals(2.0, naiveBayesMultinomialText1.getLNorm(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText1.stopwordsTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText1.normTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText1.globalInfo());
      assertEquals(2142.4921741384946, naiveBayesMultinomialText1.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText1.getUseWordFrequencies());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText1.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText1.getDebug());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText1.stemmerTipText());
      assertFalse(naiveBayesMultinomialText1.getLowercaseTokens());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText1.LNormTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText1.periodicPruningTipText());
      assertEquals(3.0, naiveBayesMultinomialText1.getMinWordFrequency(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText1.useStopListTipText());
      
      naiveBayesMultinomialText3.setStemmer(lovinsStemmer1);
      assertFalse(lovinsStemmer1.equals((Object)lovinsStemmer0));
      assertFalse(naiveBayesMultinomialText3.equals((Object)naiveBayesMultinomialText4));
      assertFalse(naiveBayesMultinomialText3.equals((Object)naiveBayesMultinomialText1));
      assertFalse(naiveBayesMultinomialText3.equals((Object)naiveBayesMultinomialText0));
      assertFalse(naiveBayesMultinomialText3.equals((Object)naiveBayesMultinomialText2));
      assertNotSame(lovinsStemmer1, lovinsStemmer0);
      assertNotSame(naiveBayesMultinomialText3, naiveBayesMultinomialText4);
      assertNotSame(naiveBayesMultinomialText3, naiveBayesMultinomialText1);
      assertNotSame(naiveBayesMultinomialText3, naiveBayesMultinomialText0);
      assertNotSame(naiveBayesMultinomialText3, naiveBayesMultinomialText2);
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText3.normTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText3.stopwordsTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText3.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText3.stemmerTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText3.periodicPruningTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText3.lowercaseTokensTipText());
      assertEquals(3.0, naiveBayesMultinomialText3.getMinWordFrequency(), 0.01);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText3.normalizeDocLengthTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText3.globalInfo());
      assertFalse(naiveBayesMultinomialText3.getUseWordFrequencies());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText3.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText3.getDebug());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText3.tokenizerTipText());
      assertFalse(naiveBayesMultinomialText3.getNormalizeDocLength());
      assertEquals(2.0, naiveBayesMultinomialText3.getLNorm(), 0.01);
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText3.useWordFrequenciesTipText());
      assertEquals(1.0, naiveBayesMultinomialText3.getNorm(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText3.useStopListTipText());
      assertFalse(naiveBayesMultinomialText3.getLowercaseTokens());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText3.LNormTipText());
      assertFalse(naiveBayesMultinomialText3.getUseStopList());
      assertEquals(0, naiveBayesMultinomialText3.getPeriodicPruning());
      
      BinarySparseInstance binarySparseInstance0 = new BinarySparseInstance(0);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertNotNull(binarySparseInstance0);
      assertEquals(1.0, binarySparseInstance0.weight(), 0.01);
      assertEquals(0, binarySparseInstance0.numValues());
      assertEquals(0, binarySparseInstance0.numAttributes());
      
      try { 
        naiveBayesMultinomialText3.distributionForInstance(binarySparseInstance0);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("weka.classifiers.bayes.NaiveBayesMultinomialText", e);
      }
  }

  /**
  //Test case number: 22
  /*Coverage entropy=2.551155577647461
  */
  @Test(timeout = 4000)
  public void test22()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      assertNotNull(naiveBayesMultinomialText0);
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      
      SGDText sGDText0 = new SGDText();
      assertEquals(0, SGDText.HINGE);
      assertEquals(1, SGDText.LOGLOSS);
      assertNotNull(sGDText0);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText0.normalizeDocLengthTipText());
      assertEquals("Whether to convert all tokens to lowercase", sGDText0.lowercaseTokensTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText0.stopwordsTipText());
      assertEquals(2.0, sGDText0.getLNorm(), 0.01);
      assertFalse(sGDText0.getDebug());
      assertEquals("The stemming algorithm to use on the words.", sGDText0.stemmerTipText());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText0.epochsTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText0.periodicPruningTipText());
      assertEquals(1.0E-4, sGDText0.getLambda(), 0.01);
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText0.outputProbsForSVMTipText());
      assertEquals(0.01, sGDText0.getLearningRate(), 0.01);
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText0.useWordFrequenciesTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText0.useStopListTipText());
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText0.lossFunctionTipText());
      assertEquals("The norm of the instances after normalization.", sGDText0.normTipText());
      assertEquals(1.0, sGDText0.getNorm(), 0.01);
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText0.globalInfo());
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText0.tokenizerTipText());
      assertEquals("The LNorm to use for document length normalization.", sGDText0.LNormTipText());
      assertEquals(1, sGDText0.getSeed());
      assertFalse(sGDText0.getOutputProbsForSVM());
      assertFalse(sGDText0.getNormalizeDocLength());
      assertFalse(sGDText0.getUseWordFrequencies());
      assertEquals("The regularization constant. (default = 0.0001)", sGDText0.lambdaTipText());
      assertFalse(sGDText0.getUseStopList());
      assertEquals("The learning rate.", sGDText0.learningRateTipText());
      assertEquals("The random number seed to be used.", sGDText0.seedTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText0.minWordFrequencyTipText());
      assertEquals(3.0, sGDText0.getMinWordFrequency(), 0.01);
      assertEquals(0, sGDText0.getPeriodicPruning());
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText0.debugTipText());
      assertEquals(500, sGDText0.getEpochs());
      assertFalse(sGDText0.getLowercaseTokens());
      
      NullStemmer nullStemmer0 = (NullStemmer)sGDText0.getStemmer();
      assertEquals(0, SGDText.HINGE);
      assertEquals(1, SGDText.LOGLOSS);
      assertNotNull(nullStemmer0);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText0.normalizeDocLengthTipText());
      assertEquals("Whether to convert all tokens to lowercase", sGDText0.lowercaseTokensTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText0.stopwordsTipText());
      assertEquals(2.0, sGDText0.getLNorm(), 0.01);
      assertFalse(sGDText0.getDebug());
      assertEquals("The stemming algorithm to use on the words.", sGDText0.stemmerTipText());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText0.epochsTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText0.periodicPruningTipText());
      assertEquals(1.0E-4, sGDText0.getLambda(), 0.01);
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText0.outputProbsForSVMTipText());
      assertEquals(0.01, sGDText0.getLearningRate(), 0.01);
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText0.useWordFrequenciesTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText0.useStopListTipText());
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText0.lossFunctionTipText());
      assertEquals("The norm of the instances after normalization.", sGDText0.normTipText());
      assertEquals(1.0, sGDText0.getNorm(), 0.01);
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText0.globalInfo());
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText0.tokenizerTipText());
      assertEquals("The LNorm to use for document length normalization.", sGDText0.LNormTipText());
      assertEquals(1, sGDText0.getSeed());
      assertFalse(sGDText0.getOutputProbsForSVM());
      assertFalse(sGDText0.getNormalizeDocLength());
      assertFalse(sGDText0.getUseWordFrequencies());
      assertEquals("The regularization constant. (default = 0.0001)", sGDText0.lambdaTipText());
      assertFalse(sGDText0.getUseStopList());
      assertEquals("The learning rate.", sGDText0.learningRateTipText());
      assertEquals("The random number seed to be used.", sGDText0.seedTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText0.minWordFrequencyTipText());
      assertEquals(3.0, sGDText0.getMinWordFrequency(), 0.01);
      assertEquals(0, sGDText0.getPeriodicPruning());
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText0.debugTipText());
      assertEquals(500, sGDText0.getEpochs());
      assertFalse(sGDText0.getLowercaseTokens());
      assertEquals("A dummy stemmer that performs no stemming at all.", nullStemmer0.globalInfo());
      
      naiveBayesMultinomialText0.m_stemmer = (Stemmer) nullStemmer0;
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText0.normalizeDocLengthTipText());
      assertEquals("Whether to convert all tokens to lowercase", sGDText0.lowercaseTokensTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText0.stopwordsTipText());
      assertEquals(2.0, sGDText0.getLNorm(), 0.01);
      assertFalse(sGDText0.getDebug());
      assertEquals("The stemming algorithm to use on the words.", sGDText0.stemmerTipText());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText0.epochsTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText0.periodicPruningTipText());
      assertEquals(1.0E-4, sGDText0.getLambda(), 0.01);
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText0.outputProbsForSVMTipText());
      assertEquals(0.01, sGDText0.getLearningRate(), 0.01);
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText0.useWordFrequenciesTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText0.useStopListTipText());
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText0.lossFunctionTipText());
      assertEquals("The norm of the instances after normalization.", sGDText0.normTipText());
      assertEquals(1.0, sGDText0.getNorm(), 0.01);
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText0.globalInfo());
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText0.tokenizerTipText());
      assertEquals("The LNorm to use for document length normalization.", sGDText0.LNormTipText());
      assertEquals(1, sGDText0.getSeed());
      assertFalse(sGDText0.getOutputProbsForSVM());
      assertFalse(sGDText0.getNormalizeDocLength());
      assertFalse(sGDText0.getUseWordFrequencies());
      assertEquals("The regularization constant. (default = 0.0001)", sGDText0.lambdaTipText());
      assertFalse(sGDText0.getUseStopList());
      assertEquals("The learning rate.", sGDText0.learningRateTipText());
      assertEquals("The random number seed to be used.", sGDText0.seedTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText0.minWordFrequencyTipText());
      assertEquals(3.0, sGDText0.getMinWordFrequency(), 0.01);
      assertEquals(0, sGDText0.getPeriodicPruning());
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText0.debugTipText());
      assertEquals(500, sGDText0.getEpochs());
      assertFalse(sGDText0.getLowercaseTokens());
      assertEquals("A dummy stemmer that performs no stemming at all.", nullStemmer0.globalInfo());
      
      naiveBayesMultinomialText0.setNorm(3803.12);
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(3803.12, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      
      NullStemmer nullStemmer1 = (NullStemmer)naiveBayesMultinomialText0.m_stemmer;
      assertNotNull(nullStemmer1);
      assertSame(nullStemmer1, nullStemmer0);
      assertEquals("A dummy stemmer that performs no stemming at all.", nullStemmer1.globalInfo());
      
      String string0 = nullStemmer1.stem(">");
      assertEquals(">", string0);
      assertNotNull(string0);
      assertSame(nullStemmer1, nullStemmer0);
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(3803.12, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("A dummy stemmer that performs no stemming at all.", nullStemmer1.globalInfo());
      
      naiveBayesMultinomialText0.setStemmer(nullStemmer1);
      assertSame(nullStemmer1, nullStemmer0);
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(3803.12, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("A dummy stemmer that performs no stemming at all.", nullStemmer1.globalInfo());
      
      naiveBayesMultinomialText0.setNorm(3803.12);
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(3803.12, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      
      Capabilities capabilities0 = naiveBayesMultinomialText0.getCapabilities();
      assertNotNull(capabilities0);
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(3803.12, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      assertFalse(capabilities0.hasDependencies());
      
      naiveBayesMultinomialText0.setPeriodicPruning(109);
      assertEquals(109, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(3803.12, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      
      Enumeration<Option> enumeration0 = naiveBayesMultinomialText0.listOptions();
      assertNotNull(enumeration0);
      assertEquals(109, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(3803.12, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      
      naiveBayesMultinomialText0.setLNorm(880.302856296);
      assertEquals(109, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(3803.12, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(880.302856296, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      
      String string1 = naiveBayesMultinomialText0.useWordFrequenciesTipText();
      assertEquals("Use word frequencies rather than binary bag of words representation", string1);
      assertFalse(string1.equals((Object)string0));
      assertNotNull(string1);
      assertEquals(109, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(3803.12, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(880.302856296, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      
      String string2 = naiveBayesMultinomialText0.stemmerTipText();
      assertEquals("The stemming algorithm to use on the words.", string2);
      assertFalse(string2.equals((Object)string1));
      assertFalse(string2.equals((Object)string0));
      assertNotNull(string2);
      assertEquals(109, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(3803.12, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(880.302856296, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      
      double double0 = naiveBayesMultinomialText0.getNorm();
      assertEquals(3803.12, double0, 0.01);
      assertEquals(109, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(3803.12, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(880.302856296, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      
      WordTokenizer wordTokenizer0 = (WordTokenizer)naiveBayesMultinomialText0.getTokenizer();
      assertNotNull(wordTokenizer0);
      assertEquals("Set of delimiter characters to use in tokenizing (\\r, \\n and \\t can be used for carriage-return, line-feed and tab)", wordTokenizer0.delimitersTipText());
      assertEquals(" \r\n\t.,;:'\"()?!", wordTokenizer0.getDelimiters());
      assertEquals("A simple tokenizer that is using the java.util.StringTokenizer class to tokenize the strings.", wordTokenizer0.globalInfo());
      assertEquals(109, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(3803.12, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(880.302856296, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      
      boolean boolean0 = naiveBayesMultinomialText0.getUseWordFrequencies();
      assertFalse(boolean0);
      assertEquals(109, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(3803.12, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(880.302856296, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      
      LovinsStemmer lovinsStemmer0 = new LovinsStemmer();
      assertNotNull(lovinsStemmer0);
      
      String[] stringArray0 = new String[5];
      stringArray0[0] = "The stemming algorithm to use on the words.";
      stringArray0[1] = "";
      stringArray0[2] = "Vnt@vR";
      stringArray0[3] = "Use word frequencies rather than binary bag of words representation";
      stringArray0[4] = "The stemming algorithm to use on the words.";
      naiveBayesMultinomialText0.setOptions(stringArray0);
      assertEquals(5, stringArray0.length);
      assertEquals(109, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(3803.12, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(880.302856296, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      
      naiveBayesMultinomialText0.setStemmer(lovinsStemmer0);
      assertEquals(109, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(3803.12, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(880.302856296, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      
      TestInstances testInstances0 = TestInstances.forCapabilities(capabilities0);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertNotNull(testInstances0);
      assertEquals(109, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(3803.12, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(880.302856296, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      assertFalse(capabilities0.hasDependencies());
      assertEquals(5, testInstances0.getNumAttributes());
      assertEquals(1, testInstances0.getNumString());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(1, testInstances0.getNumRelationalString());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(1, testInstances0.getNumDate());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(20, testInstances0.getNumInstances());
      assertFalse(testInstances0.getNoClass());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(1, testInstances0.getClassType());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(4, testInstances0.getNumClasses());
      
      Instances instances0 = testInstances0.generate("The stemming algorithm to use on the words.");
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertNotNull(instances0);
      assertEquals(109, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(3803.12, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(880.302856296, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      assertFalse(capabilities0.hasDependencies());
      assertEquals(5, testInstances0.getNumAttributes());
      assertEquals(1, testInstances0.getNumString());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(1, testInstances0.getNumRelationalString());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(1, testInstances0.getNumDate());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(20, testInstances0.getNumInstances());
      assertFalse(testInstances0.getNoClass());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(1, testInstances0.getClassType());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(4, testInstances0.getNumClasses());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertTrue(instances0.checkForStringAttributes());
      assertEquals(4, instances0.numClasses());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(4, instances0.classIndex());
      assertEquals(20, instances0.size());
      assertEquals(5, instances0.numAttributes());
      assertEquals(20, instances0.numInstances());
      
      naiveBayesMultinomialText0.buildClassifier(instances0);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals(109, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(3803.12, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(880.302856296, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      assertFalse(capabilities0.hasDependencies());
      assertEquals(5, testInstances0.getNumAttributes());
      assertEquals(1, testInstances0.getNumString());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(1, testInstances0.getNumRelationalString());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(1, testInstances0.getNumDate());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(20, testInstances0.getNumInstances());
      assertFalse(testInstances0.getNoClass());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(1, testInstances0.getClassType());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(4, testInstances0.getNumClasses());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertTrue(instances0.checkForStringAttributes());
      assertEquals(4, instances0.numClasses());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(4, instances0.classIndex());
      assertEquals(20, instances0.size());
      assertEquals(5, instances0.numAttributes());
      assertEquals(20, instances0.numInstances());
      
      String string3 = naiveBayesMultinomialText0.toString();
      assertEquals("The independent probability of a class\n--------------------------------------\nclass1\t9.0\nclass2\t4.0\nclass3\t7.0\nclass4\t4.0\n\nThe probability of a word given the class\n-----------------------------------------\n\tclass1\tclass2\tclass3\tclass4\t\nover\t2.718281828459045\t2.718281828459045\t7.38905609893065\t7.38905609893065\t\nquick\t7.38905609893065\t2.718281828459045\t7.38905609893065\t7.38905609893065\t\nlaz\t7.38905609893065\t2.718281828459045\t7.38905609893065\t2.718281828459045\t\nth\t20.085536923187668\t7.38905609893065\t7.38905609893065\t2.718281828459045\t\nbrown\t7.38905609893065\t7.38905609893065\t2.718281828459045\t2.718281828459045\t\ndog\t7.38905609893065\t2.718281828459045\t7.38905609893065\t2.718281828459045\t\nfox\t2.718281828459045\t2.718281828459045\t7.38905609893065\t7.38905609893065\t\njump\t20.085536923187668\t7.38905609893065\t2.718281828459045\t2.718281828459045\t\n", string3);
      assertFalse(string3.equals((Object)string2));
      assertFalse(string3.equals((Object)string1));
      assertFalse(string3.equals((Object)string0));
      assertNotNull(string3);
      assertEquals(109, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(3803.12, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(880.302856296, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText0.getDebug());
  }

  /**
  //Test case number: 23
  /*Coverage entropy=3.129681271475779
  */
  @Test(timeout = 4000)
  public void test23()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      assertNotNull(naiveBayesMultinomialText0);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      
      PrecomputedKernelMatrixKernel precomputedKernelMatrixKernel0 = new PrecomputedKernelMatrixKernel();
      assertNotNull(precomputedKernelMatrixKernel0);
      assertFalse(precomputedKernelMatrixKernel0.getDebug());
      assertEquals("The file holding the kernel matrix.", precomputedKernelMatrixKernel0.kernelMatrixFileTipText());
      assertEquals("Turns time-consuming checks off - use with caution.", precomputedKernelMatrixKernel0.checksTurnedOffTipText());
      assertEquals(0, precomputedKernelMatrixKernel0.numCacheHits());
      assertEquals("Turns on the output of debugging information.", precomputedKernelMatrixKernel0.debugTipText());
      assertEquals(0, precomputedKernelMatrixKernel0.numEvals());
      assertFalse(precomputedKernelMatrixKernel0.getChecksTurnedOff());
      assertEquals("This kernel is based on a static kernel matrix that is read from a file. Instances must have a single nominal attribute (excluding the class). This attribute must be the first attribute in the file and its values are used to reference rows/columns in the kernel matrix. The second attribute must be the class attribute.", precomputedKernelMatrixKernel0.globalInfo());
      
      File file0 = precomputedKernelMatrixKernel0.getKernelMatrixFile();
      assertNotNull(file0);
      assertFalse(precomputedKernelMatrixKernel0.getDebug());
      assertEquals("The file holding the kernel matrix.", precomputedKernelMatrixKernel0.kernelMatrixFileTipText());
      assertEquals("Turns time-consuming checks off - use with caution.", precomputedKernelMatrixKernel0.checksTurnedOffTipText());
      assertEquals(0, precomputedKernelMatrixKernel0.numCacheHits());
      assertEquals("Turns on the output of debugging information.", precomputedKernelMatrixKernel0.debugTipText());
      assertEquals(0, precomputedKernelMatrixKernel0.numEvals());
      assertFalse(precomputedKernelMatrixKernel0.getChecksTurnedOff());
      assertEquals("This kernel is based on a static kernel matrix that is read from a file. Instances must have a single nominal attribute (excluding the class). This attribute must be the first attribute in the file and its values are used to reference rows/columns in the kernel matrix. The second attribute must be the class attribute.", precomputedKernelMatrixKernel0.globalInfo());
      assertFalse(file0.canWrite());
      assertFalse(file0.canRead());
      assertFalse(file0.canExecute());
      assertEquals(0L, file0.lastModified());
      assertTrue(file0.isFile());
      assertEquals(0L, file0.getTotalSpace());
      assertEquals("kernelMatrix.matrix", file0.toString());
      assertFalse(file0.isAbsolute());
      assertEquals(0L, file0.length());
      assertFalse(file0.exists());
      assertFalse(file0.isDirectory());
      assertEquals(0L, file0.getFreeSpace());
      assertEquals(0L, file0.getUsableSpace());
      assertFalse(file0.isHidden());
      assertNull(file0.getParent());
      assertEquals("kernelMatrix.matrix", file0.getName());
      
      naiveBayesMultinomialText0.setStopwords(file0);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertFalse(precomputedKernelMatrixKernel0.getDebug());
      assertEquals("The file holding the kernel matrix.", precomputedKernelMatrixKernel0.kernelMatrixFileTipText());
      assertEquals("Turns time-consuming checks off - use with caution.", precomputedKernelMatrixKernel0.checksTurnedOffTipText());
      assertEquals(0, precomputedKernelMatrixKernel0.numCacheHits());
      assertEquals("Turns on the output of debugging information.", precomputedKernelMatrixKernel0.debugTipText());
      assertEquals(0, precomputedKernelMatrixKernel0.numEvals());
      assertFalse(precomputedKernelMatrixKernel0.getChecksTurnedOff());
      assertEquals("This kernel is based on a static kernel matrix that is read from a file. Instances must have a single nominal attribute (excluding the class). This attribute must be the first attribute in the file and its values are used to reference rows/columns in the kernel matrix. The second attribute must be the class attribute.", precomputedKernelMatrixKernel0.globalInfo());
      assertFalse(file0.canWrite());
      assertFalse(file0.canRead());
      assertFalse(file0.canExecute());
      assertEquals(0L, file0.lastModified());
      assertTrue(file0.isFile());
      assertEquals(0L, file0.getTotalSpace());
      assertEquals("kernelMatrix.matrix", file0.toString());
      assertFalse(file0.isAbsolute());
      assertEquals(0L, file0.length());
      assertFalse(file0.exists());
      assertFalse(file0.isDirectory());
      assertEquals(0L, file0.getFreeSpace());
      assertEquals(0L, file0.getUsableSpace());
      assertFalse(file0.isHidden());
      assertNull(file0.getParent());
      assertEquals("kernelMatrix.matrix", file0.getName());
      
      String[] stringArray0 = naiveBayesMultinomialText0.getOptions();
      assertEquals(14, stringArray0.length);
      assertNotNull(stringArray0);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      
      naiveBayesMultinomialText0.setOptions(stringArray0);
      assertEquals(14, stringArray0.length);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
  }

  /**
  //Test case number: 24
  /*Coverage entropy=1.3321790402101223
  */
  @Test(timeout = 4000)
  public void test24()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      assertNotNull(naiveBayesMultinomialText0);
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      
      boolean boolean0 = true;
      naiveBayesMultinomialText0.m_useStopList = true;
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertTrue(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      
      double[] doubleArray0 = new double[6];
      doubleArray0[1] = 404.47827;
      int[] intArray0 = new int[1];
      intArray0[0] = 0;
      BinarySparseInstance binarySparseInstance0 = new BinarySparseInstance(0, intArray0, (-894));
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertEquals(1, intArray0.length);
      assertNotNull(binarySparseInstance0);
      assertArrayEquals(new int[] {0}, intArray0);
      assertEquals(0.0, binarySparseInstance0.weight(), 0.01);
      assertEquals((-894), binarySparseInstance0.numAttributes());
      assertEquals(1, binarySparseInstance0.numValues());
      
      try { 
        naiveBayesMultinomialText0.distributionForInstance(binarySparseInstance0);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("weka.classifiers.bayes.NaiveBayesMultinomialText", e);
      }
  }

  /**
  //Test case number: 25
  /*Coverage entropy=2.7047814705228874
  */
  @Test(timeout = 4000)
  public void test25()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      assertNotNull(naiveBayesMultinomialText0);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      
      boolean boolean0 = FileSystemHandling.appendDataToFile((EvoSuiteFile) null, (byte[]) null);
      assertFalse(boolean0);
      
      naiveBayesMultinomialText0.m_useStopList = true;
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertTrue(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      
      double[] doubleArray0 = new double[6];
      doubleArray0[1] = 404.47827;
      DiscreteEstimator discreteEstimator0 = new DiscreteEstimator(2186, true);
      assertNotNull(discreteEstimator0);
      assertEquals(2186, discreteEstimator0.getNumSymbols());
      assertEquals("If set to true, estimator may output additional info to the console.", discreteEstimator0.debugTipText());
      assertFalse(discreteEstimator0.getDebug());
      assertEquals(2186.0, discreteEstimator0.getSumOfCounts(), 0.01);
      
      Capabilities capabilities0 = discreteEstimator0.getCapabilities();
      assertNotNull(capabilities0);
      assertEquals(2186, discreteEstimator0.getNumSymbols());
      assertEquals("If set to true, estimator may output additional info to the console.", discreteEstimator0.debugTipText());
      assertFalse(discreteEstimator0.getDebug());
      assertEquals(2186.0, discreteEstimator0.getSumOfCounts(), 0.01);
      assertFalse(capabilities0.hasDependencies());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      
      Capabilities capabilities1 = capabilities0.getAttributeCapabilities();
      assertFalse(capabilities1.equals((Object)capabilities0));
      assertNotNull(capabilities1);
      assertNotSame(capabilities0, capabilities1);
      assertNotSame(capabilities1, capabilities0);
      assertEquals(2186, discreteEstimator0.getNumSymbols());
      assertEquals("If set to true, estimator may output additional info to the console.", discreteEstimator0.debugTipText());
      assertFalse(discreteEstimator0.getDebug());
      assertEquals(2186.0, discreteEstimator0.getSumOfCounts(), 0.01);
      assertFalse(capabilities0.hasDependencies());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertFalse(capabilities1.hasDependencies());
      assertEquals(1, capabilities1.getMinimumNumberInstances());
      
      TestInstances testInstances0 = TestInstances.forCapabilities(capabilities1);
      assertFalse(capabilities0.equals((Object)capabilities1));
      assertFalse(capabilities1.equals((Object)capabilities0));
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertNotNull(testInstances0);
      assertNotSame(capabilities0, capabilities1);
      assertNotSame(capabilities1, capabilities0);
      assertEquals(2186, discreteEstimator0.getNumSymbols());
      assertEquals("If set to true, estimator may output additional info to the console.", discreteEstimator0.debugTipText());
      assertFalse(discreteEstimator0.getDebug());
      assertEquals(2186.0, discreteEstimator0.getSumOfCounts(), 0.01);
      assertFalse(capabilities0.hasDependencies());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertFalse(capabilities1.hasDependencies());
      assertEquals(1, capabilities1.getMinimumNumberInstances());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(0, testInstances0.getNumRelationalNominal());
      assertFalse(testInstances0.getNoClass());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(0, testInstances0.getNumNominal());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(0, testInstances0.getNumDate());
      
      TestInstances testInstances1 = new TestInstances();
      assertFalse(testInstances1.equals((Object)testInstances0));
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertNotNull(testInstances1);
      assertEquals(2, testInstances1.getNumAttributes());
      assertEquals(0, testInstances1.getNumRelational());
      assertEquals(2, testInstances1.getNumClasses());
      assertEquals(0, testInstances1.getNumString());
      assertEquals(0, testInstances1.getNumRelationalDate());
      assertEquals(0, testInstances1.getNumNumeric());
      assertEquals((-1), testInstances1.getClassIndex());
      assertEquals(1, testInstances1.getClassType());
      assertFalse(testInstances1.getNoClass());
      assertEquals(0, testInstances1.getNumRelationalString());
      assertEquals("Testdata", testInstances1.getRelation());
      assertEquals(" ", testInstances1.getWordSeparators());
      assertEquals(0, testInstances1.getNumRelationalNumeric());
      assertEquals(10, testInstances1.getNumInstancesRelational());
      assertEquals(20, testInstances1.getNumInstances());
      assertEquals(1, testInstances1.getNumNominal());
      assertEquals(2, testInstances1.getNumNominalValues());
      assertEquals(2, testInstances1.getNumRelationalNominalValues());
      assertEquals(0, testInstances1.getNumDate());
      assertEquals(1, testInstances1.getSeed());
      assertEquals(1, testInstances1.getNumRelationalNominal());
      assertFalse(testInstances1.getMultiInstance());
      
      Instances instances0 = testInstances1.generate("");
      assertFalse(testInstances1.equals((Object)testInstances0));
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertNotNull(instances0);
      assertNotSame(testInstances1, testInstances0);
      assertEquals(2, testInstances1.getNumAttributes());
      assertEquals(0, testInstances1.getNumRelational());
      assertEquals(2, testInstances1.getNumClasses());
      assertEquals(0, testInstances1.getNumString());
      assertEquals(0, testInstances1.getNumRelationalDate());
      assertEquals(0, testInstances1.getNumNumeric());
      assertEquals((-1), testInstances1.getClassIndex());
      assertEquals(1, testInstances1.getClassType());
      assertFalse(testInstances1.getNoClass());
      assertEquals(0, testInstances1.getNumRelationalString());
      assertEquals("Testdata", testInstances1.getRelation());
      assertEquals(" ", testInstances1.getWordSeparators());
      assertEquals(0, testInstances1.getNumRelationalNumeric());
      assertEquals(10, testInstances1.getNumInstancesRelational());
      assertEquals(20, testInstances1.getNumInstances());
      assertEquals(1, testInstances1.getNumNominal());
      assertEquals(2, testInstances1.getNumNominalValues());
      assertEquals(2, testInstances1.getNumRelationalNominalValues());
      assertEquals(0, testInstances1.getNumDate());
      assertEquals(1, testInstances1.getSeed());
      assertEquals(1, testInstances1.getNumRelationalNominal());
      assertFalse(testInstances1.getMultiInstance());
      assertEquals(20, instances0.size());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(2, instances0.numClasses());
      assertEquals(20, instances0.numInstances());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(2, instances0.numAttributes());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(1, instances0.classIndex());
      
      naiveBayesMultinomialText0.buildClassifier(instances0);
      assertFalse(testInstances1.equals((Object)testInstances0));
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertNotSame(testInstances1, testInstances0);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertTrue(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(2, testInstances1.getNumAttributes());
      assertEquals(0, testInstances1.getNumRelational());
      assertEquals(2, testInstances1.getNumClasses());
      assertEquals(0, testInstances1.getNumString());
      assertEquals(0, testInstances1.getNumRelationalDate());
      assertEquals(0, testInstances1.getNumNumeric());
      assertEquals((-1), testInstances1.getClassIndex());
      assertEquals(1, testInstances1.getClassType());
      assertFalse(testInstances1.getNoClass());
      assertEquals(0, testInstances1.getNumRelationalString());
      assertEquals("Testdata", testInstances1.getRelation());
      assertEquals(" ", testInstances1.getWordSeparators());
      assertEquals(0, testInstances1.getNumRelationalNumeric());
      assertEquals(10, testInstances1.getNumInstancesRelational());
      assertEquals(20, testInstances1.getNumInstances());
      assertEquals(1, testInstances1.getNumNominal());
      assertEquals(2, testInstances1.getNumNominalValues());
      assertEquals(2, testInstances1.getNumRelationalNominalValues());
      assertEquals(0, testInstances1.getNumDate());
      assertEquals(1, testInstances1.getSeed());
      assertEquals(1, testInstances1.getNumRelationalNominal());
      assertFalse(testInstances1.getMultiInstance());
      assertEquals(20, instances0.size());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(2, instances0.numClasses());
      assertEquals(20, instances0.numInstances());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(2, instances0.numAttributes());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(1, instances0.classIndex());
      
      String string0 = naiveBayesMultinomialText0.toString();
      assertEquals("The independent probability of a class\n--------------------------------------\nclass1\t12.0\nclass2\t10.0\n\nThe probability of a word given the class\n-----------------------------------------\n\tclass1\tclass2\t\n", string0);
      assertNotNull(string0);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertTrue(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      
      naiveBayesMultinomialText0.setOptions(testInstances1.DEFAULT_WORDS);
      assertFalse(testInstances1.equals((Object)testInstances0));
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertNotSame(testInstances1, testInstances0);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(2, testInstances1.getNumAttributes());
      assertEquals(0, testInstances1.getNumRelational());
      assertEquals(2, testInstances1.getNumClasses());
      assertEquals(0, testInstances1.getNumString());
      assertEquals(0, testInstances1.getNumRelationalDate());
      assertEquals(0, testInstances1.getNumNumeric());
      assertEquals((-1), testInstances1.getClassIndex());
      assertEquals(1, testInstances1.getClassType());
      assertFalse(testInstances1.getNoClass());
      assertEquals(0, testInstances1.getNumRelationalString());
      assertEquals("Testdata", testInstances1.getRelation());
      assertEquals(" ", testInstances1.getWordSeparators());
      assertEquals(0, testInstances1.getNumRelationalNumeric());
      assertEquals(10, testInstances1.getNumInstancesRelational());
      assertEquals(20, testInstances1.getNumInstances());
      assertEquals(1, testInstances1.getNumNominal());
      assertEquals(2, testInstances1.getNumNominalValues());
      assertEquals(2, testInstances1.getNumRelationalNominalValues());
      assertEquals(0, testInstances1.getNumDate());
      assertEquals(1, testInstances1.getSeed());
      assertEquals(1, testInstances1.getNumRelationalNominal());
      assertFalse(testInstances1.getMultiInstance());
  }

  /**
  //Test case number: 26
  /*Coverage entropy=2.7709738809556868
  */
  @Test(timeout = 4000)
  public void test26()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      assertNotNull(naiveBayesMultinomialText0);
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      
      NaiveBayesMultinomialText naiveBayesMultinomialText1 = new NaiveBayesMultinomialText();
      assertFalse(naiveBayesMultinomialText1.equals((Object)naiveBayesMultinomialText0));
      assertNotNull(naiveBayesMultinomialText1);
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText1.normTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText1.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText1.stopwordsTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText1.globalInfo());
      assertFalse(naiveBayesMultinomialText1.getUseWordFrequencies());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText1.stemmerTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText1.LNormTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText1.lowercaseTokensTipText());
      assertEquals(3.0, naiveBayesMultinomialText1.getMinWordFrequency(), 0.01);
      assertFalse(naiveBayesMultinomialText1.getDebug());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText1.minWordFrequencyTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText1.tokenizerTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText1.periodicPruningTipText());
      assertEquals(2.0, naiveBayesMultinomialText1.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText1.getNormalizeDocLength());
      assertEquals(0, naiveBayesMultinomialText1.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText1.useWordFrequenciesTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText1.useStopListTipText());
      assertEquals(1.0, naiveBayesMultinomialText1.getNorm(), 0.01);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText1.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText1.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText1.getUseStopList());
      
      String[] stringArray0 = new String[0];
      naiveBayesMultinomialText1.setOptions(stringArray0);
      assertFalse(naiveBayesMultinomialText1.equals((Object)naiveBayesMultinomialText0));
      assertEquals(0, stringArray0.length);
      assertNotSame(naiveBayesMultinomialText1, naiveBayesMultinomialText0);
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText1.normTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText1.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText1.stopwordsTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText1.globalInfo());
      assertFalse(naiveBayesMultinomialText1.getUseWordFrequencies());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText1.stemmerTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText1.LNormTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText1.lowercaseTokensTipText());
      assertEquals(3.0, naiveBayesMultinomialText1.getMinWordFrequency(), 0.01);
      assertFalse(naiveBayesMultinomialText1.getDebug());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText1.minWordFrequencyTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText1.tokenizerTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText1.periodicPruningTipText());
      assertEquals(2.0, naiveBayesMultinomialText1.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText1.getNormalizeDocLength());
      assertEquals(0, naiveBayesMultinomialText1.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText1.useWordFrequenciesTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText1.useStopListTipText());
      assertEquals(1.0, naiveBayesMultinomialText1.getNorm(), 0.01);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText1.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText1.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText1.getUseStopList());
      
      NullStemmer nullStemmer0 = new NullStemmer();
      assertNotNull(nullStemmer0);
      assertEquals("A dummy stemmer that performs no stemming at all.", nullStemmer0.globalInfo());
      
      naiveBayesMultinomialText0.setStemmer(nullStemmer0);
      assertFalse(naiveBayesMultinomialText0.equals((Object)naiveBayesMultinomialText1));
      assertNotSame(naiveBayesMultinomialText0, naiveBayesMultinomialText1);
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("A dummy stemmer that performs no stemming at all.", nullStemmer0.globalInfo());
      
      FindWithCapabilities findWithCapabilities0 = new FindWithCapabilities();
      assertNotNull(findWithCapabilities0);
      assertEquals("", findWithCapabilities0.getFilename());
      
      Capabilities capabilities0 = findWithCapabilities0.getCapabilities();
      assertNotNull(capabilities0);
      assertEquals("", findWithCapabilities0.getFilename());
      assertFalse(capabilities0.hasDependencies());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      
      TestInstances testInstances0 = TestInstances.forCapabilities(capabilities0);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertNotNull(testInstances0);
      assertEquals("", findWithCapabilities0.getFilename());
      assertFalse(capabilities0.hasDependencies());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(1, testInstances0.getNumAttributes());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals((-1), testInstances0.getClassIndex());
      assertFalse(testInstances0.getNoClass());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(0, testInstances0.getNumNominal());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(0, testInstances0.getNumRelationalNominal());
      assertEquals(1, testInstances0.getSeed());
      assertFalse(testInstances0.getMultiInstance());
      
      Instances instances0 = testInstances0.generate("Stacking\n\nBase classifiers\n\n");
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertNotNull(instances0);
      assertEquals("", findWithCapabilities0.getFilename());
      assertFalse(capabilities0.hasDependencies());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(1, testInstances0.getNumAttributes());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals((-1), testInstances0.getClassIndex());
      assertFalse(testInstances0.getNoClass());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(0, testInstances0.getNumNominal());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(0, testInstances0.getNumRelationalNominal());
      assertEquals(1, testInstances0.getSeed());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(0, instances0.classIndex());
      assertEquals(1, instances0.numAttributes());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(2, instances0.numClasses());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20, instances0.numInstances());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(20, instances0.size());
      
      naiveBayesMultinomialText0.buildClassifier(instances0);
      assertFalse(naiveBayesMultinomialText0.equals((Object)naiveBayesMultinomialText1));
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertNotSame(naiveBayesMultinomialText0, naiveBayesMultinomialText1);
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("", findWithCapabilities0.getFilename());
      assertFalse(capabilities0.hasDependencies());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(1, testInstances0.getNumAttributes());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals((-1), testInstances0.getClassIndex());
      assertFalse(testInstances0.getNoClass());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(0, testInstances0.getNumNominal());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(0, testInstances0.getNumRelationalNominal());
      assertEquals(1, testInstances0.getSeed());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(0, instances0.classIndex());
      assertEquals(1, instances0.numAttributes());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(2, instances0.numClasses());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20, instances0.numInstances());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(20, instances0.size());
      
      BinarySparseInstance binarySparseInstance0 = new BinarySparseInstance(678);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertNotNull(binarySparseInstance0);
      assertEquals(678, binarySparseInstance0.numValues());
      assertEquals(1.0, binarySparseInstance0.weight(), 0.01);
      assertEquals(678, binarySparseInstance0.numAttributes());
      
      int[] intArray0 = new int[6];
      intArray0[0] = 0;
      intArray0[1] = 31;
      intArray0[2] = 0;
      intArray0[3] = (-1);
      intArray0[4] = 631;
      intArray0[5] = 0;
      BinarySparseInstance binarySparseInstance1 = new BinarySparseInstance(0, intArray0, (-324));
      assertFalse(binarySparseInstance1.equals((Object)binarySparseInstance0));
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertEquals(6, intArray0.length);
      assertNotNull(binarySparseInstance1);
      assertArrayEquals(new int[] {0, 31, 0, (-1), 631, 0}, intArray0);
      assertEquals(6, binarySparseInstance1.numValues());
      assertEquals(0.0, binarySparseInstance1.weight(), 0.01);
      assertEquals((-324), binarySparseInstance1.numAttributes());
      
      double[] doubleArray0 = naiveBayesMultinomialText0.distributionForInstance(binarySparseInstance1);
      assertFalse(naiveBayesMultinomialText0.equals((Object)naiveBayesMultinomialText1));
      assertFalse(binarySparseInstance1.equals((Object)binarySparseInstance0));
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertEquals(2, doubleArray0.length);
      assertEquals(6, intArray0.length);
      assertNotNull(doubleArray0);
      assertNotSame(naiveBayesMultinomialText0, naiveBayesMultinomialText1);
      assertNotSame(binarySparseInstance1, binarySparseInstance0);
      assertArrayEquals(new double[] {0.5, 0.5}, doubleArray0, 0.01);
      assertArrayEquals(new int[] {0, 31, 0, (-1), 631, 0}, intArray0);
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(6, binarySparseInstance1.numValues());
      assertEquals(0.0, binarySparseInstance1.weight(), 0.01);
      assertEquals((-324), binarySparseInstance1.numAttributes());
      
      System.setCurrentTimeMillis(1308L);
      // Undeclared exception!
      try { 
        naiveBayesMultinomialText0.tokenizeInstance(binarySparseInstance1, true);
        fail("Expecting exception: RuntimeException");
      
      } catch(RuntimeException e) {
         //
         // DenseInstance doesn't have access to a dataset!
         //
         verifyException("weka.core.AbstractInstance", e);
      }
  }
}
