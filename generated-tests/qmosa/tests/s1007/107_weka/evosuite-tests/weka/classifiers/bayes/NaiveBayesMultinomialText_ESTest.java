/*
 * This file was automatically generated by EvoSuite
 * Fri Aug 24 10:10:25 GMT 2018
 */

package weka.classifiers.bayes;

import org.junit.Test;
import static org.junit.Assert.*;
import static org.evosuite.runtime.EvoAssertions.*;
import java.io.File;
import java.net.URI;
import java.util.ArrayList;
import java.util.Enumeration;
import java.util.HashMap;
import java.util.LinkedHashMap;
import java.util.Locale;
import java.util.Map;
import java.util.Properties;
import org.evosuite.runtime.EvoRunner;
import org.evosuite.runtime.EvoRunnerParameters;
import org.evosuite.runtime.Random;
import org.evosuite.runtime.mock.java.io.MockFile;
import org.evosuite.runtime.mock.java.net.MockURI;
import org.evosuite.runtime.testdata.EvoSuiteFile;
import org.evosuite.runtime.testdata.FileSystemHandling;
import org.junit.runner.RunWith;
import weka.classifiers.AbstractClassifier;
import weka.classifiers.bayes.NaiveBayesMultinomialText;
import weka.classifiers.functions.SGDText;
import weka.classifiers.functions.supportVector.PrecomputedKernelMatrixKernel;
import weka.classifiers.lazy.IBk;
import weka.classifiers.meta.CostSensitiveClassifier;
import weka.classifiers.misc.InputMappedClassifier;
import weka.core.AbstractInstance;
import weka.core.AllJavadoc;
import weka.core.Attribute;
import weka.core.BinarySparseInstance;
import weka.core.DenseInstance;
import weka.core.Instance;
import weka.core.Instances;
import weka.core.Option;
import weka.core.ProtectedProperties;
import weka.core.SparseInstance;
import weka.core.TestInstances;
import weka.core.neighboursearch.CoverTree;
import weka.core.neighboursearch.KDTree;
import weka.core.neighboursearch.balltrees.BallNode;
import weka.core.neighboursearch.balltrees.MiddleOutConstructor;
import weka.core.stemmers.IteratedLovinsStemmer;
import weka.core.stemmers.LovinsStemmer;
import weka.core.stemmers.SnowballStemmer;
import weka.core.stemmers.Stemmer;
import weka.core.tokenizers.Tokenizer;

@RunWith(EvoRunner.class) @EvoRunnerParameters(mockJVMNonDeterminism = true, useVFS = true, useVNET = true, resetStaticState = true, separateClassLoader = true, useJEE = true) 
public class NaiveBayesMultinomialText_ESTest extends NaiveBayesMultinomialText_ESTest_scaffolding {

  /**
  //Test case number: 0
  /*Coverage entropy=1.945910149055313
  */
  @Test(timeout = 4000)
  public void test00()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      LinkedHashMap<Integer, LinkedHashMap<String, NaiveBayesMultinomialText.Count>> linkedHashMap0 = new LinkedHashMap<Integer, LinkedHashMap<String, NaiveBayesMultinomialText.Count>>();
      naiveBayesMultinomialText0.m_probOfWordGivenClass = (Map<Integer, LinkedHashMap<String, NaiveBayesMultinomialText.Count>>) linkedHashMap0;
      naiveBayesMultinomialText0.setNormalizeDocLength(true);
      naiveBayesMultinomialText0.periodicPruningTipText();
      Integer integer0 = new Integer(1494);
      LinkedHashMap<String, NaiveBayesMultinomialText.Count> linkedHashMap1 = naiveBayesMultinomialText0.m_inputVector;
      linkedHashMap0.put(integer0, (LinkedHashMap<String, NaiveBayesMultinomialText.Count>) null);
      Integer.getInteger("");
      LinkedHashMap<String, NaiveBayesMultinomialText.Count> linkedHashMap2 = new LinkedHashMap<String, NaiveBayesMultinomialText.Count>();
      linkedHashMap0.put((Integer) null, linkedHashMap2);
      naiveBayesMultinomialText0.setLowercaseTokens(true);
      naiveBayesMultinomialText0.listOptions();
      naiveBayesMultinomialText0.toString();
      naiveBayesMultinomialText0.useWordFrequenciesTipText();
      InputMappedClassifier inputMappedClassifier0 = new InputMappedClassifier();
      int[] intArray0 = new int[8];
      intArray0[0] = 1494;
      intArray0[1] = 1494;
      intArray0[2] = 1494;
      intArray0[3] = 1494;
      intArray0[4] = 1494;
      intArray0[5] = 1494;
      intArray0[6] = 1494;
      intArray0[7] = 1494;
      // Undeclared exception!
      try { 
        BallNode.calcCentroidPivot(intArray0, (Instances) null);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("weka.core.neighboursearch.balltrees.BallNode", e);
      }
  }

  /**
  //Test case number: 1
  /*Coverage entropy=2.0794415416798357
  */
  @Test(timeout = 4000)
  public void test01()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      
      naiveBayesMultinomialText0.m_normalize = false;
      String string0 = naiveBayesMultinomialText0.toString();
      assertEquals("NaiveBayesMultinomialText: No model built yet.\n", string0);
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      
      naiveBayesMultinomialText0.pruneDictionary();
      String string1 = naiveBayesMultinomialText0.useWordFrequenciesTipText();
      assertEquals("Use word frequencies rather than binary bag of words representation", string1);
      
      IteratedLovinsStemmer iteratedLovinsStemmer0 = new IteratedLovinsStemmer();
      naiveBayesMultinomialText0.setStemmer(iteratedLovinsStemmer0);
      String[] stringArray0 = new String[1];
      stringArray0[0] = "NaiveBayesMultinomialText: No model built yet.\n";
      NaiveBayesMultinomialText.main(stringArray0);
      String string2 = naiveBayesMultinomialText0.stopwordsTipText();
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", string2);
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
  }

  /**
  //Test case number: 2
  /*Coverage entropy=2.0794415416798357
  */
  @Test(timeout = 4000)
  public void test02()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.setNormalizeDocLength(true);
      naiveBayesMultinomialText0.setNormalizeDocLength(true);
      Map<Integer, LinkedHashMap<String, NaiveBayesMultinomialText.Count>> map0 = naiveBayesMultinomialText0.m_probOfWordGivenClass;
      naiveBayesMultinomialText0.m_probOfWordGivenClass = null;
      naiveBayesMultinomialText0.getRevision();
      naiveBayesMultinomialText0.setUseStopList(true);
      naiveBayesMultinomialText0.getMinWordFrequency();
      naiveBayesMultinomialText0.useWordFrequenciesTipText();
      String[] stringArray0 = new String[2];
      stringArray0[0] = "9122";
      stringArray0[1] = "Use word frequencies rather than binary bag of words representation";
      NaiveBayesMultinomialText.main(stringArray0);
      assertEquals(2, stringArray0.length);
  }

  /**
  //Test case number: 3
  /*Coverage entropy=1.3862943611198906
  */
  @Test(timeout = 4000)
  public void test03()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.LNormTipText();
      String[] stringArray0 = new String[7];
      stringArray0[0] = "The LNorm to use for document length normalization.";
      stringArray0[1] = "The LNorm to use for document length normalization.";
      stringArray0[2] = "The LNorm to use for document length normalization.";
      stringArray0[3] = "-stemmer";
      stringArray0[4] = "The LNorm to use for document length normalization.";
      stringArray0[5] = "";
      stringArray0[6] = "The LNorm to use for document length normalization.";
      NaiveBayesMultinomialText.main(stringArray0);
      assertEquals(7, stringArray0.length);
  }

  /**
  //Test case number: 4
  /*Coverage entropy=0.6931471805599453
  */
  @Test(timeout = 4000)
  public void test04()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.minWordFrequencyTipText();
      double[] doubleArray0 = new double[0];
      SparseInstance sparseInstance0 = new SparseInstance((-1648.39068), doubleArray0);
      // Undeclared exception!
      try { 
        sparseInstance0.classValue();
        fail("Expecting exception: RuntimeException");
      
      } catch(RuntimeException e) {
         //
         // DenseInstance doesn't have access to a dataset!
         //
         verifyException("weka.core.AbstractInstance", e);
      }
  }

  /**
  //Test case number: 5
  /*Coverage entropy=2.5649493574615376
  */
  @Test(timeout = 4000)
  public void test05()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.setDebug(false);
      naiveBayesMultinomialText0.getCapabilities();
      naiveBayesMultinomialText0.setUseStopList(true);
      naiveBayesMultinomialText0.getUseStopList();
      Tokenizer tokenizer0 = naiveBayesMultinomialText0.getTokenizer();
      naiveBayesMultinomialText0.toString();
      naiveBayesMultinomialText0.getNormalizeDocLength();
      naiveBayesMultinomialText0.periodicPruningTipText();
      naiveBayesMultinomialText0.getLNorm();
      naiveBayesMultinomialText0.LNormTipText();
      naiveBayesMultinomialText0.m_tokenizer = tokenizer0;
      naiveBayesMultinomialText0.toString();
      naiveBayesMultinomialText0.stopwordsTipText();
      double double0 = 4.0;
      naiveBayesMultinomialText0.setLNorm(4.0);
      naiveBayesMultinomialText0.stemmerTipText();
      // Undeclared exception!
      try { 
        MockURI.normalize((URI) null);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("org.evosuite.runtime.mock.java.net.MockURI", e);
      }
  }

  /**
  //Test case number: 6
  /*Coverage entropy=1.3862943611198906
  */
  @Test(timeout = 4000)
  public void test06()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      HashMap<Integer, LinkedHashMap<String, NaiveBayesMultinomialText.Count>> hashMap0 = new HashMap<Integer, LinkedHashMap<String, NaiveBayesMultinomialText.Count>>();
      naiveBayesMultinomialText0.m_probOfWordGivenClass = (Map<Integer, LinkedHashMap<String, NaiveBayesMultinomialText.Count>>) hashMap0;
      double[] doubleArray0 = new double[6];
      doubleArray0[0] = (-1.0);
      doubleArray0[1] = (-873.0);
      doubleArray0[2] = (-574.0);
      doubleArray0[3] = 0.0;
      doubleArray0[4] = 0.0;
      Integer integer0 = new Integer(0);
      hashMap0.put(integer0, (LinkedHashMap<String, NaiveBayesMultinomialText.Count>) null);
      doubleArray0[5] = Double.POSITIVE_INFINITY;
      naiveBayesMultinomialText0.m_wordsPerClass = doubleArray0;
      naiveBayesMultinomialText0.setNormalizeDocLength(false);
      String string0 = naiveBayesMultinomialText0.getRevision();
      assertEquals("9122", string0);
      
      naiveBayesMultinomialText0.setTokenizer((Tokenizer) null);
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
  }

  /**
  //Test case number: 7
  /*Coverage entropy=1.6094379124341005
  */
  @Test(timeout = 4000)
  public void test07()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.lowercaseTokensTipText();
      naiveBayesMultinomialText0.setPeriodicPruning(10000);
      naiveBayesMultinomialText0.normalizeDocLengthTipText();
      CostSensitiveClassifier costSensitiveClassifier0 = new CostSensitiveClassifier();
      File file0 = costSensitiveClassifier0.getOnDemandDirectory();
      naiveBayesMultinomialText0.setStopwords(file0);
      assertEquals(10000, naiveBayesMultinomialText0.getPeriodicPruning());
  }

  /**
  //Test case number: 8
  /*Coverage entropy=2.5649493574615376
  */
  @Test(timeout = 4000)
  public void test08()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      String[] stringArray0 = new String[8];
      stringArray0[0] = "`rIIDp`1";
      stringArray0[1] = "Comparing with null object";
      stringArray0[2] = "<54{/x'o;$T9\"";
      stringArray0[3] = "The LNorm to use for document length normalization.";
      stringArray0[4] = "-P";
      stringArray0[5] = "";
      stringArray0[6] = "amongst";
      stringArray0[7] = "";
      AbstractClassifier.runClassifier(naiveBayesMultinomialText0, stringArray0);
      naiveBayesMultinomialText0.setDebug(true);
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      
      naiveBayesMultinomialText0.m_lnorm = (-1368.074);
      naiveBayesMultinomialText0.getCapabilities();
      String[] stringArray1 = new String[7];
      stringArray1[0] = "weka/core/Capabilities.props";
      stringArray1[1] = "weka/core/Capabilities.props";
      stringArray1[2] = "weka/core/Capabilities.props";
      stringArray1[3] = "weka/core/Capabilities.props";
      stringArray1[4] = "weka/core/Capabilities.props";
      stringArray1[5] = "weka/core/Capabilities.props";
      stringArray1[6] = "weka/core/Capabilities.props";
      naiveBayesMultinomialText0.setOptions(stringArray1);
      naiveBayesMultinomialText0.setPeriodicPruning(767);
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
  }

  /**
  //Test case number: 9
  /*Coverage entropy=0.9502705392332347
  */
  @Test(timeout = 4000)
  public void test09()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      int int0 = 1928;
      PrecomputedKernelMatrixKernel precomputedKernelMatrixKernel0 = new PrecomputedKernelMatrixKernel();
      File file0 = precomputedKernelMatrixKernel0.getKernelMatrixFile();
      File file1 = MockFile.createTempFile("~VlB~zA_", "~VlB~zA_", file0);
      naiveBayesMultinomialText0.setStopwords(file1);
      AbstractClassifier.makeCopies(naiveBayesMultinomialText0, 1928);
      try { 
        AbstractClassifier.forName("This should never happen!", (String[]) null);
        fail("Expecting exception: Exception");
      
      } catch(Exception e) {
         //
         // Can't find class called: This should never happen!
         //
         verifyException("weka.core.Utils", e);
      }
  }

  /**
  //Test case number: 10
  /*Coverage entropy=2.0794415416798357
  */
  @Test(timeout = 4000)
  public void test10()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.toString();
      naiveBayesMultinomialText0.setUseStopList(true);
      String[] stringArray0 = new String[0];
      NaiveBayesMultinomialText.main(stringArray0);
      naiveBayesMultinomialText0.getRevision();
      IteratedLovinsStemmer iteratedLovinsStemmer0 = new IteratedLovinsStemmer();
      naiveBayesMultinomialText0.m_stemmer = (Stemmer) iteratedLovinsStemmer0;
      naiveBayesMultinomialText0.getUseWordFrequencies();
      naiveBayesMultinomialText0.getCapabilities();
      iteratedLovinsStemmer0.stem("Uu");
      DenseInstance denseInstance0 = null;
      try {
        denseInstance0 = new DenseInstance((-403));
        fail("Expecting exception: NegativeArraySizeException");
      
      } catch(NegativeArraySizeException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("weka.core.DenseInstance", e);
      }
  }

  /**
  //Test case number: 11
  /*Coverage entropy=1.0986122886681096
  */
  @Test(timeout = 4000)
  public void test11()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      SnowballStemmer snowballStemmer0 = new SnowballStemmer("-tokenizer");
      snowballStemmer0.toString();
      boolean boolean0 = true;
      naiveBayesMultinomialText0.m_wordFrequencies = true;
      naiveBayesMultinomialText0.m_useStopList = true;
      naiveBayesMultinomialText0.m_stemmer = (Stemmer) snowballStemmer0;
      naiveBayesMultinomialText0.setPeriodicPruning(200);
      // Undeclared exception!
      try { 
        naiveBayesMultinomialText0.pruneDictionary();
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("weka.classifiers.bayes.NaiveBayesMultinomialText", e);
      }
  }

  /**
  //Test case number: 12
  /*Coverage entropy=1.0986122886681096
  */
  @Test(timeout = 4000)
  public void test12()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      SGDText sGDText0 = new SGDText();
      Stemmer stemmer0 = sGDText0.getStemmer();
      naiveBayesMultinomialText0.m_stemmer = stemmer0;
      naiveBayesMultinomialText0.setNorm(2884.3978212574098);
      naiveBayesMultinomialText0.lowercaseTokensTipText();
      CoverTree coverTree0 = new CoverTree();
      double[] doubleArray0 = new double[7];
      doubleArray0[0] = 846.162;
      doubleArray0[1] = 846.162;
      doubleArray0[2] = (double) 0;
      doubleArray0[3] = 1.0001;
      doubleArray0[4] = (double) 1;
      doubleArray0[5] = (double) 1;
      doubleArray0[6] = (double) 1;
      SparseInstance sparseInstance0 = new SparseInstance(846.162, doubleArray0);
      try { 
        coverTree0.kNearestNeighbours(sparseInstance0, (-2059));
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("weka.core.Instances", e);
      }
  }

  /**
  //Test case number: 13
  /*Coverage entropy=2.0794415416798357
  */
  @Test(timeout = 4000)
  public void test13()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertNotNull(naiveBayesMultinomialText0);
      
      naiveBayesMultinomialText0.reset();
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      
      File file0 = naiveBayesMultinomialText0.getStopwords();
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertTrue(file0.isDirectory());
      assertEquals(0L, file0.getTotalSpace());
      assertEquals(1392409281320L, file0.lastModified());
      assertEquals(0L, file0.getFreeSpace());
      assertTrue(file0.isAbsolute());
      assertTrue(file0.canRead());
      assertTrue(file0.exists());
      assertEquals("/home/ubuntu/evosuite_readability_gen/projects/107_weka", file0.toString());
      assertFalse(file0.isHidden());
      assertEquals(0L, file0.getUsableSpace());
      assertFalse(file0.isFile());
      assertTrue(file0.canWrite());
      assertEquals("/home/ubuntu/evosuite_readability_gen/projects", file0.getParent());
      assertEquals(0L, file0.length());
      assertEquals("107_weka", file0.getName());
      assertTrue(file0.canExecute());
      assertNotNull(file0);
      
      Enumeration<Option> enumeration0 = naiveBayesMultinomialText0.listOptions();
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertNotNull(enumeration0);
      
      String string0 = naiveBayesMultinomialText0.globalInfo();
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", string0);
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertNotNull(string0);
      
      double[] doubleArray0 = new double[4];
      naiveBayesMultinomialText0.m_normalize = true;
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertTrue(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      
      doubleArray0[0] = 2.0;
      doubleArray0[1] = 2.0;
      doubleArray0[2] = 2.0;
      String string1 = naiveBayesMultinomialText0.getRevision();
      assertEquals("9122", string1);
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertTrue(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertNotNull(string1);
      assertFalse(string1.equals((Object)string0));
      
      doubleArray0[3] = 2.0;
      DenseInstance denseInstance0 = new DenseInstance(2.0, doubleArray0);
      assertEquals(2.0, denseInstance0.weight(), 0.01);
      assertEquals(4, denseInstance0.numValues());
      assertEquals(4, denseInstance0.numAttributes());
      assertNotNull(denseInstance0);
      assertArrayEquals(new double[] {2.0, 2.0, 2.0, 2.0}, doubleArray0, 0.01);
      assertEquals(4, doubleArray0.length);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      
      SparseInstance sparseInstance0 = new SparseInstance(denseInstance0);
      assertEquals(2.0, denseInstance0.weight(), 0.01);
      assertEquals(4, denseInstance0.numValues());
      assertEquals(4, denseInstance0.numAttributes());
      assertEquals(2.0, sparseInstance0.weight(), 0.01);
      assertEquals(4, sparseInstance0.numValues());
      assertEquals(4, sparseInstance0.numAttributes());
      assertNotNull(sparseInstance0);
      assertArrayEquals(new double[] {2.0, 2.0, 2.0, 2.0}, doubleArray0, 0.01);
      assertEquals(4, doubleArray0.length);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      
      try { 
        naiveBayesMultinomialText0.updateClassifier(sparseInstance0);
        fail("Expecting exception: RuntimeException");
      
      } catch(RuntimeException e) {
         //
         // DenseInstance doesn't have access to a dataset!
         //
         verifyException("weka.core.AbstractInstance", e);
      }
  }

  /**
  //Test case number: 14
  /*Coverage entropy=1.3862943611198906
  */
  @Test(timeout = 4000)
  public void test14()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertNotNull(naiveBayesMultinomialText0);
      
      double[] doubleArray0 = new double[9];
      doubleArray0[0] = 243.0;
      doubleArray0[1] = (-3642.5252555);
      doubleArray0[2] = 1.0;
      doubleArray0[3] = 0.0;
      doubleArray0[4] = 429.2;
      doubleArray0[5] = (-1883.90549377865);
      doubleArray0[6] = 0.0;
      doubleArray0[7] = 2854.778808061179;
      doubleArray0[8] = (-2643.98);
      naiveBayesMultinomialText0.m_wordsPerClass = doubleArray0;
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      
      String[] stringArray0 = new String[1];
      stringArray0[0] = "cbB?eD{Rt,g;_";
      AbstractClassifier.runClassifier(naiveBayesMultinomialText0, stringArray0);
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals(1, stringArray0.length);
      
      String string0 = naiveBayesMultinomialText0.normTipText();
      assertEquals("The norm of the instances after normalization.", string0);
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertNotNull(string0);
      
      String string1 = naiveBayesMultinomialText0.normalizeDocLengthTipText();
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", string1);
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertNotNull(string1);
      assertFalse(string1.equals((Object)string0));
  }

  /**
  //Test case number: 15
  /*Coverage entropy=1.7917594692280547
  */
  @Test(timeout = 4000)
  public void test15()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertNotNull(naiveBayesMultinomialText0);
      
      naiveBayesMultinomialText0.setNormalizeDocLength(true);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertTrue(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      
      naiveBayesMultinomialText0.m_lowercaseTokens = true;
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertTrue(naiveBayesMultinomialText0.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertTrue(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      
      double[] doubleArray0 = new double[5];
      doubleArray0[0] = 2.0;
      doubleArray0[1] = 2749.328282;
      doubleArray0[2] = 0.0;
      naiveBayesMultinomialText0.setUseWordFrequencies(false);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertTrue(naiveBayesMultinomialText0.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertTrue(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      
      doubleArray0[3] = (-1493.9719035);
      doubleArray0[4] = 0.0;
      naiveBayesMultinomialText0.m_probOfClass = doubleArray0;
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertTrue(naiveBayesMultinomialText0.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertTrue(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      
      naiveBayesMultinomialText0.m_wordFrequencies = false;
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertTrue(naiveBayesMultinomialText0.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertTrue(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      
      String string0 = naiveBayesMultinomialText0.tokenizerTipText();
      assertEquals("The tokenizing algorithm to use on the strings.", string0);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertTrue(naiveBayesMultinomialText0.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertTrue(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertNotNull(string0);
      
      naiveBayesMultinomialText0.setMinWordFrequency(1.0E-10);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertTrue(naiveBayesMultinomialText0.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(1.0E-10, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertTrue(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      
      String string1 = naiveBayesMultinomialText0.stopwordsTipText();
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", string1);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertTrue(naiveBayesMultinomialText0.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(1.0E-10, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertTrue(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertNotNull(string1);
      assertFalse(string1.equals((Object)string0));
      
      naiveBayesMultinomialText0.setNormalizeDocLength(true);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertTrue(naiveBayesMultinomialText0.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(1.0E-10, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertTrue(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      
      InputMappedClassifier inputMappedClassifier0 = new InputMappedClassifier();
      assertEquals("Ignore case when matching attribute names and nomina values.", inputMappedClassifier0.ignoreCaseForNamesTipText());
      assertEquals("", inputMappedClassifier0.getModelPath());
      assertTrue(inputMappedClassifier0.getTrim());
      assertEquals("Set the path from which to load a model. Loading occurs when the first test instance is received. Environment variables can be used in the supplied path.", inputMappedClassifier0.modelPathTipText());
      assertEquals("Trim white space from each end of attribute names and nominal values before matching.", inputMappedClassifier0.trimTipText());
      assertEquals(0, inputMappedClassifier0.graphType());
      assertTrue(inputMappedClassifier0.getIgnoreCaseForNames());
      assertEquals("Don't output a report of model-to-input mappings.", inputMappedClassifier0.suppressMappingReportTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", inputMappedClassifier0.debugTipText());
      assertEquals("Wrapper classifier that addresses incompatible training and test data by building a mapping between the training data that a classifier has been built with and the incoming test instances' structure. Model attributes that are not found in the incoming instances receive missing values, so do incoming nominal attribute values that the classifier has not seen before. A new classifier can be trained or an existing one loaded from a file.", inputMappedClassifier0.globalInfo());
      assertFalse(inputMappedClassifier0.getSuppressMappingReport());
      assertFalse(inputMappedClassifier0.getDebug());
      assertEquals("The base classifier to be used.", inputMappedClassifier0.classifierTipText());
      assertNotNull(inputMappedClassifier0);
      
      Instances instances0 = null;
      try { 
        inputMappedClassifier0.getModelHeader((Instances) null);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("weka.core.Instances", e);
      }
  }

  /**
  //Test case number: 16
  /*Coverage entropy=2.772588722239781
  */
  @Test(timeout = 4000)
  public void test16()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertNotNull(naiveBayesMultinomialText0);
      
      String[] stringArray0 = new String[8];
      String string0 = naiveBayesMultinomialText0.toString();
      assertEquals("NaiveBayesMultinomialText: No model built yet.\n", string0);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertNotNull(string0);
      
      stringArray0[0] = "`rIIDp`1";
      naiveBayesMultinomialText0.setNormalizeDocLength(true);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertTrue(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      
      String[] stringArray1 = naiveBayesMultinomialText0.getOptions();
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertTrue(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertNotNull(stringArray1);
      assertFalse(stringArray1.equals((Object)stringArray0));
      assertNotSame(stringArray1, stringArray0);
      assertEquals(13, stringArray1.length);
      
      NaiveBayesMultinomialText naiveBayesMultinomialText1 = new NaiveBayesMultinomialText();
      assertFalse(naiveBayesMultinomialText1.getLowercaseTokens());
      assertEquals(1.0, naiveBayesMultinomialText1.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText1.getDebug());
      assertEquals(0, naiveBayesMultinomialText1.getPeriodicPruning());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText1.useStopListTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText1.useWordFrequenciesTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText1.stopwordsTipText());
      assertEquals(2.0, naiveBayesMultinomialText1.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText1.getUseStopList());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText1.minWordFrequencyTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText1.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText1.stemmerTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText1.LNormTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText1.lowercaseTokensTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText1.normTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText1.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText1.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText1.globalInfo());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText1.tokenizerTipText());
      assertEquals(3.0, naiveBayesMultinomialText1.getMinWordFrequency(), 0.01);
      assertFalse(naiveBayesMultinomialText1.getNormalizeDocLength());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText1.periodicPruningTipText());
      assertNotNull(naiveBayesMultinomialText1);
      assertFalse(naiveBayesMultinomialText1.equals((Object)naiveBayesMultinomialText0));
      
      // Undeclared exception!
      try { 
        naiveBayesMultinomialText0.tokenizeInstance((Instance) null, true);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("weka.classifiers.bayes.NaiveBayesMultinomialText", e);
      }
  }

  /**
  //Test case number: 17
  /*Coverage entropy=1.7478680974667573
  */
  @Test(timeout = 4000)
  public void test17()  throws Throwable  {
      boolean boolean0 = FileSystemHandling.appendLineToFile((EvoSuiteFile) null, "U { kn?EG");
      assertFalse(boolean0);
      
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertNotNull(naiveBayesMultinomialText0);
      
      naiveBayesMultinomialText0.setNorm((-469.3955741));
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals((-469.3955741), naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      
      String string0 = naiveBayesMultinomialText0.getRevision();
      assertEquals("9122", string0);
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals((-469.3955741), naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertNotNull(string0);
      
      String string1 = naiveBayesMultinomialText0.toString();
      assertEquals("NaiveBayesMultinomialText: No model built yet.\n", string1);
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals((-469.3955741), naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertNotNull(string1);
      assertFalse(string1.equals((Object)string0));
      
      naiveBayesMultinomialText0.pruneDictionary();
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals((-469.3955741), naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      
      String string2 = naiveBayesMultinomialText0.toString();
      assertEquals("NaiveBayesMultinomialText: No model built yet.\n", string2);
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals((-469.3955741), naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertNotNull(string2);
      assertFalse(string2.equals((Object)string0));
      
      PrecomputedKernelMatrixKernel precomputedKernelMatrixKernel0 = new PrecomputedKernelMatrixKernel();
      assertEquals("The file holding the kernel matrix.", precomputedKernelMatrixKernel0.kernelMatrixFileTipText());
      assertFalse(precomputedKernelMatrixKernel0.getChecksTurnedOff());
      assertEquals("Turns on the output of debugging information.", precomputedKernelMatrixKernel0.debugTipText());
      assertEquals(0, precomputedKernelMatrixKernel0.numCacheHits());
      assertEquals("This kernel is based on a static kernel matrix that is read from a file. Instances must have a single nominal attribute (excluding the class). This attribute must be the first attribute in the file and its values are used to reference rows/columns in the kernel matrix. The second attribute must be the class attribute.", precomputedKernelMatrixKernel0.globalInfo());
      assertEquals("Turns time-consuming checks off - use with caution.", precomputedKernelMatrixKernel0.checksTurnedOffTipText());
      assertFalse(precomputedKernelMatrixKernel0.getDebug());
      assertEquals(0, precomputedKernelMatrixKernel0.numEvals());
      assertNotNull(precomputedKernelMatrixKernel0);
      
      File file0 = precomputedKernelMatrixKernel0.getKernelMatrixFile();
      assertEquals("The file holding the kernel matrix.", precomputedKernelMatrixKernel0.kernelMatrixFileTipText());
      assertFalse(precomputedKernelMatrixKernel0.getChecksTurnedOff());
      assertEquals("Turns on the output of debugging information.", precomputedKernelMatrixKernel0.debugTipText());
      assertEquals(0, precomputedKernelMatrixKernel0.numCacheHits());
      assertEquals("This kernel is based on a static kernel matrix that is read from a file. Instances must have a single nominal attribute (excluding the class). This attribute must be the first attribute in the file and its values are used to reference rows/columns in the kernel matrix. The second attribute must be the class attribute.", precomputedKernelMatrixKernel0.globalInfo());
      assertEquals("Turns time-consuming checks off - use with caution.", precomputedKernelMatrixKernel0.checksTurnedOffTipText());
      assertFalse(precomputedKernelMatrixKernel0.getDebug());
      assertEquals(0, precomputedKernelMatrixKernel0.numEvals());
      assertFalse(file0.isAbsolute());
      assertEquals("kernelMatrix.matrix", file0.getName());
      assertFalse(file0.canExecute());
      assertEquals(0L, file0.length());
      assertFalse(file0.canRead());
      assertFalse(file0.canWrite());
      assertEquals(0L, file0.getFreeSpace());
      assertFalse(file0.exists());
      assertFalse(file0.isDirectory());
      assertFalse(file0.isHidden());
      assertNull(file0.getParent());
      assertEquals(0L, file0.getUsableSpace());
      assertEquals("kernelMatrix.matrix", file0.toString());
      assertEquals(0L, file0.lastModified());
      assertTrue(file0.isFile());
      assertEquals(0L, file0.getTotalSpace());
      assertNotNull(file0);
      
      naiveBayesMultinomialText0.setStopwords(file0);
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals((-469.3955741), naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("The file holding the kernel matrix.", precomputedKernelMatrixKernel0.kernelMatrixFileTipText());
      assertFalse(precomputedKernelMatrixKernel0.getChecksTurnedOff());
      assertEquals("Turns on the output of debugging information.", precomputedKernelMatrixKernel0.debugTipText());
      assertEquals(0, precomputedKernelMatrixKernel0.numCacheHits());
      assertEquals("This kernel is based on a static kernel matrix that is read from a file. Instances must have a single nominal attribute (excluding the class). This attribute must be the first attribute in the file and its values are used to reference rows/columns in the kernel matrix. The second attribute must be the class attribute.", precomputedKernelMatrixKernel0.globalInfo());
      assertEquals("Turns time-consuming checks off - use with caution.", precomputedKernelMatrixKernel0.checksTurnedOffTipText());
      assertFalse(precomputedKernelMatrixKernel0.getDebug());
      assertEquals(0, precomputedKernelMatrixKernel0.numEvals());
      assertFalse(file0.isAbsolute());
      assertEquals("kernelMatrix.matrix", file0.getName());
      assertFalse(file0.canExecute());
      assertEquals(0L, file0.length());
      assertFalse(file0.canRead());
      assertFalse(file0.canWrite());
      assertEquals(0L, file0.getFreeSpace());
      assertFalse(file0.exists());
      assertFalse(file0.isDirectory());
      assertFalse(file0.isHidden());
      assertNull(file0.getParent());
      assertEquals(0L, file0.getUsableSpace());
      assertEquals("kernelMatrix.matrix", file0.toString());
      assertEquals(0L, file0.lastModified());
      assertTrue(file0.isFile());
      assertEquals(0L, file0.getTotalSpace());
  }

  /**
  //Test case number: 18
  /*Coverage entropy=1.5607104090414063
  */
  @Test(timeout = 4000)
  public void test18()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertNotNull(naiveBayesMultinomialText0);
      
      naiveBayesMultinomialText0.pruneDictionary();
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      
      int[] intArray0 = new int[1];
      naiveBayesMultinomialText0.m_useStopList = true;
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertTrue(naiveBayesMultinomialText0.getUseStopList());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      
      byte[] byteArray0 = new byte[4];
      byteArray0[0] = (byte) (-91);
      byteArray0[1] = (byte) (-104);
      byteArray0[2] = (byte)123;
      byteArray0[3] = (byte)14;
      boolean boolean0 = FileSystemHandling.appendDataToFile((EvoSuiteFile) null, byteArray0);
      assertFalse(boolean0);
      assertArrayEquals(new byte[] {(byte) (-91), (byte) (-104), (byte)123, (byte)14}, byteArray0);
      assertEquals(4, byteArray0.length);
      
      intArray0[0] = 47;
      BinarySparseInstance binarySparseInstance0 = new BinarySparseInstance(0.0, intArray0, 47);
      assertEquals(1, binarySparseInstance0.numValues());
      assertEquals(47, binarySparseInstance0.numAttributes());
      assertEquals(0.0, binarySparseInstance0.weight(), 0.01);
      assertNotNull(binarySparseInstance0);
      assertArrayEquals(new int[] {47}, intArray0);
      assertEquals(1, intArray0.length);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      
      try { 
        naiveBayesMultinomialText0.distributionForInstance(binarySparseInstance0);
        fail("Expecting exception: RuntimeException");
      
      } catch(RuntimeException e) {
         //
         // DenseInstance doesn't have access to a dataset!
         //
         verifyException("weka.core.AbstractInstance", e);
      }
  }

  /**
  //Test case number: 19
  /*Coverage entropy=2.006954892210766
  */
  @Test(timeout = 4000)
  public void test19()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertNotNull(naiveBayesMultinomialText0);
      
      SnowballStemmer snowballStemmer0 = new SnowballStemmer("-tokenizer");
      assertFalse(snowballStemmer0.isPresent());
      assertEquals("A wrapper class for the Snowball stemmers. Only available if the Snowball classes are in the classpath.\nIf the class discovery is not dynamic, i.e., the property 'UseDynamic' in the props file 'weka/gui/GenericPropertiesCreator.props' is 'false', then the property 'org.tartarus.snowball.SnowballProgram' in the 'weka/gui/GenericObjectEditor.props' file has to be uncommented as well. If necessary you have to discover and fill in the snowball stemmers manually. You can use the 'weka.core.ClassDiscovery' for this:\n  java weka.core.ClassDiscovery org.tartarus.snowball.SnowballProgram org.tartarus.snowball.ext\n\nFor more information visit these web sites:\n  http://weka.wikispaces.com/Stemmers\n  http://snowball.tartarus.org/\n", snowballStemmer0.globalInfo());
      assertNotNull(snowballStemmer0);
      
      String string0 = snowballStemmer0.toString();
      assertEquals("weka.core.stemmers.SnowballStemmer", string0);
      assertFalse(snowballStemmer0.isPresent());
      assertEquals("A wrapper class for the Snowball stemmers. Only available if the Snowball classes are in the classpath.\nIf the class discovery is not dynamic, i.e., the property 'UseDynamic' in the props file 'weka/gui/GenericPropertiesCreator.props' is 'false', then the property 'org.tartarus.snowball.SnowballProgram' in the 'weka/gui/GenericObjectEditor.props' file has to be uncommented as well. If necessary you have to discover and fill in the snowball stemmers manually. You can use the 'weka.core.ClassDiscovery' for this:\n  java weka.core.ClassDiscovery org.tartarus.snowball.SnowballProgram org.tartarus.snowball.ext\n\nFor more information visit these web sites:\n  http://weka.wikispaces.com/Stemmers\n  http://snowball.tartarus.org/\n", snowballStemmer0.globalInfo());
      assertNotNull(string0);
      
      naiveBayesMultinomialText0.setPeriodicPruning((-2583));
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals((-2583), naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      
      naiveBayesMultinomialText0.pruneDictionary();
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals((-2583), naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      
      NaiveBayesMultinomialText naiveBayesMultinomialText1 = new NaiveBayesMultinomialText();
      assertEquals(0, naiveBayesMultinomialText1.getPeriodicPruning());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText1.useStopListTipText());
      assertEquals(1.0, naiveBayesMultinomialText1.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText1.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText1.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText1.getUseStopList());
      assertEquals(2.0, naiveBayesMultinomialText1.getLNorm(), 0.01);
      assertEquals(3.0, naiveBayesMultinomialText1.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText1.LNormTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText1.useWordFrequenciesTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText1.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText1.stemmerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText1.minWordFrequencyTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText1.stopwordsTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText1.periodicPruningTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText1.tokenizerTipText());
      assertFalse(naiveBayesMultinomialText1.getDebug());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText1.normTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText1.lowercaseTokensTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText1.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText1.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText1.globalInfo());
      assertNotNull(naiveBayesMultinomialText1);
      assertFalse(naiveBayesMultinomialText1.equals((Object)naiveBayesMultinomialText0));
      
      NaiveBayesMultinomialText naiveBayesMultinomialText2 = new NaiveBayesMultinomialText();
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText2.debugTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText2.normTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText2.stopwordsTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText2.LNormTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText2.stemmerTipText());
      assertFalse(naiveBayesMultinomialText2.getUseStopList());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText2.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText2.getPeriodicPruning());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText2.useStopListTipText());
      assertEquals(2.0, naiveBayesMultinomialText2.getLNorm(), 0.01);
      assertEquals(1.0, naiveBayesMultinomialText2.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText2.getLowercaseTokens());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText2.tokenizerTipText());
      assertEquals(3.0, naiveBayesMultinomialText2.getMinWordFrequency(), 0.01);
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText2.periodicPruningTipText());
      assertFalse(naiveBayesMultinomialText2.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText2.globalInfo());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText2.normalizeDocLengthTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText2.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText2.getDebug());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText2.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText2.getNormalizeDocLength());
      assertNotNull(naiveBayesMultinomialText2);
      assertFalse(naiveBayesMultinomialText2.equals((Object)naiveBayesMultinomialText1));
      assertFalse(naiveBayesMultinomialText2.equals((Object)naiveBayesMultinomialText0));
      
      naiveBayesMultinomialText2.setUseStopList(false);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText2.debugTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText2.normTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText2.stopwordsTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText2.LNormTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText2.stemmerTipText());
      assertFalse(naiveBayesMultinomialText2.getUseStopList());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText2.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText2.getPeriodicPruning());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText2.useStopListTipText());
      assertEquals(2.0, naiveBayesMultinomialText2.getLNorm(), 0.01);
      assertEquals(1.0, naiveBayesMultinomialText2.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText2.getLowercaseTokens());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText2.tokenizerTipText());
      assertEquals(3.0, naiveBayesMultinomialText2.getMinWordFrequency(), 0.01);
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText2.periodicPruningTipText());
      assertFalse(naiveBayesMultinomialText2.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText2.globalInfo());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText2.normalizeDocLengthTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText2.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText2.getDebug());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText2.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText2.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText2.equals((Object)naiveBayesMultinomialText1));
      assertFalse(naiveBayesMultinomialText2.equals((Object)naiveBayesMultinomialText0));
      assertNotSame(naiveBayesMultinomialText2, naiveBayesMultinomialText1);
      assertNotSame(naiveBayesMultinomialText2, naiveBayesMultinomialText0);
      
      NaiveBayesMultinomialText naiveBayesMultinomialText3 = new NaiveBayesMultinomialText();
      assertFalse(naiveBayesMultinomialText3.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText3.globalInfo());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText3.normTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText3.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText3.minWordFrequencyTipText());
      assertEquals(3.0, naiveBayesMultinomialText3.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText3.LNormTipText());
      assertFalse(naiveBayesMultinomialText3.getLowercaseTokens());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText3.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText3.useStopListTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText3.normalizeDocLengthTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText3.useWordFrequenciesTipText());
      assertFalse(naiveBayesMultinomialText3.getNormalizeDocLength());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText3.tokenizerTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText3.lowercaseTokensTipText());
      assertEquals(0, naiveBayesMultinomialText3.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText3.getDebug());
      assertEquals(1.0, naiveBayesMultinomialText3.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText3.getUseStopList());
      assertEquals(2.0, naiveBayesMultinomialText3.getLNorm(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText3.stopwordsTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText3.debugTipText());
      assertNotNull(naiveBayesMultinomialText3);
      assertFalse(naiveBayesMultinomialText3.equals((Object)naiveBayesMultinomialText2));
      assertFalse(naiveBayesMultinomialText3.equals((Object)naiveBayesMultinomialText1));
      assertFalse(naiveBayesMultinomialText3.equals((Object)naiveBayesMultinomialText0));
      
      String[] stringArray0 = new String[9];
      stringArray0[0] = "weka.core.stemmers.SnowballStemmer";
      stringArray0[1] = "weka.core.stemmers.SnowballStemmer";
      stringArray0[2] = "org.tartarus.snowball";
      stringArray0[3] = "weka.core.stemmers.SnowballStemmer";
      stringArray0[4] = "";
      stringArray0[5] = "org.tartarus.snowball.ext";
      stringArray0[6] = "-tokenizer";
      stringArray0[7] = "-tokenizer";
      stringArray0[8] = "org.tartarus.snowball";
      try { 
        naiveBayesMultinomialText0.setOptions(stringArray0);
        fail("Expecting exception: ClassNotFoundException");
      
      } catch(ClassNotFoundException e) {
      }
  }

  /**
  //Test case number: 20
  /*Coverage entropy=2.5649493574615376
  */
  @Test(timeout = 4000)
  public void test20()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertNotNull(naiveBayesMultinomialText0);
      
      double[] doubleArray0 = new double[8];
      doubleArray0[0] = 9.223372036854776E18;
      doubleArray0[1] = Double.NaN;
      doubleArray0[2] = 3.0;
      doubleArray0[3] = (-319.5617130248487);
      doubleArray0[4] = 0.0;
      doubleArray0[5] = 0.0;
      doubleArray0[6] = (-1.7976931348623157E308);
      doubleArray0[7] = 3.0;
      naiveBayesMultinomialText0.m_probOfClass = doubleArray0;
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      
      SnowballStemmer snowballStemmer0 = new SnowballStemmer("-tokenizer");
      assertFalse(snowballStemmer0.isPresent());
      assertEquals("A wrapper class for the Snowball stemmers. Only available if the Snowball classes are in the classpath.\nIf the class discovery is not dynamic, i.e., the property 'UseDynamic' in the props file 'weka/gui/GenericPropertiesCreator.props' is 'false', then the property 'org.tartarus.snowball.SnowballProgram' in the 'weka/gui/GenericObjectEditor.props' file has to be uncommented as well. If necessary you have to discover and fill in the snowball stemmers manually. You can use the 'weka.core.ClassDiscovery' for this:\n  java weka.core.ClassDiscovery org.tartarus.snowball.SnowballProgram org.tartarus.snowball.ext\n\nFor more information visit these web sites:\n  http://weka.wikispaces.com/Stemmers\n  http://snowball.tartarus.org/\n", snowballStemmer0.globalInfo());
      assertNotNull(snowballStemmer0);
      
      String string0 = snowballStemmer0.toString();
      assertEquals("weka.core.stemmers.SnowballStemmer", string0);
      assertFalse(snowballStemmer0.isPresent());
      assertEquals("A wrapper class for the Snowball stemmers. Only available if the Snowball classes are in the classpath.\nIf the class discovery is not dynamic, i.e., the property 'UseDynamic' in the props file 'weka/gui/GenericPropertiesCreator.props' is 'false', then the property 'org.tartarus.snowball.SnowballProgram' in the 'weka/gui/GenericObjectEditor.props' file has to be uncommented as well. If necessary you have to discover and fill in the snowball stemmers manually. You can use the 'weka.core.ClassDiscovery' for this:\n  java weka.core.ClassDiscovery org.tartarus.snowball.SnowballProgram org.tartarus.snowball.ext\n\nFor more information visit these web sites:\n  http://weka.wikispaces.com/Stemmers\n  http://snowball.tartarus.org/\n", snowballStemmer0.globalInfo());
      assertNotNull(string0);
      
      naiveBayesMultinomialText0.setPeriodicPruning(370);
      assertEquals(370, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      
      NaiveBayesMultinomialText naiveBayesMultinomialText1 = new NaiveBayesMultinomialText();
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText1.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText1.debugTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText1.minWordFrequencyTipText());
      assertEquals(3.0, naiveBayesMultinomialText1.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText1.LNormTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText1.useWordFrequenciesTipText());
      assertEquals(1.0, naiveBayesMultinomialText1.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText1.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText1.getDebug());
      assertEquals(0, naiveBayesMultinomialText1.getPeriodicPruning());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText1.useStopListTipText());
      assertFalse(naiveBayesMultinomialText1.getNormalizeDocLength());
      assertEquals(2.0, naiveBayesMultinomialText1.getLNorm(), 0.01);
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText1.periodicPruningTipText());
      assertFalse(naiveBayesMultinomialText1.getUseStopList());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText1.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText1.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText1.globalInfo());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText1.tokenizerTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText1.stopwordsTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText1.normTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText1.lowercaseTokensTipText());
      assertNotNull(naiveBayesMultinomialText1);
      assertFalse(naiveBayesMultinomialText1.equals((Object)naiveBayesMultinomialText0));
      
      naiveBayesMultinomialText1.pruneDictionary();
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText1.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText1.debugTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText1.minWordFrequencyTipText());
      assertEquals(3.0, naiveBayesMultinomialText1.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText1.LNormTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText1.useWordFrequenciesTipText());
      assertEquals(1.0, naiveBayesMultinomialText1.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText1.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText1.getDebug());
      assertEquals(0, naiveBayesMultinomialText1.getPeriodicPruning());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText1.useStopListTipText());
      assertFalse(naiveBayesMultinomialText1.getNormalizeDocLength());
      assertEquals(2.0, naiveBayesMultinomialText1.getLNorm(), 0.01);
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText1.periodicPruningTipText());
      assertFalse(naiveBayesMultinomialText1.getUseStopList());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText1.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText1.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText1.globalInfo());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText1.tokenizerTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText1.stopwordsTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText1.normTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText1.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText1.equals((Object)naiveBayesMultinomialText0));
      assertNotSame(naiveBayesMultinomialText1, naiveBayesMultinomialText0);
      
      NaiveBayesMultinomialText naiveBayesMultinomialText2 = new NaiveBayesMultinomialText();
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText2.LNormTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText2.minWordFrequencyTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText2.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText2.stemmerTipText());
      assertEquals(3.0, naiveBayesMultinomialText2.getMinWordFrequency(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText2.stopwordsTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText2.normTipText());
      assertFalse(naiveBayesMultinomialText2.getNormalizeDocLength());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText2.tokenizerTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText2.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText2.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText2.globalInfo());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText2.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText2.getDebug());
      assertFalse(naiveBayesMultinomialText2.getUseStopList());
      assertFalse(naiveBayesMultinomialText2.getLowercaseTokens());
      assertEquals(1.0, naiveBayesMultinomialText2.getNorm(), 0.01);
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText2.periodicPruningTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText2.useStopListTipText());
      assertEquals(2.0, naiveBayesMultinomialText2.getLNorm(), 0.01);
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText2.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText2.getPeriodicPruning());
      assertNotNull(naiveBayesMultinomialText2);
      assertFalse(naiveBayesMultinomialText2.equals((Object)naiveBayesMultinomialText1));
      assertFalse(naiveBayesMultinomialText2.equals((Object)naiveBayesMultinomialText0));
      
      naiveBayesMultinomialText1.setUseStopList(false);
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText1.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText1.debugTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText1.minWordFrequencyTipText());
      assertEquals(3.0, naiveBayesMultinomialText1.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText1.LNormTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText1.useWordFrequenciesTipText());
      assertEquals(1.0, naiveBayesMultinomialText1.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText1.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText1.getDebug());
      assertEquals(0, naiveBayesMultinomialText1.getPeriodicPruning());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText1.useStopListTipText());
      assertFalse(naiveBayesMultinomialText1.getNormalizeDocLength());
      assertEquals(2.0, naiveBayesMultinomialText1.getLNorm(), 0.01);
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText1.periodicPruningTipText());
      assertFalse(naiveBayesMultinomialText1.getUseStopList());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText1.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText1.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText1.globalInfo());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText1.tokenizerTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText1.stopwordsTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText1.normTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText1.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText1.equals((Object)naiveBayesMultinomialText0));
      assertFalse(naiveBayesMultinomialText1.equals((Object)naiveBayesMultinomialText2));
      assertNotSame(naiveBayesMultinomialText1, naiveBayesMultinomialText0);
      assertNotSame(naiveBayesMultinomialText1, naiveBayesMultinomialText2);
      
      NaiveBayesMultinomialText naiveBayesMultinomialText3 = new NaiveBayesMultinomialText();
      assertFalse(naiveBayesMultinomialText3.getUseStopList());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText3.periodicPruningTipText());
      assertEquals(1.0, naiveBayesMultinomialText3.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText3.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText3.globalInfo());
      assertEquals(0, naiveBayesMultinomialText3.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText3.useWordFrequenciesTipText());
      assertFalse(naiveBayesMultinomialText3.getDebug());
      assertEquals(2.0, naiveBayesMultinomialText3.getLNorm(), 0.01);
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText3.normTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText3.stopwordsTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText3.debugTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText3.LNormTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText3.minWordFrequencyTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText3.tokenizerTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText3.normalizeDocLengthTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText3.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText3.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText3.getLowercaseTokens());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText3.useStopListTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText3.stemmerTipText());
      assertEquals(3.0, naiveBayesMultinomialText3.getMinWordFrequency(), 0.01);
      assertNotNull(naiveBayesMultinomialText3);
      assertFalse(naiveBayesMultinomialText3.equals((Object)naiveBayesMultinomialText1));
      assertFalse(naiveBayesMultinomialText3.equals((Object)naiveBayesMultinomialText2));
      assertFalse(naiveBayesMultinomialText3.equals((Object)naiveBayesMultinomialText0));
      
      String[] stringArray0 = new String[3];
      stringArray0[0] = "org.tartarus.snowball";
      stringArray0[1] = "weka.core.stemmers.SnowballStemmer";
      stringArray0[2] = "org.tartarus.snowball.ext";
      naiveBayesMultinomialText1.setOptions(stringArray0);
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText1.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText1.debugTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText1.minWordFrequencyTipText());
      assertEquals(3.0, naiveBayesMultinomialText1.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText1.LNormTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText1.useWordFrequenciesTipText());
      assertEquals(1.0, naiveBayesMultinomialText1.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText1.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText1.getDebug());
      assertEquals(0, naiveBayesMultinomialText1.getPeriodicPruning());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText1.useStopListTipText());
      assertFalse(naiveBayesMultinomialText1.getNormalizeDocLength());
      assertEquals(2.0, naiveBayesMultinomialText1.getLNorm(), 0.01);
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText1.periodicPruningTipText());
      assertFalse(naiveBayesMultinomialText1.getUseStopList());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText1.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText1.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText1.globalInfo());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText1.tokenizerTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText1.stopwordsTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText1.normTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText1.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText1.equals((Object)naiveBayesMultinomialText0));
      assertFalse(naiveBayesMultinomialText1.equals((Object)naiveBayesMultinomialText2));
      assertFalse(naiveBayesMultinomialText1.equals((Object)naiveBayesMultinomialText3));
      assertNotSame(naiveBayesMultinomialText1, naiveBayesMultinomialText0);
      assertNotSame(naiveBayesMultinomialText1, naiveBayesMultinomialText2);
      assertNotSame(naiveBayesMultinomialText1, naiveBayesMultinomialText3);
      assertEquals(3, stringArray0.length);
      
      naiveBayesMultinomialText2.pruneDictionary();
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText2.LNormTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText2.minWordFrequencyTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText2.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText2.stemmerTipText());
      assertEquals(3.0, naiveBayesMultinomialText2.getMinWordFrequency(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText2.stopwordsTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText2.normTipText());
      assertFalse(naiveBayesMultinomialText2.getNormalizeDocLength());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText2.tokenizerTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText2.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText2.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText2.globalInfo());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText2.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText2.getDebug());
      assertFalse(naiveBayesMultinomialText2.getUseStopList());
      assertFalse(naiveBayesMultinomialText2.getLowercaseTokens());
      assertEquals(1.0, naiveBayesMultinomialText2.getNorm(), 0.01);
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText2.periodicPruningTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText2.useStopListTipText());
      assertEquals(2.0, naiveBayesMultinomialText2.getLNorm(), 0.01);
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText2.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText2.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText2.equals((Object)naiveBayesMultinomialText1));
      assertFalse(naiveBayesMultinomialText2.equals((Object)naiveBayesMultinomialText0));
      assertFalse(naiveBayesMultinomialText2.equals((Object)naiveBayesMultinomialText3));
      assertNotSame(naiveBayesMultinomialText2, naiveBayesMultinomialText1);
      assertNotSame(naiveBayesMultinomialText2, naiveBayesMultinomialText0);
      assertNotSame(naiveBayesMultinomialText2, naiveBayesMultinomialText3);
      
      String string1 = naiveBayesMultinomialText3.useStopListTipText();
      assertEquals("If true, ignores all words that are on the stoplist.", string1);
      assertFalse(naiveBayesMultinomialText3.getUseStopList());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText3.periodicPruningTipText());
      assertEquals(1.0, naiveBayesMultinomialText3.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText3.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText3.globalInfo());
      assertEquals(0, naiveBayesMultinomialText3.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText3.useWordFrequenciesTipText());
      assertFalse(naiveBayesMultinomialText3.getDebug());
      assertEquals(2.0, naiveBayesMultinomialText3.getLNorm(), 0.01);
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText3.normTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText3.stopwordsTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText3.debugTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText3.LNormTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText3.minWordFrequencyTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText3.tokenizerTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText3.normalizeDocLengthTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText3.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText3.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText3.getLowercaseTokens());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText3.useStopListTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText3.stemmerTipText());
      assertEquals(3.0, naiveBayesMultinomialText3.getMinWordFrequency(), 0.01);
      assertNotNull(string1);
      assertFalse(naiveBayesMultinomialText3.equals((Object)naiveBayesMultinomialText1));
      assertFalse(naiveBayesMultinomialText3.equals((Object)naiveBayesMultinomialText2));
      assertFalse(naiveBayesMultinomialText3.equals((Object)naiveBayesMultinomialText0));
      assertFalse(string1.equals((Object)string0));
      assertNotSame(naiveBayesMultinomialText3, naiveBayesMultinomialText1);
      assertNotSame(naiveBayesMultinomialText3, naiveBayesMultinomialText2);
      assertNotSame(naiveBayesMultinomialText3, naiveBayesMultinomialText0);
  }

  /**
  //Test case number: 21
  /*Coverage entropy=2.45221388535353
  */
  @Test(timeout = 4000)
  public void test21()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertNotNull(naiveBayesMultinomialText0);
      
      naiveBayesMultinomialText0.pruneDictionary();
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      
      naiveBayesMultinomialText0.m_useStopList = true;
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertTrue(naiveBayesMultinomialText0.getUseStopList());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      
      byte[] byteArray0 = new byte[4];
      byteArray0[0] = (byte) (-91);
      byteArray0[1] = (byte) (-104);
      boolean boolean0 = FileSystemHandling.appendDataToFile((EvoSuiteFile) null, byteArray0);
      assertFalse(boolean0);
      assertArrayEquals(new byte[] {(byte) (-91), (byte) (-104), (byte)0, (byte)0}, byteArray0);
      assertEquals(4, byteArray0.length);
      
      String[] stringArray0 = naiveBayesMultinomialText0.getOptions();
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertTrue(naiveBayesMultinomialText0.getUseStopList());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertNotNull(stringArray0);
      assertEquals(13, stringArray0.length);
      
      double[] doubleArray0 = new double[3];
      doubleArray0[0] = (double) (byte) (-104);
      doubleArray0[1] = (double) (byte) (-91);
      doubleArray0[2] = (double) (byte) (-91);
      int[] intArray0 = new int[3];
      intArray0[0] = (int) (byte) (-104);
      intArray0[1] = (-1);
      intArray0[2] = (int) (byte) (-104);
      SparseInstance sparseInstance0 = new SparseInstance((byte) (-104), doubleArray0, intArray0, (-952));
      assertEquals((-104.0), sparseInstance0.weight(), 0.01);
      assertEquals((-952), sparseInstance0.numAttributes());
      assertEquals(3, sparseInstance0.numValues());
      assertNotNull(sparseInstance0);
      assertArrayEquals(new double[] {(-104.0), (-91.0), (-91.0)}, doubleArray0, 0.01);
      assertArrayEquals(new int[] {(-104), (-1), (-104)}, intArray0);
      assertEquals(3, doubleArray0.length);
      assertEquals(3, intArray0.length);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      
      try { 
        naiveBayesMultinomialText0.distributionForInstance(sparseInstance0);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("weka.classifiers.bayes.NaiveBayesMultinomialText", e);
      }
  }

  /**
  //Test case number: 22
  /*Coverage entropy=2.488327743368588
  */
  @Test(timeout = 4000)
  public void test22()  throws Throwable  {
      byte[] byteArray0 = new byte[2];
      byteArray0[0] = (byte)86;
      byteArray0[1] = (byte) (-29);
      boolean boolean0 = FileSystemHandling.appendDataToFile((EvoSuiteFile) null, byteArray0);
      assertFalse(boolean0);
      assertArrayEquals(new byte[] {(byte)86, (byte) (-29)}, byteArray0);
      assertEquals(2, byteArray0.length);
      
      Random.setNextRandom((-1299));
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertNotNull(naiveBayesMultinomialText0);
      
      naiveBayesMultinomialText0.m_normalize = true;
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertTrue(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      
      naiveBayesMultinomialText0.m_lowercaseTokens = true;
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertTrue(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertTrue(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      
      String[] stringArray0 = naiveBayesMultinomialText0.getOptions();
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertTrue(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertTrue(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertNotNull(stringArray0);
      assertEquals(14, stringArray0.length);
  }

  /**
  //Test case number: 23
  /*Coverage entropy=3.198916528720684
  */
  @Test(timeout = 4000)
  public void test23()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertNotNull(naiveBayesMultinomialText0);
      
      FileSystemHandling fileSystemHandling0 = new FileSystemHandling();
      assertNotNull(fileSystemHandling0);
      
      NaiveBayesMultinomialText naiveBayesMultinomialText1 = new NaiveBayesMultinomialText();
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText1.normalizeDocLengthTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText1.tokenizerTipText());
      assertEquals(3.0, naiveBayesMultinomialText1.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText1.LNormTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText1.lowercaseTokensTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText1.useWordFrequenciesTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText1.useStopListTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText1.stemmerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText1.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText1.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText1.getLowercaseTokens());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText1.periodicPruningTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText1.stopwordsTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText1.normTipText());
      assertFalse(naiveBayesMultinomialText1.getDebug());
      assertFalse(naiveBayesMultinomialText1.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText1.globalInfo());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText1.debugTipText());
      assertEquals(0, naiveBayesMultinomialText1.getPeriodicPruning());
      assertEquals(1.0, naiveBayesMultinomialText1.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText1.getUseStopList());
      assertEquals(2.0, naiveBayesMultinomialText1.getLNorm(), 0.01);
      assertNotNull(naiveBayesMultinomialText1);
      assertFalse(naiveBayesMultinomialText1.equals((Object)naiveBayesMultinomialText0));
      
      FileSystemHandling fileSystemHandling1 = new FileSystemHandling();
      assertNotNull(fileSystemHandling1);
      assertFalse(fileSystemHandling1.equals((Object)fileSystemHandling0));
      
      String[] stringArray0 = naiveBayesMultinomialText0.getOptions();
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertNotNull(stringArray0);
      assertFalse(naiveBayesMultinomialText0.equals((Object)naiveBayesMultinomialText1));
      assertNotSame(naiveBayesMultinomialText0, naiveBayesMultinomialText1);
      assertEquals(12, stringArray0.length);
      
      naiveBayesMultinomialText1.setUseStopList(false);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText1.normalizeDocLengthTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText1.tokenizerTipText());
      assertEquals(3.0, naiveBayesMultinomialText1.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText1.LNormTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText1.lowercaseTokensTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText1.useWordFrequenciesTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText1.useStopListTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText1.stemmerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText1.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText1.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText1.getLowercaseTokens());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText1.periodicPruningTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText1.stopwordsTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText1.normTipText());
      assertFalse(naiveBayesMultinomialText1.getDebug());
      assertFalse(naiveBayesMultinomialText1.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText1.globalInfo());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText1.debugTipText());
      assertEquals(0, naiveBayesMultinomialText1.getPeriodicPruning());
      assertEquals(1.0, naiveBayesMultinomialText1.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText1.getUseStopList());
      assertEquals(2.0, naiveBayesMultinomialText1.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText1.equals((Object)naiveBayesMultinomialText0));
      assertNotSame(naiveBayesMultinomialText1, naiveBayesMultinomialText0);
      
      NaiveBayesMultinomialText naiveBayesMultinomialText2 = new NaiveBayesMultinomialText();
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText2.normTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText2.stopwordsTipText());
      assertFalse(naiveBayesMultinomialText2.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText2.globalInfo());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText2.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText2.getDebug());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText2.minWordFrequencyTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText2.LNormTipText());
      assertFalse(naiveBayesMultinomialText2.getLowercaseTokens());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText2.useStopListTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText2.stemmerTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText2.tokenizerTipText());
      assertEquals(3.0, naiveBayesMultinomialText2.getMinWordFrequency(), 0.01);
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText2.periodicPruningTipText());
      assertFalse(naiveBayesMultinomialText2.getNormalizeDocLength());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText2.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText2.getPeriodicPruning());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText2.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText2.getUseStopList());
      assertEquals(1.0, naiveBayesMultinomialText2.getNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText2.debugTipText());
      assertEquals(2.0, naiveBayesMultinomialText2.getLNorm(), 0.01);
      assertNotNull(naiveBayesMultinomialText2);
      assertFalse(naiveBayesMultinomialText2.equals((Object)naiveBayesMultinomialText0));
      assertFalse(naiveBayesMultinomialText2.equals((Object)naiveBayesMultinomialText1));
      
      naiveBayesMultinomialText2.setOptions(stringArray0);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText2.normTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText2.stopwordsTipText());
      assertFalse(naiveBayesMultinomialText2.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText2.globalInfo());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText2.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText2.getDebug());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText2.minWordFrequencyTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText2.LNormTipText());
      assertFalse(naiveBayesMultinomialText2.getLowercaseTokens());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText2.useStopListTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText2.stemmerTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText2.tokenizerTipText());
      assertEquals(3.0, naiveBayesMultinomialText2.getMinWordFrequency(), 0.01);
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText2.periodicPruningTipText());
      assertFalse(naiveBayesMultinomialText2.getNormalizeDocLength());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText2.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText2.getPeriodicPruning());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText2.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText2.getUseStopList());
      assertEquals(1.0, naiveBayesMultinomialText2.getNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText2.debugTipText());
      assertEquals(2.0, naiveBayesMultinomialText2.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.equals((Object)naiveBayesMultinomialText1));
      assertFalse(naiveBayesMultinomialText0.equals((Object)naiveBayesMultinomialText2));
      assertFalse(naiveBayesMultinomialText2.equals((Object)naiveBayesMultinomialText0));
      assertFalse(naiveBayesMultinomialText2.equals((Object)naiveBayesMultinomialText1));
      assertNotSame(naiveBayesMultinomialText0, naiveBayesMultinomialText1);
      assertNotSame(naiveBayesMultinomialText0, naiveBayesMultinomialText2);
      assertNotSame(naiveBayesMultinomialText2, naiveBayesMultinomialText0);
      assertNotSame(naiveBayesMultinomialText2, naiveBayesMultinomialText1);
      assertEquals(12, stringArray0.length);
      
      naiveBayesMultinomialText2.setOptions(stringArray0);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText2.normTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText2.stopwordsTipText());
      assertFalse(naiveBayesMultinomialText2.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText2.globalInfo());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText2.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText2.getDebug());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText2.minWordFrequencyTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText2.LNormTipText());
      assertFalse(naiveBayesMultinomialText2.getLowercaseTokens());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText2.useStopListTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText2.stemmerTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText2.tokenizerTipText());
      assertEquals(3.0, naiveBayesMultinomialText2.getMinWordFrequency(), 0.01);
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText2.periodicPruningTipText());
      assertFalse(naiveBayesMultinomialText2.getNormalizeDocLength());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText2.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText2.getPeriodicPruning());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText2.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText2.getUseStopList());
      assertEquals(1.0, naiveBayesMultinomialText2.getNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText2.debugTipText());
      assertEquals(2.0, naiveBayesMultinomialText2.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.equals((Object)naiveBayesMultinomialText1));
      assertFalse(naiveBayesMultinomialText0.equals((Object)naiveBayesMultinomialText2));
      assertFalse(naiveBayesMultinomialText2.equals((Object)naiveBayesMultinomialText0));
      assertFalse(naiveBayesMultinomialText2.equals((Object)naiveBayesMultinomialText1));
      assertNotSame(naiveBayesMultinomialText0, naiveBayesMultinomialText1);
      assertNotSame(naiveBayesMultinomialText0, naiveBayesMultinomialText2);
      assertNotSame(naiveBayesMultinomialText2, naiveBayesMultinomialText0);
      assertNotSame(naiveBayesMultinomialText2, naiveBayesMultinomialText1);
      assertEquals(12, stringArray0.length);
  }

  /**
  //Test case number: 24
  /*Coverage entropy=2.83154442690767
  */
  @Test(timeout = 4000)
  public void test24()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertNotNull(naiveBayesMultinomialText0);
      
      naiveBayesMultinomialText0.pruneDictionary();
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      
      naiveBayesMultinomialText0.m_useStopList = true;
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertTrue(naiveBayesMultinomialText0.getUseStopList());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      
      byte[] byteArray0 = new byte[4];
      byteArray0[0] = (byte) (-91);
      boolean boolean0 = FileSystemHandling.appendDataToFile((EvoSuiteFile) null, byteArray0);
      assertFalse(boolean0);
      assertArrayEquals(new byte[] {(byte) (-91), (byte)0, (byte)0, (byte)0}, byteArray0);
      assertEquals(4, byteArray0.length);
      
      NaiveBayesMultinomialText naiveBayesMultinomialText1 = new NaiveBayesMultinomialText();
      assertFalse(naiveBayesMultinomialText1.getUseStopList());
      assertFalse(naiveBayesMultinomialText1.getDebug());
      assertEquals(1.0, naiveBayesMultinomialText1.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText1.getLowercaseTokens());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText1.useStopListTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText1.periodicPruningTipText());
      assertEquals(2.0, naiveBayesMultinomialText1.getLNorm(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText1.stopwordsTipText());
      assertFalse(naiveBayesMultinomialText1.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText1.getUseWordFrequencies());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText1.normTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText1.globalInfo());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText1.normalizeDocLengthTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText1.lowercaseTokensTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText1.minWordFrequencyTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText1.LNormTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText1.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText1.debugTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText1.tokenizerTipText());
      assertEquals(3.0, naiveBayesMultinomialText1.getMinWordFrequency(), 0.01);
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText1.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText1.getPeriodicPruning());
      assertNotNull(naiveBayesMultinomialText1);
      assertFalse(naiveBayesMultinomialText1.equals((Object)naiveBayesMultinomialText0));
      
      NaiveBayesMultinomialText naiveBayesMultinomialText2 = new NaiveBayesMultinomialText();
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText2.normalizeDocLengthTipText());
      assertEquals(0, naiveBayesMultinomialText2.getPeriodicPruning());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText2.tokenizerTipText());
      assertFalse(naiveBayesMultinomialText2.getNormalizeDocLength());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText2.minWordFrequencyTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText2.useWordFrequenciesTipText());
      assertEquals(3.0, naiveBayesMultinomialText2.getMinWordFrequency(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText2.useStopListTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText2.LNormTipText());
      assertFalse(naiveBayesMultinomialText2.getLowercaseTokens());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText2.stemmerTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText2.stopwordsTipText());
      assertFalse(naiveBayesMultinomialText2.getUseWordFrequencies());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText2.normTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText2.globalInfo());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText2.periodicPruningTipText());
      assertEquals(1.0, naiveBayesMultinomialText2.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText2.getUseStopList());
      assertFalse(naiveBayesMultinomialText2.getDebug());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText2.lowercaseTokensTipText());
      assertEquals(2.0, naiveBayesMultinomialText2.getLNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText2.debugTipText());
      assertNotNull(naiveBayesMultinomialText2);
      assertFalse(naiveBayesMultinomialText2.equals((Object)naiveBayesMultinomialText0));
      assertFalse(naiveBayesMultinomialText2.equals((Object)naiveBayesMultinomialText1));
      
      TestInstances testInstances0 = new TestInstances();
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(20, testInstances0.getNumInstances());
      assertFalse(testInstances0.getNoClass());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertNotNull(testInstances0);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      
      Instances instances0 = testInstances0.generate();
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(20, testInstances0.getNumInstances());
      assertFalse(testInstances0.getNoClass());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(1, instances0.classIndex());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(20, instances0.size());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20, instances0.numInstances());
      assertEquals(2, instances0.numClasses());
      assertEquals(2, instances0.numAttributes());
      assertNotNull(instances0);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      
      naiveBayesMultinomialText0.buildClassifier(instances0);
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertTrue(naiveBayesMultinomialText0.getUseStopList());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(20, testInstances0.getNumInstances());
      assertFalse(testInstances0.getNoClass());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(1, instances0.classIndex());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(20, instances0.size());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20, instances0.numInstances());
      assertEquals(2, instances0.numClasses());
      assertEquals(2, instances0.numAttributes());
      assertFalse(naiveBayesMultinomialText0.equals((Object)naiveBayesMultinomialText1));
      assertFalse(naiveBayesMultinomialText0.equals((Object)naiveBayesMultinomialText2));
      assertNotSame(naiveBayesMultinomialText0, naiveBayesMultinomialText1);
      assertNotSame(naiveBayesMultinomialText0, naiveBayesMultinomialText2);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      
      String[] stringArray0 = naiveBayesMultinomialText0.getOptions();
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertTrue(naiveBayesMultinomialText0.getUseStopList());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertNotNull(stringArray0);
      assertFalse(naiveBayesMultinomialText0.equals((Object)naiveBayesMultinomialText1));
      assertFalse(naiveBayesMultinomialText0.equals((Object)naiveBayesMultinomialText2));
      assertNotSame(naiveBayesMultinomialText0, naiveBayesMultinomialText1);
      assertNotSame(naiveBayesMultinomialText0, naiveBayesMultinomialText2);
      assertEquals(13, stringArray0.length);
  }

  /**
  //Test case number: 25
  /*Coverage entropy=2.6872198947505592
  */
  @Test(timeout = 4000)
  public void test25()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertNotNull(naiveBayesMultinomialText0);
      
      naiveBayesMultinomialText0.setPeriodicPruning(2440);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals(2440, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      
      NaiveBayesMultinomialText naiveBayesMultinomialText1 = new NaiveBayesMultinomialText();
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText1.useWordFrequenciesTipText());
      assertEquals(2.0, naiveBayesMultinomialText1.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText1.getUseStopList());
      assertFalse(naiveBayesMultinomialText1.getDebug());
      assertFalse(naiveBayesMultinomialText1.getNormalizeDocLength());
      assertEquals(0, naiveBayesMultinomialText1.getPeriodicPruning());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText1.normalizeDocLengthTipText());
      assertEquals(1.0, naiveBayesMultinomialText1.getNorm(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText1.stopwordsTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText1.debugTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText1.normTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText1.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText1.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText1.globalInfo());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText1.lowercaseTokensTipText());
      assertEquals(3.0, naiveBayesMultinomialText1.getMinWordFrequency(), 0.01);
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText1.periodicPruningTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText1.LNormTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText1.tokenizerTipText());
      assertFalse(naiveBayesMultinomialText1.getLowercaseTokens());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText1.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText1.useStopListTipText());
      assertNotNull(naiveBayesMultinomialText1);
      assertFalse(naiveBayesMultinomialText1.equals((Object)naiveBayesMultinomialText0));
      
      TestInstances testInstances0 = new TestInstances();
      assertEquals(0, testInstances0.getNumString());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getNoClass());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(1, testInstances0.getClassType());
      assertNotNull(testInstances0);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      
      Instances instances0 = testInstances0.generate();
      assertEquals(0, testInstances0.getNumString());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getNoClass());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(1, testInstances0.getClassType());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(2, instances0.numClasses());
      assertEquals(20, instances0.size());
      assertEquals(1, instances0.classIndex());
      assertEquals(2, instances0.numAttributes());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals("Testdata", instances0.relationName());
      assertEquals(20, instances0.numInstances());
      assertNotNull(instances0);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      
      naiveBayesMultinomialText0.buildClassifier(instances0);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals(2440, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getNoClass());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(1, testInstances0.getClassType());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(2, instances0.numClasses());
      assertEquals(20, instances0.size());
      assertEquals(1, instances0.classIndex());
      assertEquals(2, instances0.numAttributes());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals("Testdata", instances0.relationName());
      assertEquals(20, instances0.numInstances());
      assertFalse(naiveBayesMultinomialText0.equals((Object)naiveBayesMultinomialText1));
      assertNotSame(naiveBayesMultinomialText0, naiveBayesMultinomialText1);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      
      String[] stringArray0 = naiveBayesMultinomialText1.getOptions();
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText1.useWordFrequenciesTipText());
      assertEquals(2.0, naiveBayesMultinomialText1.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText1.getUseStopList());
      assertFalse(naiveBayesMultinomialText1.getDebug());
      assertFalse(naiveBayesMultinomialText1.getNormalizeDocLength());
      assertEquals(0, naiveBayesMultinomialText1.getPeriodicPruning());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText1.normalizeDocLengthTipText());
      assertEquals(1.0, naiveBayesMultinomialText1.getNorm(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText1.stopwordsTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText1.debugTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText1.normTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText1.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText1.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText1.globalInfo());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText1.lowercaseTokensTipText());
      assertEquals(3.0, naiveBayesMultinomialText1.getMinWordFrequency(), 0.01);
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText1.periodicPruningTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText1.LNormTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText1.tokenizerTipText());
      assertFalse(naiveBayesMultinomialText1.getLowercaseTokens());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText1.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText1.useStopListTipText());
      assertNotNull(stringArray0);
      assertFalse(naiveBayesMultinomialText1.equals((Object)naiveBayesMultinomialText0));
      assertNotSame(naiveBayesMultinomialText1, naiveBayesMultinomialText0);
      assertEquals(12, stringArray0.length);
  }

  /**
  //Test case number: 26
  /*Coverage entropy=2.994490307954244
  */
  @Test(timeout = 4000)
  public void test26()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertNotNull(naiveBayesMultinomialText0);
      
      FileSystemHandling fileSystemHandling0 = new FileSystemHandling();
      assertNotNull(fileSystemHandling0);
      
      naiveBayesMultinomialText0.setUseWordFrequencies(true);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertTrue(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      
      String[] stringArray0 = naiveBayesMultinomialText0.getOptions();
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertTrue(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertNotNull(stringArray0);
      assertEquals(13, stringArray0.length);
      
      naiveBayesMultinomialText0.pruneDictionary();
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertTrue(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      
      naiveBayesMultinomialText0.setOptions(stringArray0);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertTrue(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals(13, stringArray0.length);
      
      String[] stringArray1 = Locale.getISOLanguages();
      assertNotNull(stringArray1);
      assertFalse(stringArray1.equals((Object)stringArray0));
      assertNotSame(stringArray1, stringArray0);
      assertEquals(188, stringArray1.length);
      
      int[] intArray0 = new int[3];
      intArray0[0] = 80;
      intArray0[1] = (-4569);
      intArray0[2] = 554;
      BinarySparseInstance binarySparseInstance0 = new BinarySparseInstance(80, intArray0, 35);
      assertEquals(3, binarySparseInstance0.numValues());
      assertEquals(80.0, binarySparseInstance0.weight(), 0.01);
      assertEquals(35, binarySparseInstance0.numAttributes());
      assertNotNull(binarySparseInstance0);
      assertArrayEquals(new int[] {80, (-4569), 554}, intArray0);
      assertEquals(3, intArray0.length);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      
      NaiveBayesMultinomialText naiveBayesMultinomialText1 = new NaiveBayesMultinomialText();
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText1.LNormTipText());
      assertEquals(3.0, naiveBayesMultinomialText1.getMinWordFrequency(), 0.01);
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText1.lowercaseTokensTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText1.minWordFrequencyTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText1.tokenizerTipText());
      assertFalse(naiveBayesMultinomialText1.getNormalizeDocLength());
      assertEquals(2.0, naiveBayesMultinomialText1.getLNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText1.debugTipText());
      assertFalse(naiveBayesMultinomialText1.getUseWordFrequencies());
      assertEquals(1.0, naiveBayesMultinomialText1.getNorm(), 0.01);
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText1.globalInfo());
      assertFalse(naiveBayesMultinomialText1.getDebug());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText1.normTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText1.normalizeDocLengthTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText1.periodicPruningTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText1.stopwordsTipText());
      assertFalse(naiveBayesMultinomialText1.getUseStopList());
      assertEquals(0, naiveBayesMultinomialText1.getPeriodicPruning());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText1.useStopListTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText1.stemmerTipText());
      assertFalse(naiveBayesMultinomialText1.getLowercaseTokens());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText1.useWordFrequenciesTipText());
      assertNotNull(naiveBayesMultinomialText1);
      assertFalse(naiveBayesMultinomialText1.equals((Object)naiveBayesMultinomialText0));
      
      try { 
        naiveBayesMultinomialText1.distributionForInstance(binarySparseInstance0);
        fail("Expecting exception: RuntimeException");
      
      } catch(RuntimeException e) {
         //
         // DenseInstance doesn't have access to a dataset!
         //
         verifyException("weka.core.AbstractInstance", e);
      }
  }

  /**
  //Test case number: 27
  /*Coverage entropy=3.0061757447926345
  */
  @Test(timeout = 4000)
  public void test27()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertNotNull(naiveBayesMultinomialText0);
      
      FileSystemHandling fileSystemHandling0 = new FileSystemHandling();
      assertNotNull(fileSystemHandling0);
      
      String[] stringArray0 = naiveBayesMultinomialText0.getOptions();
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertNotNull(stringArray0);
      assertEquals(12, stringArray0.length);
      
      naiveBayesMultinomialText0.pruneDictionary();
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      
      naiveBayesMultinomialText0.setOptions(stringArray0);
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(12, stringArray0.length);
      
      String[] stringArray1 = Locale.getISOCountries();
      assertNotNull(stringArray1);
      assertFalse(stringArray1.equals((Object)stringArray0));
      assertNotSame(stringArray1, stringArray0);
      assertEquals(250, stringArray1.length);
      
      IBk iBk0 = new IBk((-500));
      assertFalse(iBk0.getMeanSquared());
      assertEquals("The number of neighbours to use.", iBk0.KNNTipText());
      assertEquals("Gets the maximum number of instances allowed in the training pool. The addition of new instances above this value will result in old instances being removed. A value of 0 signifies no limit to the number of training instances.", iBk0.windowSizeTipText());
      assertEquals(0, iBk0.getWindowSize());
      assertEquals("The nearest neighbour search algorithm to use (Default: weka.core.neighboursearch.LinearNNSearch).", iBk0.nearestNeighbourSearchAlgorithmTipText());
      assertFalse(iBk0.getDebug());
      assertEquals("Whether hold-one-out cross-validation will be used to select the best k value.", iBk0.crossValidateTipText());
      assertEquals("Whether the mean squared error is used rather than mean absolute error when doing cross-validation for regression problems.", iBk0.meanSquaredTipText());
      assertEquals("Gets the distance weighting method used.", iBk0.distanceWeightingTipText());
      assertFalse(iBk0.getCrossValidate());
      assertEquals((-500), iBk0.getKNN());
      assertEquals("If set to true, classifier may output additional info to the console.", iBk0.debugTipText());
      assertNotNull(iBk0);
      assertEquals(4, IBk.WEIGHT_SIMILARITY);
      assertEquals(1, IBk.WEIGHT_NONE);
      assertEquals(2, IBk.WEIGHT_INVERSE);
      
      TestInstances testInstances0 = new TestInstances();
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(1, testInstances0.getSeed());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(20, testInstances0.getNumInstances());
      assertFalse(testInstances0.getNoClass());
      assertEquals(1, testInstances0.getClassType());
      assertEquals((-1), testInstances0.getClassIndex());
      assertNotNull(testInstances0);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      
      Instances instances0 = testInstances0.generate("      throw new NullPointerException(\"No input instance format defined\");;\n");
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(1, testInstances0.getSeed());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(20, testInstances0.getNumInstances());
      assertFalse(testInstances0.getNoClass());
      assertEquals(1, testInstances0.getClassType());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(2, instances0.numClasses());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(1, instances0.classIndex());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(20, instances0.numInstances());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(20, instances0.size());
      assertEquals(2, instances0.numAttributes());
      assertNotNull(instances0);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      
      double[] doubleArray0 = new double[4];
      doubleArray0[0] = (double) (-500);
      doubleArray0[1] = (double) (-500);
      doubleArray0[2] = (double) (-1);
      doubleArray0[3] = 11.0;
      Instances instances1 = iBk0.pruneToK(instances0, doubleArray0, (-1));
      assertFalse(iBk0.getMeanSquared());
      assertEquals("The number of neighbours to use.", iBk0.KNNTipText());
      assertEquals("Gets the maximum number of instances allowed in the training pool. The addition of new instances above this value will result in old instances being removed. A value of 0 signifies no limit to the number of training instances.", iBk0.windowSizeTipText());
      assertEquals(0, iBk0.getWindowSize());
      assertEquals("The nearest neighbour search algorithm to use (Default: weka.core.neighboursearch.LinearNNSearch).", iBk0.nearestNeighbourSearchAlgorithmTipText());
      assertFalse(iBk0.getDebug());
      assertEquals("Whether hold-one-out cross-validation will be used to select the best k value.", iBk0.crossValidateTipText());
      assertEquals("Whether the mean squared error is used rather than mean absolute error when doing cross-validation for regression problems.", iBk0.meanSquaredTipText());
      assertEquals("Gets the distance weighting method used.", iBk0.distanceWeightingTipText());
      assertFalse(iBk0.getCrossValidate());
      assertEquals((-500), iBk0.getKNN());
      assertEquals("If set to true, classifier may output additional info to the console.", iBk0.debugTipText());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(1, testInstances0.getSeed());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(20, testInstances0.getNumInstances());
      assertFalse(testInstances0.getNoClass());
      assertEquals(1, testInstances0.getClassType());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(2, instances0.numClasses());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(1, instances0.classIndex());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(20, instances0.numInstances());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(20, instances0.size());
      assertEquals(2, instances0.numAttributes());
      assertEquals(2, instances1.numClasses());
      assertFalse(instances1.checkForStringAttributes());
      assertEquals(2, instances1.numAttributes());
      assertEquals(2, instances1.size());
      assertEquals(2.0, instances1.sumOfWeights(), 0.01);
      assertEquals(2, instances1.numInstances());
      assertEquals("Testdata", instances1.relationName());
      assertEquals(1, instances1.classIndex());
      assertNotNull(instances1);
      assertArrayEquals(new double[] {(-500.0), (-500.0), (-1.0), 11.0}, doubleArray0, 0.01);
      assertFalse(instances1.equals((Object)instances0));
      assertNotSame(instances0, instances1);
      assertNotSame(instances1, instances0);
      assertEquals(4, doubleArray0.length);
      assertEquals(4, IBk.WEIGHT_SIMILARITY);
      assertEquals(1, IBk.WEIGHT_NONE);
      assertEquals(2, IBk.WEIGHT_INVERSE);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      
      naiveBayesMultinomialText0.buildClassifier(instances1);
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertFalse(iBk0.getMeanSquared());
      assertEquals("The number of neighbours to use.", iBk0.KNNTipText());
      assertEquals("Gets the maximum number of instances allowed in the training pool. The addition of new instances above this value will result in old instances being removed. A value of 0 signifies no limit to the number of training instances.", iBk0.windowSizeTipText());
      assertEquals(0, iBk0.getWindowSize());
      assertEquals("The nearest neighbour search algorithm to use (Default: weka.core.neighboursearch.LinearNNSearch).", iBk0.nearestNeighbourSearchAlgorithmTipText());
      assertFalse(iBk0.getDebug());
      assertEquals("Whether hold-one-out cross-validation will be used to select the best k value.", iBk0.crossValidateTipText());
      assertEquals("Whether the mean squared error is used rather than mean absolute error when doing cross-validation for regression problems.", iBk0.meanSquaredTipText());
      assertEquals("Gets the distance weighting method used.", iBk0.distanceWeightingTipText());
      assertFalse(iBk0.getCrossValidate());
      assertEquals((-500), iBk0.getKNN());
      assertEquals("If set to true, classifier may output additional info to the console.", iBk0.debugTipText());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(1, testInstances0.getSeed());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(20, testInstances0.getNumInstances());
      assertFalse(testInstances0.getNoClass());
      assertEquals(1, testInstances0.getClassType());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(2, instances0.numClasses());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(1, instances0.classIndex());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(20, instances0.numInstances());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(20, instances0.size());
      assertEquals(2, instances0.numAttributes());
      assertEquals(2, instances1.numClasses());
      assertFalse(instances1.checkForStringAttributes());
      assertEquals(2, instances1.numAttributes());
      assertEquals(2, instances1.size());
      assertEquals(2.0, instances1.sumOfWeights(), 0.01);
      assertEquals(2, instances1.numInstances());
      assertEquals("Testdata", instances1.relationName());
      assertEquals(1, instances1.classIndex());
      assertArrayEquals(new double[] {(-500.0), (-500.0), (-1.0), 11.0}, doubleArray0, 0.01);
      assertFalse(instances0.equals((Object)instances1));
      assertFalse(instances1.equals((Object)instances0));
      assertNotSame(instances0, instances1);
      assertNotSame(instances1, instances0);
      assertEquals(4, doubleArray0.length);
      assertEquals(4, IBk.WEIGHT_SIMILARITY);
      assertEquals(1, IBk.WEIGHT_NONE);
      assertEquals(2, IBk.WEIGHT_INVERSE);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      
      String string0 = naiveBayesMultinomialText0.toString();
      assertEquals("The independent probability of a class\n--------------------------------------\nclass1\t2.0\nclass2\t2.0\n\nThe probability of a word given the class\n-----------------------------------------\n\tclass1\tclass2\t\n", string0);
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertNotNull(string0);
  }

  /**
  //Test case number: 28
  /*Coverage entropy=2.1972245773362196
  */
  @Test(timeout = 4000)
  public void test28()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertNotNull(naiveBayesMultinomialText0);
      
      boolean boolean0 = FileSystemHandling.shouldAllThrowIOExceptions();
      assertTrue(boolean0);
      
      TestInstances testInstances0 = new TestInstances();
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(20, testInstances0.getNumInstances());
      assertFalse(testInstances0.getNoClass());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(1, testInstances0.getSeed());
      assertNotNull(testInstances0);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      
      Instances instances0 = testInstances0.generate();
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(20, testInstances0.getNumInstances());
      assertFalse(testInstances0.getNoClass());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(2, instances0.numAttributes());
      assertEquals(1, instances0.classIndex());
      assertEquals(20, instances0.numInstances());
      assertEquals("Testdata", instances0.relationName());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20, instances0.size());
      assertEquals(2, instances0.numClasses());
      assertNotNull(instances0);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      
      NaiveBayesMultinomialText naiveBayesMultinomialText1 = new NaiveBayesMultinomialText();
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText1.periodicPruningTipText());
      assertFalse(naiveBayesMultinomialText1.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText1.globalInfo());
      assertFalse(naiveBayesMultinomialText1.getUseStopList());
      assertFalse(naiveBayesMultinomialText1.getDebug());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText1.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText1.getPeriodicPruning());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText1.LNormTipText());
      assertFalse(naiveBayesMultinomialText1.getLowercaseTokens());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText1.useStopListTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText1.stemmerTipText());
      assertEquals(3.0, naiveBayesMultinomialText1.getMinWordFrequency(), 0.01);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText1.tokenizerTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText1.normalizeDocLengthTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText1.lowercaseTokensTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText1.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText1.getNormalizeDocLength());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText1.debugTipText());
      assertEquals(2.0, naiveBayesMultinomialText1.getLNorm(), 0.01);
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText1.normTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText1.stopwordsTipText());
      assertEquals(1.0, naiveBayesMultinomialText1.getNorm(), 0.01);
      assertNotNull(naiveBayesMultinomialText1);
      assertFalse(naiveBayesMultinomialText1.equals((Object)naiveBayesMultinomialText0));
      
      naiveBayesMultinomialText0.buildClassifier(instances0);
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(20, testInstances0.getNumInstances());
      assertFalse(testInstances0.getNoClass());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(2, instances0.numAttributes());
      assertEquals(1, instances0.classIndex());
      assertEquals(20, instances0.numInstances());
      assertEquals("Testdata", instances0.relationName());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20, instances0.size());
      assertEquals(2, instances0.numClasses());
      assertFalse(naiveBayesMultinomialText0.equals((Object)naiveBayesMultinomialText1));
      assertNotSame(naiveBayesMultinomialText0, naiveBayesMultinomialText1);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      
      LinkedHashMap<String, NaiveBayesMultinomialText.Count> linkedHashMap0 = new LinkedHashMap<String, NaiveBayesMultinomialText.Count>();
      assertTrue(linkedHashMap0.isEmpty());
      assertEquals(0, linkedHashMap0.size());
      assertNotNull(linkedHashMap0);
      
      LinkedHashMap<Integer, LinkedHashMap<String, NaiveBayesMultinomialText.Count>> linkedHashMap1 = new LinkedHashMap<Integer, LinkedHashMap<String, NaiveBayesMultinomialText.Count>>();
      assertEquals(0, linkedHashMap1.size());
      assertTrue(linkedHashMap1.isEmpty());
      assertNotNull(linkedHashMap1);
      
      NaiveBayesMultinomialText naiveBayesMultinomialText2 = new NaiveBayesMultinomialText();
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText2.stopwordsTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText2.periodicPruningTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText2.debugTipText());
      assertFalse(naiveBayesMultinomialText2.getDebug());
      assertEquals(1.0, naiveBayesMultinomialText2.getNorm(), 0.01);
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText2.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText2.getUseWordFrequencies());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText2.normTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText2.globalInfo());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText2.normalizeDocLengthTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText2.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText2.getNormalizeDocLength());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText2.tokenizerTipText());
      assertEquals(3.0, naiveBayesMultinomialText2.getMinWordFrequency(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText2.useStopListTipText());
      assertEquals(2.0, naiveBayesMultinomialText2.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText2.getUseStopList());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText2.LNormTipText());
      assertFalse(naiveBayesMultinomialText2.getLowercaseTokens());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText2.stemmerTipText());
      assertEquals(0, naiveBayesMultinomialText2.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText2.useWordFrequenciesTipText());
      assertNotNull(naiveBayesMultinomialText2);
      assertFalse(naiveBayesMultinomialText2.equals((Object)naiveBayesMultinomialText0));
      assertFalse(naiveBayesMultinomialText2.equals((Object)naiveBayesMultinomialText1));
      
      int[] intArray0 = new int[3];
      intArray0[0] = (-1);
      intArray0[1] = (-1);
      intArray0[2] = (-1);
      BinarySparseInstance binarySparseInstance0 = new BinarySparseInstance((-1), intArray0, (-2));
      assertEquals(3, binarySparseInstance0.numValues());
      assertEquals((-1.0), binarySparseInstance0.weight(), 0.01);
      assertEquals((-2), binarySparseInstance0.numAttributes());
      assertNotNull(binarySparseInstance0);
      assertArrayEquals(new int[] {(-1), (-1), (-1)}, intArray0);
      assertEquals(3, intArray0.length);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      
      BinarySparseInstance binarySparseInstance1 = (BinarySparseInstance)binarySparseInstance0.copy();
      assertEquals(3, binarySparseInstance0.numValues());
      assertEquals((-1.0), binarySparseInstance0.weight(), 0.01);
      assertEquals((-2), binarySparseInstance0.numAttributes());
      assertEquals((-1.0), binarySparseInstance1.weight(), 0.01);
      assertEquals((-2), binarySparseInstance1.numAttributes());
      assertEquals(3, binarySparseInstance1.numValues());
      assertNotNull(binarySparseInstance1);
      assertArrayEquals(new int[] {(-1), (-1), (-1)}, intArray0);
      assertFalse(binarySparseInstance1.equals((Object)binarySparseInstance0));
      assertNotSame(binarySparseInstance0, binarySparseInstance1);
      assertNotSame(binarySparseInstance1, binarySparseInstance0);
      assertEquals(3, intArray0.length);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      
      double[] doubleArray0 = naiveBayesMultinomialText0.distributionForInstance(binarySparseInstance0);
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(3, binarySparseInstance0.numValues());
      assertEquals((-1.0), binarySparseInstance0.weight(), 0.01);
      assertEquals((-2), binarySparseInstance0.numAttributes());
      assertNotNull(doubleArray0);
      assertArrayEquals(new int[] {(-1), (-1), (-1)}, intArray0);
      assertArrayEquals(new double[] {0.5454545454545454, 0.4545454545454546}, doubleArray0, 0.01);
      assertFalse(naiveBayesMultinomialText0.equals((Object)naiveBayesMultinomialText1));
      assertFalse(naiveBayesMultinomialText0.equals((Object)naiveBayesMultinomialText2));
      assertFalse(binarySparseInstance0.equals((Object)binarySparseInstance1));
      assertNotSame(naiveBayesMultinomialText0, naiveBayesMultinomialText1);
      assertNotSame(naiveBayesMultinomialText0, naiveBayesMultinomialText2);
      assertNotSame(binarySparseInstance0, binarySparseInstance1);
      assertEquals(3, intArray0.length);
      assertEquals(2, doubleArray0.length);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
  }
}
