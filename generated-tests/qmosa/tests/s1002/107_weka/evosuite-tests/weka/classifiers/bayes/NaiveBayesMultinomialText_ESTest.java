/*
 * This file was automatically generated by EvoSuite
 * Fri Aug 24 08:48:20 GMT 2018
 */

package weka.classifiers.bayes;

import org.junit.Test;
import static org.junit.Assert.*;
import static org.evosuite.runtime.EvoAssertions.*;
import java.io.File;
import java.net.URI;
import java.util.Enumeration;
import java.util.LinkedHashMap;
import java.util.Map;
import org.evosuite.runtime.EvoRunner;
import org.evosuite.runtime.EvoRunnerParameters;
import org.evosuite.runtime.Random;
import org.evosuite.runtime.mock.java.io.MockFile;
import org.evosuite.runtime.mock.java.net.MockURI;
import org.evosuite.runtime.testdata.FileSystemHandling;
import org.junit.runner.RunWith;
import weka.classifiers.bayes.NaiveBayesMultinomialText;
import weka.classifiers.functions.SGDText;
import weka.classifiers.misc.SerializedClassifier;
import weka.core.AbstractInstance;
import weka.core.Capabilities;
import weka.core.Instance;
import weka.core.Instances;
import weka.core.Option;
import weka.core.SparseInstance;
import weka.core.TestInstances;
import weka.core.neighboursearch.CoverTree;
import weka.core.stemmers.NullStemmer;
import weka.core.stemmers.Stemmer;
import weka.core.tokenizers.Tokenizer;
import weka.core.tokenizers.WordTokenizer;

@RunWith(EvoRunner.class) @EvoRunnerParameters(mockJVMNonDeterminism = true, useVFS = true, useVNET = true, resetStaticState = true, separateClassLoader = true, useJEE = true) 
public class NaiveBayesMultinomialText_ESTest extends NaiveBayesMultinomialText_ESTest_scaffolding {

  /**
  //Test case number: 0
  /*Coverage entropy=2.3025850929940455
  */
  @Test(timeout = 4000)
  public void test00()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      LinkedHashMap<String, NaiveBayesMultinomialText.Count> linkedHashMap0 = new LinkedHashMap<String, NaiveBayesMultinomialText.Count>();
      naiveBayesMultinomialText0.m_inputVector = linkedHashMap0;
      naiveBayesMultinomialText0.stemmerTipText();
      naiveBayesMultinomialText0.m_wordFrequencies = true;
      naiveBayesMultinomialText0.setLNorm(0);
      naiveBayesMultinomialText0.setPeriodicPruning(0);
      Stemmer stemmer0 = naiveBayesMultinomialText0.getStemmer();
      naiveBayesMultinomialText0.setStemmer(stemmer0);
      naiveBayesMultinomialText0.getNormalizeDocLength();
      naiveBayesMultinomialText0.getStopwords();
      naiveBayesMultinomialText0.setStemmer(stemmer0);
      String[] stringArray0 = new String[5];
      stringArray0[0] = "The stemming algorithm to use on the words.";
      stringArray0[1] = "The stemming algorithm to use on the words.";
      stringArray0[2] = "The stemming algorithm to use on the words.";
      stringArray0[3] = "The stemming algorithm to use on the words.";
      stringArray0[4] = "The stemming algorithm to use on the words.";
      NaiveBayesMultinomialText.main(stringArray0);
      assertEquals(5, stringArray0.length);
  }

  /**
  //Test case number: 1
  /*Coverage entropy=2.3025850929940455
  */
  @Test(timeout = 4000)
  public void test01()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      WordTokenizer wordTokenizer0 = (WordTokenizer)naiveBayesMultinomialText0.m_tokenizer;
      naiveBayesMultinomialText0.setTokenizer(wordTokenizer0);
      naiveBayesMultinomialText0.setUseStopList(true);
      naiveBayesMultinomialText0.periodicPruningTipText();
      naiveBayesMultinomialText0.useStopListTipText();
      naiveBayesMultinomialText0.stopwordsTipText();
      naiveBayesMultinomialText0.setNormalizeDocLength(true);
      naiveBayesMultinomialText0.getMinWordFrequency();
      naiveBayesMultinomialText0.normalizeDocLengthTipText();
      double double0 = naiveBayesMultinomialText0.getLNorm();
      assertTrue(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(2.0, double0, 0.01);
  }

  /**
  //Test case number: 2
  /*Coverage entropy=2.890371757896165
  */
  @Test(timeout = 4000)
  public void test02()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.setLNorm((-607.0));
      naiveBayesMultinomialText0.pruneDictionary();
      naiveBayesMultinomialText0.m_lowercaseTokens = true;
      naiveBayesMultinomialText0.LNormTipText();
      naiveBayesMultinomialText0.listOptions();
      String[] stringArray0 = new String[5];
      stringArray0[0] = "The LNorm to use for document length normalization.";
      naiveBayesMultinomialText0.setDebug(true);
      stringArray0[1] = "The LNorm to use for document length normalization.";
      stringArray0[2] = "The LNorm to use for document length normalization.";
      stringArray0[3] = ">=YCD~,q3|w";
      stringArray0[4] = "The LNorm to use for document length normalization.";
      NaiveBayesMultinomialText.main(stringArray0);
      naiveBayesMultinomialText0.useWordFrequenciesTipText();
      naiveBayesMultinomialText0.getMinWordFrequency();
      naiveBayesMultinomialText0.getStemmer();
      naiveBayesMultinomialText0.setLowercaseTokens(true);
      naiveBayesMultinomialText0.stemmerTipText();
      naiveBayesMultinomialText0.listOptions();
      naiveBayesMultinomialText0.setMinWordFrequency(0.0);
      naiveBayesMultinomialText0.getUseStopList();
      naiveBayesMultinomialText0.useWordFrequenciesTipText();
      naiveBayesMultinomialText0.getRevision();
      File file0 = naiveBayesMultinomialText0.getStopwords();
      naiveBayesMultinomialText0.setStopwords(file0);
      naiveBayesMultinomialText0.getTokenizer();
      naiveBayesMultinomialText0.globalInfo();
      assertEquals((-607.0), naiveBayesMultinomialText0.getLNorm(), 0.01);
  }

  /**
  //Test case number: 3
  /*Coverage entropy=1.0986122886681096
  */
  @Test(timeout = 4000)
  public void test03()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      
      naiveBayesMultinomialText0.m_wordFrequencies = true;
      naiveBayesMultinomialText0.minWordFrequencyTipText();
      String string0 = naiveBayesMultinomialText0.toString();
      assertEquals("NaiveBayesMultinomialText: No model built yet.\n", string0);
  }

  /**
  //Test case number: 4
  /*Coverage entropy=2.6390573296152584
  */
  @Test(timeout = 4000)
  public void test04()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.getRevision();
      naiveBayesMultinomialText0.setUseStopList(true);
      naiveBayesMultinomialText0.setLNorm((-2219.455458));
      naiveBayesMultinomialText0.getStemmer();
      naiveBayesMultinomialText0.getRevision();
      naiveBayesMultinomialText0.setUseStopList(false);
      naiveBayesMultinomialText0.minWordFrequencyTipText();
      String[] stringArray0 = new String[9];
      stringArray0[0] = "";
      stringArray0[1] = "Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value";
      stringArray0[2] = "9122";
      stringArray0[3] = "9122";
      stringArray0[4] = "9122";
      stringArray0[5] = "N;&)o;Ymz3Q|F3@|";
      stringArray0[6] = "9122";
      stringArray0[7] = "Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value";
      stringArray0[8] = "9122";
      naiveBayesMultinomialText0.setOptions(stringArray0);
      assertEquals((-2219.455458), naiveBayesMultinomialText0.getLNorm(), 0.01);
  }

  /**
  //Test case number: 5
  /*Coverage entropy=2.995732273553991
  */
  @Test(timeout = 4000)
  public void test05()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      double[] doubleArray0 = new double[1];
      doubleArray0[0] = 0.0;
      naiveBayesMultinomialText0.m_wordsPerClass = doubleArray0;
      naiveBayesMultinomialText0.m_minWordP = 0.0;
      naiveBayesMultinomialText0.m_lowercaseTokens = true;
      naiveBayesMultinomialText0.listOptions();
      naiveBayesMultinomialText0.getCapabilities();
      String[] stringArray0 = new String[9];
      stringArray0[0] = "weka/core/Capabilities.props";
      stringArray0[1] = "weka/core/Capabilities.props";
      stringArray0[2] = "weka/core/Capabilities.props";
      stringArray0[3] = "weka/core/Capabilities.props";
      stringArray0[4] = "weka/core/Capabilities.props";
      stringArray0[5] = "weka/core/Capabilities.props";
      stringArray0[6] = "weka/core/Capabilities.props";
      stringArray0[7] = "weka/core/Capabilities.props";
      stringArray0[8] = "weka/core/Capabilities.props";
      NaiveBayesMultinomialText.main(stringArray0);
      naiveBayesMultinomialText0.getMinWordFrequency();
      naiveBayesMultinomialText0.setNorm(0.0);
      naiveBayesMultinomialText0.globalInfo();
      naiveBayesMultinomialText0.normalizeDocLengthTipText();
      naiveBayesMultinomialText0.getNorm();
      naiveBayesMultinomialText0.getUseStopList();
      naiveBayesMultinomialText0.getOptions();
      naiveBayesMultinomialText0.setNormalizeDocLength(false);
      assertTrue(naiveBayesMultinomialText0.getLowercaseTokens());
  }

  /**
  //Test case number: 6
  /*Coverage entropy=3.0646194052010944
  */
  @Test(timeout = 4000)
  public void test06()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertNotNull(naiveBayesMultinomialText0);
      
      NaiveBayesMultinomialText naiveBayesMultinomialText1 = new NaiveBayesMultinomialText();
      assertFalse(naiveBayesMultinomialText1.equals((Object)naiveBayesMultinomialText0));
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText1.minWordFrequencyTipText());
      assertEquals(3.0, naiveBayesMultinomialText1.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText1.LNormTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText1.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText1.debugTipText());
      assertFalse(naiveBayesMultinomialText1.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText1.globalInfo());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText1.normalizeDocLengthTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText1.normTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText1.periodicPruningTipText());
      assertFalse(naiveBayesMultinomialText1.getNormalizeDocLength());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText1.tokenizerTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText1.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText1.getDebug());
      assertEquals(1.0, naiveBayesMultinomialText1.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText1.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText1.getUseStopList());
      assertEquals(2.0, naiveBayesMultinomialText1.getLNorm(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText1.stopwordsTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText1.useStopListTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText1.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText1.getPeriodicPruning());
      assertNotNull(naiveBayesMultinomialText1);
      
      Map<Integer, LinkedHashMap<String, NaiveBayesMultinomialText.Count>> map0 = naiveBayesMultinomialText1.m_probOfWordGivenClass;
      assertNull(map0);
      
      naiveBayesMultinomialText0.m_probOfWordGivenClass = null;
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      
      String[] stringArray0 = new String[1];
      String[] stringArray1 = naiveBayesMultinomialText0.getOptions();
      assertNotSame(naiveBayesMultinomialText0, naiveBayesMultinomialText1);
      assertNotSame(stringArray1, stringArray0);
      assertFalse(naiveBayesMultinomialText0.equals((Object)naiveBayesMultinomialText1));
      assertFalse(stringArray1.equals((Object)stringArray0));
      assertEquals(12, stringArray1.length);
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertNotNull(stringArray1);
      
      stringArray0[0] = "\tOnly outputs the graph representation of the classifier.\n";
      naiveBayesMultinomialText0.setOptions(stringArray0);
      assertNotSame(naiveBayesMultinomialText0, naiveBayesMultinomialText1);
      assertNotSame(stringArray0, stringArray1);
      assertFalse(naiveBayesMultinomialText0.equals((Object)naiveBayesMultinomialText1));
      assertFalse(stringArray0.equals((Object)stringArray1));
      assertEquals(1, stringArray0.length);
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      
      String string0 = naiveBayesMultinomialText0.useStopListTipText();
      assertNotSame(naiveBayesMultinomialText0, naiveBayesMultinomialText1);
      assertEquals("If true, ignores all words that are on the stoplist.", string0);
      assertFalse(naiveBayesMultinomialText0.equals((Object)naiveBayesMultinomialText1));
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertNotNull(string0);
      
      NaiveBayesMultinomialText.main(stringArray0);
      assertNotSame(stringArray0, stringArray1);
      assertFalse(stringArray0.equals((Object)stringArray1));
      assertEquals(1, stringArray0.length);
      
      File file0 = naiveBayesMultinomialText1.getStopwords();
      assertNotSame(naiveBayesMultinomialText1, naiveBayesMultinomialText0);
      assertFalse(naiveBayesMultinomialText1.equals((Object)naiveBayesMultinomialText0));
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText1.minWordFrequencyTipText());
      assertEquals(3.0, naiveBayesMultinomialText1.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText1.LNormTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText1.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText1.debugTipText());
      assertFalse(naiveBayesMultinomialText1.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText1.globalInfo());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText1.normalizeDocLengthTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText1.normTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText1.periodicPruningTipText());
      assertFalse(naiveBayesMultinomialText1.getNormalizeDocLength());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText1.tokenizerTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText1.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText1.getDebug());
      assertEquals(1.0, naiveBayesMultinomialText1.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText1.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText1.getUseStopList());
      assertEquals(2.0, naiveBayesMultinomialText1.getLNorm(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText1.stopwordsTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText1.useStopListTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText1.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText1.getPeriodicPruning());
      assertTrue(file0.canWrite());
      assertEquals(0L, file0.getUsableSpace());
      assertFalse(file0.isHidden());
      assertTrue(file0.canExecute());
      assertEquals("/home/ubuntu/evosuite_readability_gen/projects", file0.getParent());
      assertFalse(file0.isFile());
      assertEquals(1392409281320L, file0.lastModified());
      assertEquals(0L, file0.getTotalSpace());
      assertTrue(file0.exists());
      assertTrue(file0.canRead());
      assertEquals(0L, file0.getFreeSpace());
      assertEquals(0L, file0.length());
      assertEquals("107_weka", file0.getName());
      assertEquals("/home/ubuntu/evosuite_readability_gen/projects/107_weka", file0.toString());
      assertTrue(file0.isDirectory());
      assertTrue(file0.isAbsolute());
      assertNotNull(file0);
      
      naiveBayesMultinomialText1.pruneDictionary();
      assertNotSame(naiveBayesMultinomialText1, naiveBayesMultinomialText0);
      assertFalse(naiveBayesMultinomialText1.equals((Object)naiveBayesMultinomialText0));
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText1.minWordFrequencyTipText());
      assertEquals(3.0, naiveBayesMultinomialText1.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText1.LNormTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText1.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText1.debugTipText());
      assertFalse(naiveBayesMultinomialText1.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText1.globalInfo());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText1.normalizeDocLengthTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText1.normTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText1.periodicPruningTipText());
      assertFalse(naiveBayesMultinomialText1.getNormalizeDocLength());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText1.tokenizerTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText1.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText1.getDebug());
      assertEquals(1.0, naiveBayesMultinomialText1.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText1.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText1.getUseStopList());
      assertEquals(2.0, naiveBayesMultinomialText1.getLNorm(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText1.stopwordsTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText1.useStopListTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText1.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText1.getPeriodicPruning());
      
      String string1 = naiveBayesMultinomialText0.minWordFrequencyTipText();
      assertNotSame(naiveBayesMultinomialText0, naiveBayesMultinomialText1);
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", string1);
      assertFalse(naiveBayesMultinomialText0.equals((Object)naiveBayesMultinomialText1));
      assertFalse(string1.equals((Object)string0));
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertNotNull(string1);
  }

  /**
  //Test case number: 7
  /*Coverage entropy=2.8875778250565496
  */
  @Test(timeout = 4000)
  public void test07()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertNotNull(naiveBayesMultinomialText0);
      
      double double0 = naiveBayesMultinomialText0.getMinWordFrequency();
      assertEquals(3.0, double0, 0.01);
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      
      String string0 = naiveBayesMultinomialText0.normalizeDocLengthTipText();
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", string0);
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertNotNull(string0);
      
      String string1 = naiveBayesMultinomialText0.getRevision();
      assertEquals("9122", string1);
      assertFalse(string1.equals((Object)string0));
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertNotNull(string1);
      
      String[] stringArray0 = naiveBayesMultinomialText0.getOptions();
      assertEquals(12, stringArray0.length);
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertNotNull(stringArray0);
      
      double[] doubleArray0 = new double[1];
      doubleArray0[0] = 3.0;
      MockFile mockFile0 = (MockFile)naiveBayesMultinomialText0.m_stopwordsFile;
      assertEquals(0L, mockFile0.getUsableSpace());
      assertEquals("107_weka", mockFile0.getName());
      assertEquals("/home/ubuntu/evosuite_readability_gen/projects", mockFile0.getParent());
      assertEquals(1392409281320L, mockFile0.lastModified());
      assertTrue(mockFile0.exists());
      assertTrue(mockFile0.canRead());
      assertTrue(mockFile0.isDirectory());
      assertEquals(0L, mockFile0.getTotalSpace());
      assertEquals(0L, mockFile0.length());
      assertTrue(mockFile0.canExecute());
      assertFalse(mockFile0.isFile());
      assertEquals("/home/ubuntu/evosuite_readability_gen/projects/107_weka", mockFile0.toString());
      assertFalse(mockFile0.isHidden());
      assertEquals(0L, mockFile0.getFreeSpace());
      assertTrue(mockFile0.canWrite());
      assertTrue(mockFile0.isAbsolute());
      assertNotNull(mockFile0);
      
      File file0 = MockFile.createTempFile("9122", "[Ui}e/:M?L*<?", (File) mockFile0);
      assertNotSame(mockFile0, file0);
      assertNotSame(file0, mockFile0);
      assertFalse(file0.equals((Object)mockFile0));
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals(0L, mockFile0.getUsableSpace());
      assertEquals("107_weka", mockFile0.getName());
      assertEquals("/home/ubuntu/evosuite_readability_gen/projects", mockFile0.getParent());
      assertEquals(1392409281320L, mockFile0.lastModified());
      assertTrue(mockFile0.exists());
      assertTrue(mockFile0.canRead());
      assertTrue(mockFile0.isDirectory());
      assertEquals(0L, mockFile0.getTotalSpace());
      assertEquals(0L, mockFile0.length());
      assertTrue(mockFile0.canExecute());
      assertFalse(mockFile0.isFile());
      assertEquals("/home/ubuntu/evosuite_readability_gen/projects/107_weka", mockFile0.toString());
      assertFalse(mockFile0.isHidden());
      assertEquals(0L, mockFile0.getFreeSpace());
      assertTrue(mockFile0.canWrite());
      assertTrue(mockFile0.isAbsolute());
      assertEquals("/home/ubuntu/evosuite_readability_gen/projects/107_weka/91220[Ui}e", file0.getParent());
      assertFalse(file0.isDirectory());
      assertEquals(0L, file0.getUsableSpace());
      assertEquals(0L, file0.getFreeSpace());
      assertTrue(file0.canWrite());
      assertEquals(":M?L*<?", file0.getName());
      assertEquals("/home/ubuntu/evosuite_readability_gen/projects/107_weka/91220[Ui}e/:M?L*<?", file0.toString());
      assertFalse(file0.isHidden());
      assertTrue(file0.canExecute());
      assertTrue(file0.isAbsolute());
      assertTrue(file0.canRead());
      assertTrue(file0.isFile());
      assertEquals(0L, file0.length());
      assertEquals(1392409281320L, file0.lastModified());
      assertTrue(file0.exists());
      assertEquals(0L, file0.getTotalSpace());
      assertNotNull(file0);
      
      naiveBayesMultinomialText0.setStopwords(file0);
      assertNotSame(mockFile0, file0);
      assertNotSame(file0, mockFile0);
      assertFalse(mockFile0.equals((Object)file0));
      assertFalse(file0.equals((Object)mockFile0));
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertTrue(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals(0L, mockFile0.getUsableSpace());
      assertEquals("107_weka", mockFile0.getName());
      assertEquals("/home/ubuntu/evosuite_readability_gen/projects", mockFile0.getParent());
      assertEquals(1392409281320L, mockFile0.lastModified());
      assertTrue(mockFile0.exists());
      assertTrue(mockFile0.canRead());
      assertTrue(mockFile0.isDirectory());
      assertEquals(0L, mockFile0.getTotalSpace());
      assertEquals(0L, mockFile0.length());
      assertTrue(mockFile0.canExecute());
      assertFalse(mockFile0.isFile());
      assertEquals("/home/ubuntu/evosuite_readability_gen/projects/107_weka", mockFile0.toString());
      assertFalse(mockFile0.isHidden());
      assertEquals(0L, mockFile0.getFreeSpace());
      assertTrue(mockFile0.canWrite());
      assertTrue(mockFile0.isAbsolute());
      assertEquals("/home/ubuntu/evosuite_readability_gen/projects/107_weka/91220[Ui}e", file0.getParent());
      assertFalse(file0.isDirectory());
      assertEquals(0L, file0.getUsableSpace());
      assertEquals(0L, file0.getFreeSpace());
      assertTrue(file0.canWrite());
      assertEquals(":M?L*<?", file0.getName());
      assertEquals("/home/ubuntu/evosuite_readability_gen/projects/107_weka/91220[Ui}e/:M?L*<?", file0.toString());
      assertFalse(file0.isHidden());
      assertTrue(file0.canExecute());
      assertTrue(file0.isAbsolute());
      assertTrue(file0.canRead());
      assertTrue(file0.isFile());
      assertEquals(0L, file0.length());
      assertEquals(1392409281320L, file0.lastModified());
      assertTrue(file0.exists());
      assertEquals(0L, file0.getTotalSpace());
      
      naiveBayesMultinomialText0.m_probOfClass = doubleArray0;
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertTrue(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      
      boolean boolean0 = naiveBayesMultinomialText0.getNormalizeDocLength();
      assertFalse(boolean0);
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertTrue(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      
      naiveBayesMultinomialText0.setNorm(3.0);
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertTrue(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      
      int int0 = naiveBayesMultinomialText0.getPeriodicPruning();
      assertEquals(0, int0);
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertTrue(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      
      naiveBayesMultinomialText0.setLNorm((-2256.360856009));
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertTrue(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals((-2256.360856009), naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals(3.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      
      boolean boolean1 = naiveBayesMultinomialText0.getUseWordFrequencies();
      assertFalse(boolean1);
      assertTrue(boolean1 == boolean0);
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertTrue(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals((-2256.360856009), naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals(3.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      
      URI uRI0 = MockURI.aHttpURI;
      assertEquals("foo.bar", uRI0.getHost());
      assertEquals((-1), uRI0.getPort());
      assertFalse(uRI0.isOpaque());
      assertNull(uRI0.getRawFragment());
      assertNull(uRI0.getRawUserInfo());
      assertNull(uRI0.getRawQuery());
      assertEquals("", uRI0.getRawPath());
      assertEquals("http", uRI0.getScheme());
      assertEquals("foo.bar", uRI0.getRawAuthority());
      assertTrue(uRI0.isAbsolute());
      assertEquals("http://foo.bar", uRI0.toString());
      assertNotNull(uRI0);
      
      MockFile mockFile1 = null;
      try {
        mockFile1 = new MockFile(uRI0);
        fail("Expecting exception: IllegalArgumentException");
      
      } catch(IllegalArgumentException e) {
         //
         // URI scheme is not \"file\"
         //
         verifyException("java.io.File", e);
      }
  }

  /**
  //Test case number: 8
  /*Coverage entropy=1.4750763110546947
  */
  @Test(timeout = 4000)
  public void test08()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertNotNull(naiveBayesMultinomialText0);
      
      naiveBayesMultinomialText0.setUseStopList(false);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      
      Map<Integer, LinkedHashMap<String, NaiveBayesMultinomialText.Count>> map0 = naiveBayesMultinomialText0.m_probOfWordGivenClass;
      assertNull(map0);
      
      naiveBayesMultinomialText0.m_probOfWordGivenClass = null;
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      
      boolean boolean0 = naiveBayesMultinomialText0.getUseStopList();
      assertFalse(boolean0);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      
      double[] doubleArray0 = new double[9];
      doubleArray0[0] = 0.964;
      doubleArray0[1] = 1922.097;
      doubleArray0[2] = 0.0;
      doubleArray0[3] = 0.964;
      doubleArray0[4] = 0.964;
      doubleArray0[5] = 0.964;
      doubleArray0[6] = 0.964;
      doubleArray0[7] = 0.964;
      doubleArray0[8] = 0.964;
      SparseInstance sparseInstance0 = new SparseInstance(0.964, doubleArray0);
      assertArrayEquals(new double[] {0.964, 1922.097, 0.0, 0.964, 0.964, 0.964, 0.964, 0.964, 0.964}, doubleArray0, 0.01);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertEquals(9, doubleArray0.length);
      assertEquals(8, sparseInstance0.numValues());
      assertEquals(0.964, sparseInstance0.weight(), 0.01);
      assertEquals(9, sparseInstance0.numAttributes());
      assertNotNull(sparseInstance0);
      
      try { 
        naiveBayesMultinomialText0.classifyInstance(sparseInstance0);
        fail("Expecting exception: RuntimeException");
      
      } catch(RuntimeException e) {
         //
         // DenseInstance doesn't have access to a dataset!
         //
         verifyException("weka.core.AbstractInstance", e);
      }
  }

  /**
  //Test case number: 9
  /*Coverage entropy=1.0986122886681096
  */
  @Test(timeout = 4000)
  public void test09()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertNotNull(naiveBayesMultinomialText0);
      
      naiveBayesMultinomialText0.m_periodicP = 2301;
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals(2301, naiveBayesMultinomialText0.getPeriodicPruning());
      
      int int0 = naiveBayesMultinomialText0.getPeriodicPruning();
      assertEquals(2301, int0);
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals(2301, naiveBayesMultinomialText0.getPeriodicPruning());
      
      // Undeclared exception!
      try { 
        naiveBayesMultinomialText0.pruneDictionary();
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("weka.classifiers.bayes.NaiveBayesMultinomialText", e);
      }
  }

  /**
  //Test case number: 10
  /*Coverage entropy=3.0445224377234217
  */
  @Test(timeout = 4000)
  public void test10()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertNotNull(naiveBayesMultinomialText0);
      
      TestInstances testInstances0 = new TestInstances();
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(20, testInstances0.getNumInstances());
      assertFalse(testInstances0.getNoClass());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(1, testInstances0.getClassType());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(2, testInstances0.getNumAttributes());
      assertNotNull(testInstances0);
      
      testInstances0.setRelation("RG1=e66u2");
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumString());
      assertEquals("RG1=e66u2", testInstances0.getRelation());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(20, testInstances0.getNumInstances());
      assertFalse(testInstances0.getNoClass());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(1, testInstances0.getClassType());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(2, testInstances0.getNumAttributes());
      
      NaiveBayesMultinomialText naiveBayesMultinomialText1 = new NaiveBayesMultinomialText();
      assertFalse(naiveBayesMultinomialText1.equals((Object)naiveBayesMultinomialText0));
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText1.normalizeDocLengthTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText1.tokenizerTipText());
      assertEquals(3.0, naiveBayesMultinomialText1.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText1.LNormTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText1.lowercaseTokensTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText1.useWordFrequenciesTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText1.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText1.useStopListTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText1.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText1.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText1.getNormalizeDocLength());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText1.periodicPruningTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText1.stopwordsTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText1.normTipText());
      assertFalse(naiveBayesMultinomialText1.getDebug());
      assertFalse(naiveBayesMultinomialText1.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText1.globalInfo());
      assertEquals(0, naiveBayesMultinomialText1.getPeriodicPruning());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText1.debugTipText());
      assertEquals(1.0, naiveBayesMultinomialText1.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText1.getUseStopList());
      assertEquals(2.0, naiveBayesMultinomialText1.getLNorm(), 0.01);
      assertNotNull(naiveBayesMultinomialText1);
      
      LinkedHashMap<String, NaiveBayesMultinomialText.Count> linkedHashMap0 = naiveBayesMultinomialText1.m_inputVector;
      assertNull(linkedHashMap0);
      
      naiveBayesMultinomialText0.m_inputVector = null;
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      
      SGDText sGDText0 = new SGDText();
      assertEquals(1, SGDText.LOGLOSS);
      assertEquals(0, SGDText.HINGE);
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText0.debugTipText());
      assertEquals(500, sGDText0.getEpochs());
      assertEquals("The stemming algorithm to use on the words.", sGDText0.stemmerTipText());
      assertEquals("Whether to convert all tokens to lowercase", sGDText0.lowercaseTokensTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText0.normalizeDocLengthTipText());
      assertEquals(3.0, sGDText0.getMinWordFrequency(), 0.01);
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText0.epochsTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText0.useStopListTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText0.stopwordsTipText());
      assertEquals("The learning rate.", sGDText0.learningRateTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText0.periodicPruningTipText());
      assertFalse(sGDText0.getLowercaseTokens());
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText0.tokenizerTipText());
      assertFalse(sGDText0.getNormalizeDocLength());
      assertFalse(sGDText0.getDebug());
      assertEquals(0, sGDText0.getPeriodicPruning());
      assertFalse(sGDText0.getUseStopList());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText0.minWordFrequencyTipText());
      assertEquals("The random number seed to be used.", sGDText0.seedTipText());
      assertEquals(2.0, sGDText0.getLNorm(), 0.01);
      assertEquals(1.0, sGDText0.getNorm(), 0.01);
      assertEquals("The norm of the instances after normalization.", sGDText0.normTipText());
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText0.lossFunctionTipText());
      assertEquals(1, sGDText0.getSeed());
      assertEquals("The LNorm to use for document length normalization.", sGDText0.LNormTipText());
      assertEquals(0.01, sGDText0.getLearningRate(), 0.01);
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText0.outputProbsForSVMTipText());
      assertEquals("The regularization constant. (default = 0.0001)", sGDText0.lambdaTipText());
      assertEquals(1.0E-4, sGDText0.getLambda(), 0.01);
      assertFalse(sGDText0.getUseWordFrequencies());
      assertFalse(sGDText0.getOutputProbsForSVM());
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText0.globalInfo());
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText0.useWordFrequenciesTipText());
      assertNotNull(sGDText0);
      
      WordTokenizer wordTokenizer0 = (WordTokenizer)sGDText0.getTokenizer();
      assertEquals(1, SGDText.LOGLOSS);
      assertEquals(0, SGDText.HINGE);
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText0.debugTipText());
      assertEquals(500, sGDText0.getEpochs());
      assertEquals("The stemming algorithm to use on the words.", sGDText0.stemmerTipText());
      assertEquals("Whether to convert all tokens to lowercase", sGDText0.lowercaseTokensTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText0.normalizeDocLengthTipText());
      assertEquals(3.0, sGDText0.getMinWordFrequency(), 0.01);
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText0.epochsTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText0.useStopListTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText0.stopwordsTipText());
      assertEquals("The learning rate.", sGDText0.learningRateTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText0.periodicPruningTipText());
      assertFalse(sGDText0.getLowercaseTokens());
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText0.tokenizerTipText());
      assertFalse(sGDText0.getNormalizeDocLength());
      assertFalse(sGDText0.getDebug());
      assertEquals(0, sGDText0.getPeriodicPruning());
      assertFalse(sGDText0.getUseStopList());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText0.minWordFrequencyTipText());
      assertEquals("The random number seed to be used.", sGDText0.seedTipText());
      assertEquals(2.0, sGDText0.getLNorm(), 0.01);
      assertEquals(1.0, sGDText0.getNorm(), 0.01);
      assertEquals("The norm of the instances after normalization.", sGDText0.normTipText());
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText0.lossFunctionTipText());
      assertEquals(1, sGDText0.getSeed());
      assertEquals("The LNorm to use for document length normalization.", sGDText0.LNormTipText());
      assertEquals(0.01, sGDText0.getLearningRate(), 0.01);
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText0.outputProbsForSVMTipText());
      assertEquals("The regularization constant. (default = 0.0001)", sGDText0.lambdaTipText());
      assertEquals(1.0E-4, sGDText0.getLambda(), 0.01);
      assertFalse(sGDText0.getUseWordFrequencies());
      assertFalse(sGDText0.getOutputProbsForSVM());
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText0.globalInfo());
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText0.useWordFrequenciesTipText());
      assertEquals("A simple tokenizer that is using the java.util.StringTokenizer class to tokenize the strings.", wordTokenizer0.globalInfo());
      assertEquals(" \r\n\t.,;:'\"()?!", wordTokenizer0.getDelimiters());
      assertEquals("Set of delimiter characters to use in tokenizing (\\r, \\n and \\t can be used for carriage-return, line-feed and tab)", wordTokenizer0.delimitersTipText());
      assertNotNull(wordTokenizer0);
      
      naiveBayesMultinomialText0.m_tokenizer = (Tokenizer) wordTokenizer0;
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText0.debugTipText());
      assertEquals(500, sGDText0.getEpochs());
      assertEquals("The stemming algorithm to use on the words.", sGDText0.stemmerTipText());
      assertEquals("Whether to convert all tokens to lowercase", sGDText0.lowercaseTokensTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText0.normalizeDocLengthTipText());
      assertEquals(3.0, sGDText0.getMinWordFrequency(), 0.01);
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText0.epochsTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText0.useStopListTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText0.stopwordsTipText());
      assertEquals("The learning rate.", sGDText0.learningRateTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText0.periodicPruningTipText());
      assertFalse(sGDText0.getLowercaseTokens());
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText0.tokenizerTipText());
      assertFalse(sGDText0.getNormalizeDocLength());
      assertFalse(sGDText0.getDebug());
      assertEquals(0, sGDText0.getPeriodicPruning());
      assertFalse(sGDText0.getUseStopList());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText0.minWordFrequencyTipText());
      assertEquals("The random number seed to be used.", sGDText0.seedTipText());
      assertEquals(2.0, sGDText0.getLNorm(), 0.01);
      assertEquals(1.0, sGDText0.getNorm(), 0.01);
      assertEquals("The norm of the instances after normalization.", sGDText0.normTipText());
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText0.lossFunctionTipText());
      assertEquals(1, sGDText0.getSeed());
      assertEquals("The LNorm to use for document length normalization.", sGDText0.LNormTipText());
      assertEquals(0.01, sGDText0.getLearningRate(), 0.01);
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText0.outputProbsForSVMTipText());
      assertEquals("The regularization constant. (default = 0.0001)", sGDText0.lambdaTipText());
      assertEquals(1.0E-4, sGDText0.getLambda(), 0.01);
      assertFalse(sGDText0.getUseWordFrequencies());
      assertFalse(sGDText0.getOutputProbsForSVM());
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText0.globalInfo());
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText0.useWordFrequenciesTipText());
      assertEquals("A simple tokenizer that is using the java.util.StringTokenizer class to tokenize the strings.", wordTokenizer0.globalInfo());
      assertEquals(" \r\n\t.,;:'\"()?!", wordTokenizer0.getDelimiters());
      assertEquals("Set of delimiter characters to use in tokenizing (\\r, \\n and \\t can be used for carriage-return, line-feed and tab)", wordTokenizer0.delimitersTipText());
      
      naiveBayesMultinomialText0.m_lowercaseTokens = false;
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      
      TestInstances.main(testInstances0.DEFAULT_WORDS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumString());
      assertEquals("RG1=e66u2", testInstances0.getRelation());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(20, testInstances0.getNumInstances());
      assertFalse(testInstances0.getNoClass());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(1, testInstances0.getClassType());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(2, testInstances0.getNumAttributes());
      
      Instances instances0 = testInstances0.generate("");
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumString());
      assertEquals("RG1=e66u2", testInstances0.getRelation());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(20, testInstances0.getNumInstances());
      assertFalse(testInstances0.getNoClass());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(1, testInstances0.getClassType());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(2, instances0.numClasses());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20, instances0.size());
      assertEquals(1, instances0.classIndex());
      assertEquals("RG1=e66u2", instances0.relationName());
      assertEquals(20, instances0.numInstances());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(2, instances0.numAttributes());
      assertNotNull(instances0);
      
      NullStemmer nullStemmer0 = new NullStemmer();
      assertEquals("A dummy stemmer that performs no stemming at all.", nullStemmer0.globalInfo());
      assertNotNull(nullStemmer0);
      
      boolean boolean0 = instances0.remove((Object) nullStemmer0);
      assertFalse(boolean0);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumString());
      assertEquals("RG1=e66u2", testInstances0.getRelation());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(20, testInstances0.getNumInstances());
      assertFalse(testInstances0.getNoClass());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(1, testInstances0.getClassType());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(2, instances0.numClasses());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20, instances0.size());
      assertEquals(1, instances0.classIndex());
      assertEquals("RG1=e66u2", instances0.relationName());
      assertEquals(20, instances0.numInstances());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(2, instances0.numAttributes());
      assertEquals("A dummy stemmer that performs no stemming at all.", nullStemmer0.globalInfo());
      
      naiveBayesMultinomialText0.buildClassifier(instances0);
      assertNotSame(naiveBayesMultinomialText0, naiveBayesMultinomialText1);
      assertFalse(naiveBayesMultinomialText0.equals((Object)naiveBayesMultinomialText1));
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumString());
      assertEquals("RG1=e66u2", testInstances0.getRelation());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(20, testInstances0.getNumInstances());
      assertFalse(testInstances0.getNoClass());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(1, testInstances0.getClassType());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(2, instances0.numClasses());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20, instances0.size());
      assertEquals(1, instances0.classIndex());
      assertEquals("RG1=e66u2", instances0.relationName());
      assertEquals(20, instances0.numInstances());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(2, instances0.numAttributes());
      
      naiveBayesMultinomialText0.setPeriodicPruning(0);
      assertNotSame(naiveBayesMultinomialText0, naiveBayesMultinomialText1);
      assertFalse(naiveBayesMultinomialText0.equals((Object)naiveBayesMultinomialText1));
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      
      NaiveBayesMultinomialText.main(testInstances0.DEFAULT_WORDS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumString());
      assertEquals("RG1=e66u2", testInstances0.getRelation());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(20, testInstances0.getNumInstances());
      assertFalse(testInstances0.getNoClass());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(1, testInstances0.getClassType());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(2, testInstances0.getNumAttributes());
      
      String string0 = naiveBayesMultinomialText0.tokenizerTipText();
      assertNotSame(naiveBayesMultinomialText0, naiveBayesMultinomialText1);
      assertEquals("The tokenizing algorithm to use on the strings.", string0);
      assertFalse(naiveBayesMultinomialText0.equals((Object)naiveBayesMultinomialText1));
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertNotNull(string0);
      
      boolean boolean1 = naiveBayesMultinomialText0.getUseStopList();
      assertNotSame(naiveBayesMultinomialText0, naiveBayesMultinomialText1);
      assertFalse(boolean1);
      assertFalse(naiveBayesMultinomialText0.equals((Object)naiveBayesMultinomialText1));
      assertTrue(boolean1 == boolean0);
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      
      naiveBayesMultinomialText0.setNorm((-1));
      assertNotSame(naiveBayesMultinomialText0, naiveBayesMultinomialText1);
      assertFalse(naiveBayesMultinomialText0.equals((Object)naiveBayesMultinomialText1));
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals((-1.0), naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      
      String string1 = naiveBayesMultinomialText0.lowercaseTokensTipText();
      assertNotSame(naiveBayesMultinomialText0, naiveBayesMultinomialText1);
      assertEquals("Whether to convert all tokens to lowercase", string1);
      assertFalse(naiveBayesMultinomialText0.equals((Object)naiveBayesMultinomialText1));
      assertFalse(string1.equals((Object)string0));
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals((-1.0), naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertNotNull(string1);
      
      naiveBayesMultinomialText0.pruneDictionary();
      assertNotSame(naiveBayesMultinomialText0, naiveBayesMultinomialText1);
      assertFalse(naiveBayesMultinomialText0.equals((Object)naiveBayesMultinomialText1));
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals((-1.0), naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      
      String string2 = naiveBayesMultinomialText0.toString();
      assertNotSame(naiveBayesMultinomialText0, naiveBayesMultinomialText1);
      assertEquals("The independent probability of a class\n--------------------------------------\nclass1\t11.0\nclass2\t11.0\n\nThe probability of a word given the class\n-----------------------------------------\n\tclass1\tclass2\t\n", string2);
      assertFalse(naiveBayesMultinomialText0.equals((Object)naiveBayesMultinomialText1));
      assertFalse(string2.equals((Object)string1));
      assertFalse(string2.equals((Object)string0));
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals((-1.0), naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertNotNull(string2);
      
      sGDText0.setPeriodicPruning(0);
      assertEquals(1, SGDText.LOGLOSS);
      assertEquals(0, SGDText.HINGE);
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText0.debugTipText());
      assertEquals(500, sGDText0.getEpochs());
      assertEquals("The stemming algorithm to use on the words.", sGDText0.stemmerTipText());
      assertEquals("Whether to convert all tokens to lowercase", sGDText0.lowercaseTokensTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText0.normalizeDocLengthTipText());
      assertEquals(3.0, sGDText0.getMinWordFrequency(), 0.01);
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText0.epochsTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText0.useStopListTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText0.stopwordsTipText());
      assertEquals("The learning rate.", sGDText0.learningRateTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText0.periodicPruningTipText());
      assertFalse(sGDText0.getLowercaseTokens());
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText0.tokenizerTipText());
      assertFalse(sGDText0.getNormalizeDocLength());
      assertFalse(sGDText0.getDebug());
      assertEquals(0, sGDText0.getPeriodicPruning());
      assertFalse(sGDText0.getUseStopList());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText0.minWordFrequencyTipText());
      assertEquals("The random number seed to be used.", sGDText0.seedTipText());
      assertEquals(2.0, sGDText0.getLNorm(), 0.01);
      assertEquals(1.0, sGDText0.getNorm(), 0.01);
      assertEquals("The norm of the instances after normalization.", sGDText0.normTipText());
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText0.lossFunctionTipText());
      assertEquals(1, sGDText0.getSeed());
      assertEquals("The LNorm to use for document length normalization.", sGDText0.LNormTipText());
      assertEquals(0.01, sGDText0.getLearningRate(), 0.01);
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText0.outputProbsForSVMTipText());
      assertEquals("The regularization constant. (default = 0.0001)", sGDText0.lambdaTipText());
      assertEquals(1.0E-4, sGDText0.getLambda(), 0.01);
      assertFalse(sGDText0.getUseWordFrequencies());
      assertFalse(sGDText0.getOutputProbsForSVM());
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText0.globalInfo());
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText0.useWordFrequenciesTipText());
      
      String string3 = naiveBayesMultinomialText0.periodicPruningTipText();
      assertNotSame(naiveBayesMultinomialText0, naiveBayesMultinomialText1);
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", string3);
      assertFalse(naiveBayesMultinomialText0.equals((Object)naiveBayesMultinomialText1));
      assertFalse(string3.equals((Object)string2));
      assertFalse(string3.equals((Object)string0));
      assertFalse(string3.equals((Object)string1));
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals((-1.0), naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertNotNull(string3);
      
      WordTokenizer wordTokenizer1 = (WordTokenizer)naiveBayesMultinomialText0.getTokenizer();
      assertNotSame(naiveBayesMultinomialText0, naiveBayesMultinomialText1);
      assertSame(wordTokenizer1, wordTokenizer0);
      assertFalse(naiveBayesMultinomialText0.equals((Object)naiveBayesMultinomialText1));
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals((-1.0), naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals(" \r\n\t.,;:'\"()?!", wordTokenizer1.getDelimiters());
      assertEquals("A simple tokenizer that is using the java.util.StringTokenizer class to tokenize the strings.", wordTokenizer1.globalInfo());
      assertEquals("Set of delimiter characters to use in tokenizing (\\r, \\n and \\t can be used for carriage-return, line-feed and tab)", wordTokenizer1.delimitersTipText());
      assertNotNull(wordTokenizer1);
      
      String string4 = naiveBayesMultinomialText0.getRevision();
      assertNotSame(naiveBayesMultinomialText0, naiveBayesMultinomialText1);
      assertEquals("9122", string4);
      assertFalse(naiveBayesMultinomialText0.equals((Object)naiveBayesMultinomialText1));
      assertFalse(string4.equals((Object)string3));
      assertFalse(string4.equals((Object)string0));
      assertFalse(string4.equals((Object)string1));
      assertFalse(string4.equals((Object)string2));
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals((-1.0), naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertNotNull(string4);
      
      String string5 = naiveBayesMultinomialText1.minWordFrequencyTipText();
      assertNotSame(naiveBayesMultinomialText1, naiveBayesMultinomialText0);
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", string5);
      assertFalse(naiveBayesMultinomialText1.equals((Object)naiveBayesMultinomialText0));
      assertFalse(string5.equals((Object)string3));
      assertFalse(string5.equals((Object)string0));
      assertFalse(string5.equals((Object)string1));
      assertFalse(string5.equals((Object)string2));
      assertFalse(string5.equals((Object)string4));
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText1.normalizeDocLengthTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText1.tokenizerTipText());
      assertEquals(3.0, naiveBayesMultinomialText1.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText1.LNormTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText1.lowercaseTokensTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText1.useWordFrequenciesTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText1.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText1.useStopListTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText1.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText1.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText1.getNormalizeDocLength());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText1.periodicPruningTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText1.stopwordsTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText1.normTipText());
      assertFalse(naiveBayesMultinomialText1.getDebug());
      assertFalse(naiveBayesMultinomialText1.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText1.globalInfo());
      assertEquals(0, naiveBayesMultinomialText1.getPeriodicPruning());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText1.debugTipText());
      assertEquals(1.0, naiveBayesMultinomialText1.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText1.getUseStopList());
      assertEquals(2.0, naiveBayesMultinomialText1.getLNorm(), 0.01);
      assertNotNull(string5);
      
      naiveBayesMultinomialText0.setLNorm((-2703.21318412));
      assertNotSame(naiveBayesMultinomialText0, naiveBayesMultinomialText1);
      assertFalse(naiveBayesMultinomialText0.equals((Object)naiveBayesMultinomialText1));
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals((-1.0), naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals((-2703.21318412), naiveBayesMultinomialText0.getLNorm(), 0.01);
  }

  /**
  //Test case number: 11
  /*Coverage entropy=3.178053830347946
  */
  @Test(timeout = 4000)
  public void test11()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertNotNull(naiveBayesMultinomialText0);
      
      double double0 = naiveBayesMultinomialText0.getLNorm();
      assertEquals(2.0, double0, 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      
      String[] stringArray0 = new String[5];
      naiveBayesMultinomialText0.setUseStopList(false);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      
      stringArray0[0] = "k=Ld";
      naiveBayesMultinomialText0.setPeriodicPruning((-3983));
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals((-3983), naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      
      stringArray0[1] = "-lowercase";
      naiveBayesMultinomialText0.m_wordFrequencies = true;
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals((-3983), naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertTrue(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      
      WordTokenizer wordTokenizer0 = (WordTokenizer)naiveBayesMultinomialText0.m_tokenizer;
      assertEquals("Set of delimiter characters to use in tokenizing (\\r, \\n and \\t can be used for carriage-return, line-feed and tab)", wordTokenizer0.delimitersTipText());
      assertEquals("A simple tokenizer that is using the java.util.StringTokenizer class to tokenize the strings.", wordTokenizer0.globalInfo());
      assertEquals(" \r\n\t.,;:'\"()?!", wordTokenizer0.getDelimiters());
      assertNotNull(wordTokenizer0);
      
      naiveBayesMultinomialText0.m_tokenizer = (Tokenizer) wordTokenizer0;
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals((-3983), naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertTrue(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Set of delimiter characters to use in tokenizing (\\r, \\n and \\t can be used for carriage-return, line-feed and tab)", wordTokenizer0.delimitersTipText());
      assertEquals("A simple tokenizer that is using the java.util.StringTokenizer class to tokenize the strings.", wordTokenizer0.globalInfo());
      assertEquals(" \r\n\t.,;:'\"()?!", wordTokenizer0.getDelimiters());
      
      stringArray0[2] = "aGzm`hNg|cgD#..";
      stringArray0[3] = "";
      naiveBayesMultinomialText0.setMinWordFrequency((-3983));
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals((-3983), naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals((-3983.0), naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertTrue(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      
      stringArray0[4] = "P";
      naiveBayesMultinomialText0.setUseWordFrequencies(true);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals((-3983), naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals((-3983.0), naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertTrue(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      
      NaiveBayesMultinomialText.main(stringArray0);
      assertEquals(5, stringArray0.length);
      
      String string0 = naiveBayesMultinomialText0.globalInfo();
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", string0);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals((-3983), naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals((-3983.0), naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertTrue(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertNotNull(string0);
      
      double double1 = naiveBayesMultinomialText0.getMinWordFrequency();
      assertEquals((-3983.0), double1, 0.01);
      assertNotEquals(double1, double0, 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals((-3983), naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals((-3983.0), naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertTrue(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      
      String string1 = naiveBayesMultinomialText0.normTipText();
      assertEquals("The norm of the instances after normalization.", string1);
      assertFalse(string1.equals((Object)string0));
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals((-3983), naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals((-3983.0), naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertTrue(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertNotNull(string1);
      
      String[] stringArray1 = naiveBayesMultinomialText0.getOptions();
      assertNotSame(stringArray1, stringArray0);
      assertFalse(stringArray1.equals((Object)stringArray0));
      assertEquals(13, stringArray1.length);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals((-3983), naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals((-3983.0), naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertTrue(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertNotNull(stringArray1);
      
      String string2 = naiveBayesMultinomialText0.LNormTipText();
      assertEquals("The LNorm to use for document length normalization.", string2);
      assertFalse(string2.equals((Object)string0));
      assertFalse(string2.equals((Object)string1));
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals((-3983), naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals((-3983.0), naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertTrue(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertNotNull(string2);
      
      naiveBayesMultinomialText0.setLNorm((-697.1));
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals((-3983), naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals((-697.1), naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals((-3983.0), naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertTrue(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      
      String string3 = naiveBayesMultinomialText0.stemmerTipText();
      assertEquals("The stemming algorithm to use on the words.", string3);
      assertFalse(string3.equals((Object)string0));
      assertFalse(string3.equals((Object)string1));
      assertFalse(string3.equals((Object)string2));
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals((-3983), naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals((-697.1), naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals((-3983.0), naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertTrue(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertNotNull(string3);
      
      File file0 = naiveBayesMultinomialText0.getStopwords();
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals((-3983), naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals((-697.1), naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals((-3983.0), naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertTrue(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("107_weka", file0.getName());
      assertEquals(0L, file0.length());
      assertTrue(file0.exists());
      assertEquals(1392409281320L, file0.lastModified());
      assertEquals("/home/ubuntu/evosuite_readability_gen/projects", file0.getParent());
      assertEquals(0L, file0.getTotalSpace());
      assertTrue(file0.canExecute());
      assertFalse(file0.isFile());
      assertTrue(file0.isAbsolute());
      assertTrue(file0.canRead());
      assertEquals(0L, file0.getFreeSpace());
      assertFalse(file0.isHidden());
      assertTrue(file0.canWrite());
      assertEquals("/home/ubuntu/evosuite_readability_gen/projects/107_weka", file0.toString());
      assertTrue(file0.isDirectory());
      assertEquals(0L, file0.getUsableSpace());
      assertNotNull(file0);
      
      Enumeration<Option> enumeration0 = naiveBayesMultinomialText0.listOptions();
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals((-3983), naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals((-697.1), naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals((-3983.0), naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertTrue(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertNotNull(enumeration0);
  }

  /**
  //Test case number: 12
  /*Coverage entropy=3.396633717042106
  */
  @Test(timeout = 4000)
  public void test12()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertNotNull(naiveBayesMultinomialText0);
      
      NaiveBayesMultinomialText naiveBayesMultinomialText1 = new NaiveBayesMultinomialText();
      assertFalse(naiveBayesMultinomialText1.equals((Object)naiveBayesMultinomialText0));
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText1.stopwordsTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText1.normTipText());
      assertFalse(naiveBayesMultinomialText1.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText1.globalInfo());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText1.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText1.stemmerTipText());
      assertFalse(naiveBayesMultinomialText1.getUseStopList());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText1.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText1.getPeriodicPruning());
      assertEquals(2.0, naiveBayesMultinomialText1.getLNorm(), 0.01);
      assertEquals(3.0, naiveBayesMultinomialText1.getMinWordFrequency(), 0.01);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText1.normalizeDocLengthTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText1.useStopListTipText());
      assertFalse(naiveBayesMultinomialText1.getLowercaseTokens());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText1.LNormTipText());
      assertEquals(1.0, naiveBayesMultinomialText1.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText1.getDebug());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText1.lowercaseTokensTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText1.periodicPruningTipText());
      assertFalse(naiveBayesMultinomialText1.getNormalizeDocLength());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText1.tokenizerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText1.minWordFrequencyTipText());
      assertNotNull(naiveBayesMultinomialText1);
      
      String[] stringArray0 = naiveBayesMultinomialText1.getOptions();
      assertNotSame(naiveBayesMultinomialText1, naiveBayesMultinomialText0);
      assertFalse(naiveBayesMultinomialText1.equals((Object)naiveBayesMultinomialText0));
      assertEquals(12, stringArray0.length);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText1.stopwordsTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText1.normTipText());
      assertFalse(naiveBayesMultinomialText1.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText1.globalInfo());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText1.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText1.stemmerTipText());
      assertFalse(naiveBayesMultinomialText1.getUseStopList());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText1.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText1.getPeriodicPruning());
      assertEquals(2.0, naiveBayesMultinomialText1.getLNorm(), 0.01);
      assertEquals(3.0, naiveBayesMultinomialText1.getMinWordFrequency(), 0.01);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText1.normalizeDocLengthTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText1.useStopListTipText());
      assertFalse(naiveBayesMultinomialText1.getLowercaseTokens());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText1.LNormTipText());
      assertEquals(1.0, naiveBayesMultinomialText1.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText1.getDebug());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText1.lowercaseTokensTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText1.periodicPruningTipText());
      assertFalse(naiveBayesMultinomialText1.getNormalizeDocLength());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText1.tokenizerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText1.minWordFrequencyTipText());
      assertNotNull(stringArray0);
      
      NaiveBayesMultinomialText naiveBayesMultinomialText2 = new NaiveBayesMultinomialText();
      assertFalse(naiveBayesMultinomialText2.equals((Object)naiveBayesMultinomialText0));
      assertFalse(naiveBayesMultinomialText2.equals((Object)naiveBayesMultinomialText1));
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText2.normTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText2.stopwordsTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText2.debugTipText());
      assertFalse(naiveBayesMultinomialText2.getUseWordFrequencies());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText2.stemmerTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText2.globalInfo());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText2.LNormTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText2.lowercaseTokensTipText());
      assertEquals(3.0, naiveBayesMultinomialText2.getMinWordFrequency(), 0.01);
      assertFalse(naiveBayesMultinomialText2.getDebug());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText2.tokenizerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText2.minWordFrequencyTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText2.periodicPruningTipText());
      assertFalse(naiveBayesMultinomialText2.getNormalizeDocLength());
      assertEquals(2.0, naiveBayesMultinomialText2.getLNorm(), 0.01);
      assertEquals(0, naiveBayesMultinomialText2.getPeriodicPruning());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText2.useStopListTipText());
      assertFalse(naiveBayesMultinomialText2.getLowercaseTokens());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText2.normalizeDocLengthTipText());
      assertEquals(1.0, naiveBayesMultinomialText2.getNorm(), 0.01);
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText2.useWordFrequenciesTipText());
      assertFalse(naiveBayesMultinomialText2.getUseStopList());
      assertNotNull(naiveBayesMultinomialText2);
      
      naiveBayesMultinomialText2.setOptions(stringArray0);
      assertNotSame(naiveBayesMultinomialText1, naiveBayesMultinomialText0);
      assertNotSame(naiveBayesMultinomialText1, naiveBayesMultinomialText2);
      assertNotSame(naiveBayesMultinomialText2, naiveBayesMultinomialText0);
      assertNotSame(naiveBayesMultinomialText2, naiveBayesMultinomialText1);
      assertFalse(naiveBayesMultinomialText1.equals((Object)naiveBayesMultinomialText0));
      assertFalse(naiveBayesMultinomialText1.equals((Object)naiveBayesMultinomialText2));
      assertFalse(naiveBayesMultinomialText2.equals((Object)naiveBayesMultinomialText0));
      assertFalse(naiveBayesMultinomialText2.equals((Object)naiveBayesMultinomialText1));
      assertEquals(12, stringArray0.length);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText1.stopwordsTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText1.normTipText());
      assertFalse(naiveBayesMultinomialText1.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText1.globalInfo());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText1.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText1.stemmerTipText());
      assertFalse(naiveBayesMultinomialText1.getUseStopList());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText1.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText1.getPeriodicPruning());
      assertEquals(2.0, naiveBayesMultinomialText1.getLNorm(), 0.01);
      assertEquals(3.0, naiveBayesMultinomialText1.getMinWordFrequency(), 0.01);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText1.normalizeDocLengthTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText1.useStopListTipText());
      assertFalse(naiveBayesMultinomialText1.getLowercaseTokens());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText1.LNormTipText());
      assertEquals(1.0, naiveBayesMultinomialText1.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText1.getDebug());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText1.lowercaseTokensTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText1.periodicPruningTipText());
      assertFalse(naiveBayesMultinomialText1.getNormalizeDocLength());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText1.tokenizerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText1.minWordFrequencyTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText2.normTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText2.stopwordsTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText2.debugTipText());
      assertFalse(naiveBayesMultinomialText2.getUseWordFrequencies());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText2.stemmerTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText2.globalInfo());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText2.LNormTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText2.lowercaseTokensTipText());
      assertEquals(3.0, naiveBayesMultinomialText2.getMinWordFrequency(), 0.01);
      assertFalse(naiveBayesMultinomialText2.getDebug());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText2.tokenizerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText2.minWordFrequencyTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText2.periodicPruningTipText());
      assertFalse(naiveBayesMultinomialText2.getNormalizeDocLength());
      assertEquals(2.0, naiveBayesMultinomialText2.getLNorm(), 0.01);
      assertEquals(0, naiveBayesMultinomialText2.getPeriodicPruning());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText2.useStopListTipText());
      assertFalse(naiveBayesMultinomialText2.getLowercaseTokens());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText2.normalizeDocLengthTipText());
      assertEquals(1.0, naiveBayesMultinomialText2.getNorm(), 0.01);
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText2.useWordFrequenciesTipText());
      assertFalse(naiveBayesMultinomialText2.getUseStopList());
      
      NaiveBayesMultinomialText naiveBayesMultinomialText3 = new NaiveBayesMultinomialText();
      assertFalse(naiveBayesMultinomialText3.equals((Object)naiveBayesMultinomialText0));
      assertFalse(naiveBayesMultinomialText3.equals((Object)naiveBayesMultinomialText2));
      assertFalse(naiveBayesMultinomialText3.equals((Object)naiveBayesMultinomialText1));
      assertEquals(0, naiveBayesMultinomialText3.getPeriodicPruning());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText3.useStopListTipText());
      assertFalse(naiveBayesMultinomialText3.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText3.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText3.getUseStopList());
      assertEquals(2.0, naiveBayesMultinomialText3.getLNorm(), 0.01);
      assertEquals(3.0, naiveBayesMultinomialText3.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText3.LNormTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText3.useWordFrequenciesTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText3.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText3.stemmerTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText3.stopwordsTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText3.minWordFrequencyTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText3.periodicPruningTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText3.tokenizerTipText());
      assertEquals(1.0, naiveBayesMultinomialText3.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText3.getDebug());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText3.lowercaseTokensTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText3.normalizeDocLengthTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText3.normTipText());
      assertFalse(naiveBayesMultinomialText3.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText3.globalInfo());
      assertNotNull(naiveBayesMultinomialText3);
      
      String string0 = naiveBayesMultinomialText3.useStopListTipText();
      assertNotSame(naiveBayesMultinomialText3, naiveBayesMultinomialText0);
      assertNotSame(naiveBayesMultinomialText3, naiveBayesMultinomialText2);
      assertNotSame(naiveBayesMultinomialText3, naiveBayesMultinomialText1);
      assertEquals("If true, ignores all words that are on the stoplist.", string0);
      assertFalse(naiveBayesMultinomialText3.equals((Object)naiveBayesMultinomialText0));
      assertFalse(naiveBayesMultinomialText3.equals((Object)naiveBayesMultinomialText2));
      assertFalse(naiveBayesMultinomialText3.equals((Object)naiveBayesMultinomialText1));
      assertEquals(0, naiveBayesMultinomialText3.getPeriodicPruning());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText3.useStopListTipText());
      assertFalse(naiveBayesMultinomialText3.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText3.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText3.getUseStopList());
      assertEquals(2.0, naiveBayesMultinomialText3.getLNorm(), 0.01);
      assertEquals(3.0, naiveBayesMultinomialText3.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText3.LNormTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText3.useWordFrequenciesTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText3.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText3.stemmerTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText3.stopwordsTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText3.minWordFrequencyTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText3.periodicPruningTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText3.tokenizerTipText());
      assertEquals(1.0, naiveBayesMultinomialText3.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText3.getDebug());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText3.lowercaseTokensTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText3.normalizeDocLengthTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText3.normTipText());
      assertFalse(naiveBayesMultinomialText3.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText3.globalInfo());
      assertNotNull(string0);
      
      NaiveBayesMultinomialText.main(stringArray0);
      assertNotSame(naiveBayesMultinomialText1, naiveBayesMultinomialText0);
      assertNotSame(naiveBayesMultinomialText1, naiveBayesMultinomialText3);
      assertNotSame(naiveBayesMultinomialText1, naiveBayesMultinomialText2);
      assertFalse(naiveBayesMultinomialText1.equals((Object)naiveBayesMultinomialText0));
      assertFalse(naiveBayesMultinomialText1.equals((Object)naiveBayesMultinomialText3));
      assertFalse(naiveBayesMultinomialText1.equals((Object)naiveBayesMultinomialText2));
      assertEquals(12, stringArray0.length);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText1.stopwordsTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText1.normTipText());
      assertFalse(naiveBayesMultinomialText1.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText1.globalInfo());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText1.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText1.stemmerTipText());
      assertFalse(naiveBayesMultinomialText1.getUseStopList());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText1.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText1.getPeriodicPruning());
      assertEquals(2.0, naiveBayesMultinomialText1.getLNorm(), 0.01);
      assertEquals(3.0, naiveBayesMultinomialText1.getMinWordFrequency(), 0.01);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText1.normalizeDocLengthTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText1.useStopListTipText());
      assertFalse(naiveBayesMultinomialText1.getLowercaseTokens());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText1.LNormTipText());
      assertEquals(1.0, naiveBayesMultinomialText1.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText1.getDebug());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText1.lowercaseTokensTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText1.periodicPruningTipText());
      assertFalse(naiveBayesMultinomialText1.getNormalizeDocLength());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText1.tokenizerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText1.minWordFrequencyTipText());
      
      File file0 = naiveBayesMultinomialText2.getStopwords();
      assertNotSame(naiveBayesMultinomialText2, naiveBayesMultinomialText0);
      assertNotSame(naiveBayesMultinomialText2, naiveBayesMultinomialText3);
      assertNotSame(naiveBayesMultinomialText2, naiveBayesMultinomialText1);
      assertFalse(naiveBayesMultinomialText2.equals((Object)naiveBayesMultinomialText0));
      assertFalse(naiveBayesMultinomialText2.equals((Object)naiveBayesMultinomialText3));
      assertFalse(naiveBayesMultinomialText2.equals((Object)naiveBayesMultinomialText1));
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText2.normTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText2.stopwordsTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText2.debugTipText());
      assertFalse(naiveBayesMultinomialText2.getUseWordFrequencies());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText2.stemmerTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText2.globalInfo());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText2.LNormTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText2.lowercaseTokensTipText());
      assertEquals(3.0, naiveBayesMultinomialText2.getMinWordFrequency(), 0.01);
      assertFalse(naiveBayesMultinomialText2.getDebug());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText2.tokenizerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText2.minWordFrequencyTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText2.periodicPruningTipText());
      assertFalse(naiveBayesMultinomialText2.getNormalizeDocLength());
      assertEquals(2.0, naiveBayesMultinomialText2.getLNorm(), 0.01);
      assertEquals(0, naiveBayesMultinomialText2.getPeriodicPruning());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText2.useStopListTipText());
      assertFalse(naiveBayesMultinomialText2.getLowercaseTokens());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText2.normalizeDocLengthTipText());
      assertEquals(1.0, naiveBayesMultinomialText2.getNorm(), 0.01);
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText2.useWordFrequenciesTipText());
      assertFalse(naiveBayesMultinomialText2.getUseStopList());
      assertFalse(file0.isHidden());
      assertTrue(file0.canWrite());
      assertEquals(0L, file0.getUsableSpace());
      assertEquals("/home/ubuntu/evosuite_readability_gen/projects", file0.getParent());
      assertTrue(file0.canExecute());
      assertFalse(file0.isFile());
      assertEquals("107_weka", file0.getName());
      assertEquals("/home/ubuntu/evosuite_readability_gen/projects/107_weka", file0.toString());
      assertTrue(file0.isDirectory());
      assertTrue(file0.isAbsolute());
      assertEquals(0L, file0.length());
      assertTrue(file0.exists());
      assertEquals(0L, file0.getFreeSpace());
      assertEquals(0L, file0.getTotalSpace());
      assertTrue(file0.canRead());
      assertEquals(1392409281320L, file0.lastModified());
      assertNotNull(file0);
      
      naiveBayesMultinomialText1.pruneDictionary();
      assertNotSame(naiveBayesMultinomialText1, naiveBayesMultinomialText0);
      assertNotSame(naiveBayesMultinomialText1, naiveBayesMultinomialText3);
      assertNotSame(naiveBayesMultinomialText1, naiveBayesMultinomialText2);
      assertFalse(naiveBayesMultinomialText1.equals((Object)naiveBayesMultinomialText0));
      assertFalse(naiveBayesMultinomialText1.equals((Object)naiveBayesMultinomialText3));
      assertFalse(naiveBayesMultinomialText1.equals((Object)naiveBayesMultinomialText2));
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText1.stopwordsTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText1.normTipText());
      assertFalse(naiveBayesMultinomialText1.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText1.globalInfo());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText1.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText1.stemmerTipText());
      assertFalse(naiveBayesMultinomialText1.getUseStopList());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText1.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText1.getPeriodicPruning());
      assertEquals(2.0, naiveBayesMultinomialText1.getLNorm(), 0.01);
      assertEquals(3.0, naiveBayesMultinomialText1.getMinWordFrequency(), 0.01);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText1.normalizeDocLengthTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText1.useStopListTipText());
      assertFalse(naiveBayesMultinomialText1.getLowercaseTokens());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText1.LNormTipText());
      assertEquals(1.0, naiveBayesMultinomialText1.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText1.getDebug());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText1.lowercaseTokensTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText1.periodicPruningTipText());
      assertFalse(naiveBayesMultinomialText1.getNormalizeDocLength());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText1.tokenizerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText1.minWordFrequencyTipText());
      
      String string1 = naiveBayesMultinomialText1.minWordFrequencyTipText();
      assertNotSame(naiveBayesMultinomialText1, naiveBayesMultinomialText0);
      assertNotSame(naiveBayesMultinomialText1, naiveBayesMultinomialText3);
      assertNotSame(naiveBayesMultinomialText1, naiveBayesMultinomialText2);
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", string1);
      assertFalse(naiveBayesMultinomialText1.equals((Object)naiveBayesMultinomialText0));
      assertFalse(naiveBayesMultinomialText1.equals((Object)naiveBayesMultinomialText3));
      assertFalse(naiveBayesMultinomialText1.equals((Object)naiveBayesMultinomialText2));
      assertFalse(string1.equals((Object)string0));
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText1.stopwordsTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText1.normTipText());
      assertFalse(naiveBayesMultinomialText1.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText1.globalInfo());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText1.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText1.stemmerTipText());
      assertFalse(naiveBayesMultinomialText1.getUseStopList());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText1.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText1.getPeriodicPruning());
      assertEquals(2.0, naiveBayesMultinomialText1.getLNorm(), 0.01);
      assertEquals(3.0, naiveBayesMultinomialText1.getMinWordFrequency(), 0.01);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText1.normalizeDocLengthTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText1.useStopListTipText());
      assertFalse(naiveBayesMultinomialText1.getLowercaseTokens());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText1.LNormTipText());
      assertEquals(1.0, naiveBayesMultinomialText1.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText1.getDebug());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText1.lowercaseTokensTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText1.periodicPruningTipText());
      assertFalse(naiveBayesMultinomialText1.getNormalizeDocLength());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText1.tokenizerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText1.minWordFrequencyTipText());
      assertNotNull(string1);
  }

  /**
  //Test case number: 13
  /*Coverage entropy=3.0280290733074104
  */
  @Test(timeout = 4000)
  public void test13()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertNotNull(naiveBayesMultinomialText0);
      
      TestInstances testInstances0 = new TestInstances();
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertFalse(testInstances0.getNoClass());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertNotNull(testInstances0);
      
      testInstances0.setRelation("RG1=e66u2");
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertFalse(testInstances0.getNoClass());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(1, testInstances0.getSeed());
      assertEquals("RG1=e66u2", testInstances0.getRelation());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      
      NaiveBayesMultinomialText naiveBayesMultinomialText1 = new NaiveBayesMultinomialText();
      assertFalse(naiveBayesMultinomialText1.equals((Object)naiveBayesMultinomialText0));
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText1.tokenizerTipText());
      assertFalse(naiveBayesMultinomialText1.getNormalizeDocLength());
      assertEquals(2.0, naiveBayesMultinomialText1.getLNorm(), 0.01);
      assertEquals(3.0, naiveBayesMultinomialText1.getMinWordFrequency(), 0.01);
      assertEquals(1.0, naiveBayesMultinomialText1.getNorm(), 0.01);
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText1.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText1.getDebug());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText1.debugTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText1.lowercaseTokensTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText1.globalInfo());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText1.normalizeDocLengthTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText1.normTipText());
      assertFalse(naiveBayesMultinomialText1.getUseWordFrequencies());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText1.stopwordsTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText1.periodicPruningTipText());
      assertFalse(naiveBayesMultinomialText1.getUseStopList());
      assertEquals(0, naiveBayesMultinomialText1.getPeriodicPruning());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText1.useStopListTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText1.stemmerTipText());
      assertFalse(naiveBayesMultinomialText1.getLowercaseTokens());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText1.LNormTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText1.useWordFrequenciesTipText());
      assertNotNull(naiveBayesMultinomialText1);
      
      LinkedHashMap<String, NaiveBayesMultinomialText.Count> linkedHashMap0 = naiveBayesMultinomialText1.m_inputVector;
      assertNull(linkedHashMap0);
      
      naiveBayesMultinomialText0.m_inputVector = null;
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      
      SGDText sGDText0 = new SGDText();
      assertEquals(1, SGDText.LOGLOSS);
      assertEquals(0, SGDText.HINGE);
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText0.lossFunctionTipText());
      assertFalse(sGDText0.getOutputProbsForSVM());
      assertFalse(sGDText0.getUseStopList());
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText0.globalInfo());
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText0.useWordFrequenciesTipText());
      assertEquals(1.0, sGDText0.getNorm(), 0.01);
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText0.outputProbsForSVMTipText());
      assertFalse(sGDText0.getLowercaseTokens());
      assertEquals("Whether to convert all tokens to lowercase", sGDText0.lowercaseTokensTipText());
      assertFalse(sGDText0.getUseWordFrequencies());
      assertEquals("The regularization constant. (default = 0.0001)", sGDText0.lambdaTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText0.stopwordsTipText());
      assertEquals("The stemming algorithm to use on the words.", sGDText0.stemmerTipText());
      assertEquals(3.0, sGDText0.getMinWordFrequency(), 0.01);
      assertEquals(500, sGDText0.getEpochs());
      assertEquals(2.0, sGDText0.getLNorm(), 0.01);
      assertEquals(0, sGDText0.getPeriodicPruning());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText0.epochsTipText());
      assertEquals("The random number seed to be used.", sGDText0.seedTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText0.debugTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText0.minWordFrequencyTipText());
      assertFalse(sGDText0.getDebug());
      assertEquals(0.01, sGDText0.getLearningRate(), 0.01);
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText0.tokenizerTipText());
      assertFalse(sGDText0.getNormalizeDocLength());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText0.periodicPruningTipText());
      assertEquals(1.0E-4, sGDText0.getLambda(), 0.01);
      assertEquals("The learning rate.", sGDText0.learningRateTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText0.normalizeDocLengthTipText());
      assertEquals(1, sGDText0.getSeed());
      assertEquals("The norm of the instances after normalization.", sGDText0.normTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText0.useStopListTipText());
      assertEquals("The LNorm to use for document length normalization.", sGDText0.LNormTipText());
      assertNotNull(sGDText0);
      
      WordTokenizer wordTokenizer0 = (WordTokenizer)sGDText0.getTokenizer();
      assertEquals(1, SGDText.LOGLOSS);
      assertEquals(0, SGDText.HINGE);
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText0.lossFunctionTipText());
      assertFalse(sGDText0.getOutputProbsForSVM());
      assertFalse(sGDText0.getUseStopList());
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText0.globalInfo());
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText0.useWordFrequenciesTipText());
      assertEquals(1.0, sGDText0.getNorm(), 0.01);
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText0.outputProbsForSVMTipText());
      assertFalse(sGDText0.getLowercaseTokens());
      assertEquals("Whether to convert all tokens to lowercase", sGDText0.lowercaseTokensTipText());
      assertFalse(sGDText0.getUseWordFrequencies());
      assertEquals("The regularization constant. (default = 0.0001)", sGDText0.lambdaTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText0.stopwordsTipText());
      assertEquals("The stemming algorithm to use on the words.", sGDText0.stemmerTipText());
      assertEquals(3.0, sGDText0.getMinWordFrequency(), 0.01);
      assertEquals(500, sGDText0.getEpochs());
      assertEquals(2.0, sGDText0.getLNorm(), 0.01);
      assertEquals(0, sGDText0.getPeriodicPruning());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText0.epochsTipText());
      assertEquals("The random number seed to be used.", sGDText0.seedTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText0.debugTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText0.minWordFrequencyTipText());
      assertFalse(sGDText0.getDebug());
      assertEquals(0.01, sGDText0.getLearningRate(), 0.01);
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText0.tokenizerTipText());
      assertFalse(sGDText0.getNormalizeDocLength());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText0.periodicPruningTipText());
      assertEquals(1.0E-4, sGDText0.getLambda(), 0.01);
      assertEquals("The learning rate.", sGDText0.learningRateTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText0.normalizeDocLengthTipText());
      assertEquals(1, sGDText0.getSeed());
      assertEquals("The norm of the instances after normalization.", sGDText0.normTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText0.useStopListTipText());
      assertEquals("The LNorm to use for document length normalization.", sGDText0.LNormTipText());
      assertEquals("Set of delimiter characters to use in tokenizing (\\r, \\n and \\t can be used for carriage-return, line-feed and tab)", wordTokenizer0.delimitersTipText());
      assertEquals(" \r\n\t.,;:'\"()?!", wordTokenizer0.getDelimiters());
      assertEquals("A simple tokenizer that is using the java.util.StringTokenizer class to tokenize the strings.", wordTokenizer0.globalInfo());
      assertNotNull(wordTokenizer0);
      
      naiveBayesMultinomialText0.m_tokenizer = (Tokenizer) wordTokenizer0;
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText0.lossFunctionTipText());
      assertFalse(sGDText0.getOutputProbsForSVM());
      assertFalse(sGDText0.getUseStopList());
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText0.globalInfo());
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText0.useWordFrequenciesTipText());
      assertEquals(1.0, sGDText0.getNorm(), 0.01);
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText0.outputProbsForSVMTipText());
      assertFalse(sGDText0.getLowercaseTokens());
      assertEquals("Whether to convert all tokens to lowercase", sGDText0.lowercaseTokensTipText());
      assertFalse(sGDText0.getUseWordFrequencies());
      assertEquals("The regularization constant. (default = 0.0001)", sGDText0.lambdaTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText0.stopwordsTipText());
      assertEquals("The stemming algorithm to use on the words.", sGDText0.stemmerTipText());
      assertEquals(3.0, sGDText0.getMinWordFrequency(), 0.01);
      assertEquals(500, sGDText0.getEpochs());
      assertEquals(2.0, sGDText0.getLNorm(), 0.01);
      assertEquals(0, sGDText0.getPeriodicPruning());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText0.epochsTipText());
      assertEquals("The random number seed to be used.", sGDText0.seedTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText0.debugTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText0.minWordFrequencyTipText());
      assertFalse(sGDText0.getDebug());
      assertEquals(0.01, sGDText0.getLearningRate(), 0.01);
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText0.tokenizerTipText());
      assertFalse(sGDText0.getNormalizeDocLength());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText0.periodicPruningTipText());
      assertEquals(1.0E-4, sGDText0.getLambda(), 0.01);
      assertEquals("The learning rate.", sGDText0.learningRateTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText0.normalizeDocLengthTipText());
      assertEquals(1, sGDText0.getSeed());
      assertEquals("The norm of the instances after normalization.", sGDText0.normTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText0.useStopListTipText());
      assertEquals("The LNorm to use for document length normalization.", sGDText0.LNormTipText());
      assertEquals("Set of delimiter characters to use in tokenizing (\\r, \\n and \\t can be used for carriage-return, line-feed and tab)", wordTokenizer0.delimitersTipText());
      assertEquals(" \r\n\t.,;:'\"()?!", wordTokenizer0.getDelimiters());
      assertEquals("A simple tokenizer that is using the java.util.StringTokenizer class to tokenize the strings.", wordTokenizer0.globalInfo());
      
      naiveBayesMultinomialText0.m_lowercaseTokens = false;
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      
      TestInstances.main(testInstances0.DEFAULT_WORDS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertFalse(testInstances0.getNoClass());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(1, testInstances0.getSeed());
      assertEquals("RG1=e66u2", testInstances0.getRelation());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      
      Instances instances0 = testInstances0.generate("");
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertFalse(testInstances0.getNoClass());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(1, testInstances0.getSeed());
      assertEquals("RG1=e66u2", testInstances0.getRelation());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals("RG1=e66u2", instances0.relationName());
      assertEquals(20, instances0.numInstances());
      assertEquals(1, instances0.classIndex());
      assertEquals(2, instances0.numClasses());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20, instances0.size());
      assertEquals(2, instances0.numAttributes());
      assertNotNull(instances0);
      
      NullStemmer nullStemmer0 = new NullStemmer();
      assertEquals("A dummy stemmer that performs no stemming at all.", nullStemmer0.globalInfo());
      assertNotNull(nullStemmer0);
      
      boolean boolean0 = instances0.remove((Object) nullStemmer0);
      assertFalse(boolean0);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertFalse(testInstances0.getNoClass());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(1, testInstances0.getSeed());
      assertEquals("RG1=e66u2", testInstances0.getRelation());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals("RG1=e66u2", instances0.relationName());
      assertEquals(20, instances0.numInstances());
      assertEquals(1, instances0.classIndex());
      assertEquals(2, instances0.numClasses());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20, instances0.size());
      assertEquals(2, instances0.numAttributes());
      assertEquals("A dummy stemmer that performs no stemming at all.", nullStemmer0.globalInfo());
      
      naiveBayesMultinomialText0.buildClassifier(instances0);
      assertNotSame(naiveBayesMultinomialText0, naiveBayesMultinomialText1);
      assertFalse(naiveBayesMultinomialText0.equals((Object)naiveBayesMultinomialText1));
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertFalse(testInstances0.getNoClass());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(1, testInstances0.getSeed());
      assertEquals("RG1=e66u2", testInstances0.getRelation());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals("RG1=e66u2", instances0.relationName());
      assertEquals(20, instances0.numInstances());
      assertEquals(1, instances0.classIndex());
      assertEquals(2, instances0.numClasses());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20, instances0.size());
      assertEquals(2, instances0.numAttributes());
      
      naiveBayesMultinomialText0.setPeriodicPruning(0);
      assertNotSame(naiveBayesMultinomialText0, naiveBayesMultinomialText1);
      assertFalse(naiveBayesMultinomialText0.equals((Object)naiveBayesMultinomialText1));
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      
      NaiveBayesMultinomialText.main(testInstances0.DEFAULT_WORDS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertFalse(testInstances0.getNoClass());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(1, testInstances0.getSeed());
      assertEquals("RG1=e66u2", testInstances0.getRelation());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      
      String string0 = naiveBayesMultinomialText0.tokenizerTipText();
      assertNotSame(naiveBayesMultinomialText0, naiveBayesMultinomialText1);
      assertEquals("The tokenizing algorithm to use on the strings.", string0);
      assertFalse(naiveBayesMultinomialText0.equals((Object)naiveBayesMultinomialText1));
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertNotNull(string0);
      
      boolean boolean1 = naiveBayesMultinomialText0.getUseStopList();
      assertNotSame(naiveBayesMultinomialText0, naiveBayesMultinomialText1);
      assertFalse(boolean1);
      assertFalse(naiveBayesMultinomialText0.equals((Object)naiveBayesMultinomialText1));
      assertTrue(boolean1 == boolean0);
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      
      naiveBayesMultinomialText0.setNorm((-1));
      assertNotSame(naiveBayesMultinomialText0, naiveBayesMultinomialText1);
      assertFalse(naiveBayesMultinomialText0.equals((Object)naiveBayesMultinomialText1));
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals((-1.0), naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      
      String string1 = naiveBayesMultinomialText0.lowercaseTokensTipText();
      assertNotSame(naiveBayesMultinomialText0, naiveBayesMultinomialText1);
      assertEquals("Whether to convert all tokens to lowercase", string1);
      assertFalse(naiveBayesMultinomialText0.equals((Object)naiveBayesMultinomialText1));
      assertFalse(string1.equals((Object)string0));
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals((-1.0), naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertNotNull(string1);
      
      naiveBayesMultinomialText0.pruneDictionary();
      assertNotSame(naiveBayesMultinomialText0, naiveBayesMultinomialText1);
      assertFalse(naiveBayesMultinomialText0.equals((Object)naiveBayesMultinomialText1));
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals((-1.0), naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      
      String string2 = naiveBayesMultinomialText0.stopwordsTipText();
      assertNotSame(naiveBayesMultinomialText0, naiveBayesMultinomialText1);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", string2);
      assertFalse(naiveBayesMultinomialText0.equals((Object)naiveBayesMultinomialText1));
      assertFalse(string2.equals((Object)string0));
      assertFalse(string2.equals((Object)string1));
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals((-1.0), naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertNotNull(string2);
      
      sGDText0.setPeriodicPruning(0);
      assertEquals(1, SGDText.LOGLOSS);
      assertEquals(0, SGDText.HINGE);
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText0.lossFunctionTipText());
      assertFalse(sGDText0.getOutputProbsForSVM());
      assertFalse(sGDText0.getUseStopList());
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText0.globalInfo());
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText0.useWordFrequenciesTipText());
      assertEquals(1.0, sGDText0.getNorm(), 0.01);
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText0.outputProbsForSVMTipText());
      assertFalse(sGDText0.getLowercaseTokens());
      assertEquals("Whether to convert all tokens to lowercase", sGDText0.lowercaseTokensTipText());
      assertFalse(sGDText0.getUseWordFrequencies());
      assertEquals("The regularization constant. (default = 0.0001)", sGDText0.lambdaTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText0.stopwordsTipText());
      assertEquals("The stemming algorithm to use on the words.", sGDText0.stemmerTipText());
      assertEquals(3.0, sGDText0.getMinWordFrequency(), 0.01);
      assertEquals(500, sGDText0.getEpochs());
      assertEquals(2.0, sGDText0.getLNorm(), 0.01);
      assertEquals(0, sGDText0.getPeriodicPruning());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText0.epochsTipText());
      assertEquals("The random number seed to be used.", sGDText0.seedTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText0.debugTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText0.minWordFrequencyTipText());
      assertFalse(sGDText0.getDebug());
      assertEquals(0.01, sGDText0.getLearningRate(), 0.01);
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText0.tokenizerTipText());
      assertFalse(sGDText0.getNormalizeDocLength());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText0.periodicPruningTipText());
      assertEquals(1.0E-4, sGDText0.getLambda(), 0.01);
      assertEquals("The learning rate.", sGDText0.learningRateTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText0.normalizeDocLengthTipText());
      assertEquals(1, sGDText0.getSeed());
      assertEquals("The norm of the instances after normalization.", sGDText0.normTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText0.useStopListTipText());
      assertEquals("The LNorm to use for document length normalization.", sGDText0.LNormTipText());
      
      String string3 = naiveBayesMultinomialText0.periodicPruningTipText();
      assertNotSame(naiveBayesMultinomialText0, naiveBayesMultinomialText1);
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", string3);
      assertFalse(naiveBayesMultinomialText0.equals((Object)naiveBayesMultinomialText1));
      assertFalse(string3.equals((Object)string2));
      assertFalse(string3.equals((Object)string0));
      assertFalse(string3.equals((Object)string1));
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals((-1.0), naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertNotNull(string3);
      
      WordTokenizer wordTokenizer1 = (WordTokenizer)naiveBayesMultinomialText0.getTokenizer();
      assertNotSame(naiveBayesMultinomialText0, naiveBayesMultinomialText1);
      assertSame(wordTokenizer1, wordTokenizer0);
      assertFalse(naiveBayesMultinomialText0.equals((Object)naiveBayesMultinomialText1));
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals((-1.0), naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("A simple tokenizer that is using the java.util.StringTokenizer class to tokenize the strings.", wordTokenizer1.globalInfo());
      assertEquals("Set of delimiter characters to use in tokenizing (\\r, \\n and \\t can be used for carriage-return, line-feed and tab)", wordTokenizer1.delimitersTipText());
      assertEquals(" \r\n\t.,;:'\"()?!", wordTokenizer1.getDelimiters());
      assertNotNull(wordTokenizer1);
      
      String string4 = naiveBayesMultinomialText1.debugTipText();
      assertNotSame(naiveBayesMultinomialText1, naiveBayesMultinomialText0);
      assertEquals("If set to true, classifier may output additional info to the console.", string4);
      assertFalse(naiveBayesMultinomialText1.equals((Object)naiveBayesMultinomialText0));
      assertFalse(string4.equals((Object)string3));
      assertFalse(string4.equals((Object)string1));
      assertFalse(string4.equals((Object)string0));
      assertFalse(string4.equals((Object)string2));
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText1.tokenizerTipText());
      assertFalse(naiveBayesMultinomialText1.getNormalizeDocLength());
      assertEquals(2.0, naiveBayesMultinomialText1.getLNorm(), 0.01);
      assertEquals(3.0, naiveBayesMultinomialText1.getMinWordFrequency(), 0.01);
      assertEquals(1.0, naiveBayesMultinomialText1.getNorm(), 0.01);
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText1.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText1.getDebug());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText1.debugTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText1.lowercaseTokensTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText1.globalInfo());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText1.normalizeDocLengthTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText1.normTipText());
      assertFalse(naiveBayesMultinomialText1.getUseWordFrequencies());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText1.stopwordsTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText1.periodicPruningTipText());
      assertFalse(naiveBayesMultinomialText1.getUseStopList());
      assertEquals(0, naiveBayesMultinomialText1.getPeriodicPruning());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText1.useStopListTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText1.stemmerTipText());
      assertFalse(naiveBayesMultinomialText1.getLowercaseTokens());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText1.LNormTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText1.useWordFrequenciesTipText());
      assertNotNull(string4);
      
      String string5 = naiveBayesMultinomialText1.minWordFrequencyTipText();
      assertNotSame(naiveBayesMultinomialText1, naiveBayesMultinomialText0);
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", string5);
      assertFalse(naiveBayesMultinomialText1.equals((Object)naiveBayesMultinomialText0));
      assertFalse(string5.equals((Object)string3));
      assertFalse(string5.equals((Object)string0));
      assertFalse(string5.equals((Object)string2));
      assertFalse(string5.equals((Object)string1));
      assertFalse(string5.equals((Object)string4));
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText1.tokenizerTipText());
      assertFalse(naiveBayesMultinomialText1.getNormalizeDocLength());
      assertEquals(2.0, naiveBayesMultinomialText1.getLNorm(), 0.01);
      assertEquals(3.0, naiveBayesMultinomialText1.getMinWordFrequency(), 0.01);
      assertEquals(1.0, naiveBayesMultinomialText1.getNorm(), 0.01);
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText1.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText1.getDebug());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText1.debugTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText1.lowercaseTokensTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText1.globalInfo());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText1.normalizeDocLengthTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText1.normTipText());
      assertFalse(naiveBayesMultinomialText1.getUseWordFrequencies());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText1.stopwordsTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText1.periodicPruningTipText());
      assertFalse(naiveBayesMultinomialText1.getUseStopList());
      assertEquals(0, naiveBayesMultinomialText1.getPeriodicPruning());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText1.useStopListTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText1.stemmerTipText());
      assertFalse(naiveBayesMultinomialText1.getLowercaseTokens());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText1.LNormTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText1.useWordFrequenciesTipText());
      assertNotNull(string5);
      
      naiveBayesMultinomialText0.setLNorm((-2703.21318412));
      assertNotSame(naiveBayesMultinomialText0, naiveBayesMultinomialText1);
      assertFalse(naiveBayesMultinomialText0.equals((Object)naiveBayesMultinomialText1));
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals((-2703.21318412), naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals((-1.0), naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      
      Capabilities capabilities0 = sGDText0.getCapabilities();
      assertEquals(1, SGDText.LOGLOSS);
      assertEquals(0, SGDText.HINGE);
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText0.lossFunctionTipText());
      assertFalse(sGDText0.getOutputProbsForSVM());
      assertFalse(sGDText0.getUseStopList());
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText0.globalInfo());
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText0.useWordFrequenciesTipText());
      assertEquals(1.0, sGDText0.getNorm(), 0.01);
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText0.outputProbsForSVMTipText());
      assertFalse(sGDText0.getLowercaseTokens());
      assertEquals("Whether to convert all tokens to lowercase", sGDText0.lowercaseTokensTipText());
      assertFalse(sGDText0.getUseWordFrequencies());
      assertEquals("The regularization constant. (default = 0.0001)", sGDText0.lambdaTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText0.stopwordsTipText());
      assertEquals("The stemming algorithm to use on the words.", sGDText0.stemmerTipText());
      assertEquals(3.0, sGDText0.getMinWordFrequency(), 0.01);
      assertEquals(500, sGDText0.getEpochs());
      assertEquals(2.0, sGDText0.getLNorm(), 0.01);
      assertEquals(0, sGDText0.getPeriodicPruning());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText0.epochsTipText());
      assertEquals("The random number seed to be used.", sGDText0.seedTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText0.debugTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText0.minWordFrequencyTipText());
      assertFalse(sGDText0.getDebug());
      assertEquals(0.01, sGDText0.getLearningRate(), 0.01);
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText0.tokenizerTipText());
      assertFalse(sGDText0.getNormalizeDocLength());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText0.periodicPruningTipText());
      assertEquals(1.0E-4, sGDText0.getLambda(), 0.01);
      assertEquals("The learning rate.", sGDText0.learningRateTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText0.normalizeDocLengthTipText());
      assertEquals(1, sGDText0.getSeed());
      assertEquals("The norm of the instances after normalization.", sGDText0.normTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText0.useStopListTipText());
      assertEquals("The LNorm to use for document length normalization.", sGDText0.LNormTipText());
      assertFalse(capabilities0.hasDependencies());
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      assertNotNull(capabilities0);
      
      MockFile mockFile0 = (MockFile)naiveBayesMultinomialText1.m_stopwordsFile;
      assertTrue(mockFile0.canExecute());
      assertEquals("/home/ubuntu/evosuite_readability_gen/projects", mockFile0.getParent());
      assertTrue(mockFile0.isAbsolute());
      assertTrue(mockFile0.canRead());
      assertEquals(0L, mockFile0.getUsableSpace());
      assertFalse(mockFile0.isHidden());
      assertTrue(mockFile0.isDirectory());
      assertTrue(mockFile0.canWrite());
      assertEquals("107_weka", mockFile0.getName());
      assertEquals(0L, mockFile0.getFreeSpace());
      assertEquals(0L, mockFile0.length());
      assertEquals("/home/ubuntu/evosuite_readability_gen/projects/107_weka", mockFile0.toString());
      assertEquals(0L, mockFile0.getTotalSpace());
      assertEquals(1392409281320L, mockFile0.lastModified());
      assertTrue(mockFile0.exists());
      assertFalse(mockFile0.isFile());
      assertNotNull(mockFile0);
      
      MockFile mockFile1 = new MockFile(mockFile0, "Whether to convert all tokens to lowercase");
      assertFalse(naiveBayesMultinomialText1.equals((Object)naiveBayesMultinomialText0));
      assertFalse(mockFile1.equals((Object)mockFile0));
      assertNotNull(mockFile1);
      
      naiveBayesMultinomialText0.setStopwords(mockFile1);
      assertNotSame(naiveBayesMultinomialText0, naiveBayesMultinomialText1);
      assertNotSame(naiveBayesMultinomialText1, naiveBayesMultinomialText0);
      assertNotSame(mockFile0, mockFile1);
      assertNotSame(mockFile1, mockFile0);
      assertFalse(naiveBayesMultinomialText0.equals((Object)naiveBayesMultinomialText1));
      assertFalse(naiveBayesMultinomialText1.equals((Object)naiveBayesMultinomialText0));
      assertFalse(mockFile0.equals((Object)mockFile1));
      assertFalse(mockFile1.equals((Object)mockFile0));
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals((-2703.21318412), naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals((-1.0), naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText1.tokenizerTipText());
      assertFalse(naiveBayesMultinomialText1.getNormalizeDocLength());
      assertEquals(2.0, naiveBayesMultinomialText1.getLNorm(), 0.01);
      assertEquals(3.0, naiveBayesMultinomialText1.getMinWordFrequency(), 0.01);
      assertEquals(1.0, naiveBayesMultinomialText1.getNorm(), 0.01);
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText1.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText1.getDebug());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText1.debugTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText1.lowercaseTokensTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText1.globalInfo());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText1.normalizeDocLengthTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText1.normTipText());
      assertFalse(naiveBayesMultinomialText1.getUseWordFrequencies());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText1.stopwordsTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText1.periodicPruningTipText());
      assertFalse(naiveBayesMultinomialText1.getUseStopList());
      assertEquals(0, naiveBayesMultinomialText1.getPeriodicPruning());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText1.useStopListTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText1.stemmerTipText());
      assertFalse(naiveBayesMultinomialText1.getLowercaseTokens());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText1.LNormTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText1.useWordFrequenciesTipText());
      assertTrue(mockFile0.canExecute());
      assertEquals("/home/ubuntu/evosuite_readability_gen/projects", mockFile0.getParent());
      assertTrue(mockFile0.isAbsolute());
      assertTrue(mockFile0.canRead());
      assertEquals(0L, mockFile0.getUsableSpace());
      assertFalse(mockFile0.isHidden());
      assertTrue(mockFile0.isDirectory());
      assertTrue(mockFile0.canWrite());
      assertEquals("107_weka", mockFile0.getName());
      assertEquals(0L, mockFile0.getFreeSpace());
      assertEquals(0L, mockFile0.length());
      assertEquals("/home/ubuntu/evosuite_readability_gen/projects/107_weka", mockFile0.toString());
      assertEquals(0L, mockFile0.getTotalSpace());
      assertEquals(1392409281320L, mockFile0.lastModified());
      assertTrue(mockFile0.exists());
      assertFalse(mockFile0.isFile());
      assertFalse(mockFile1.exists());
      assertTrue(mockFile1.isFile());
      assertEquals(0L, mockFile1.getTotalSpace());
      assertFalse(mockFile1.canExecute());
      assertFalse(mockFile1.canRead());
      assertEquals("/home/ubuntu/evosuite_readability_gen/projects/107_weka/Whether to convert all tokens to lowercase", mockFile1.toString());
      assertEquals("/home/ubuntu/evosuite_readability_gen/projects/107_weka", mockFile1.getParent());
      assertEquals(0L, mockFile1.length());
      assertFalse(mockFile1.canWrite());
      assertEquals(0L, mockFile1.getFreeSpace());
      assertFalse(mockFile1.isHidden());
      assertFalse(mockFile1.isDirectory());
      assertEquals(0L, mockFile1.getUsableSpace());
      assertEquals(0L, mockFile1.lastModified());
      assertTrue(mockFile1.isAbsolute());
      assertEquals("Whether to convert all tokens to lowercase", mockFile1.getName());
  }

  /**
  //Test case number: 14
  /*Coverage entropy=2.5714847932212934
  */
  @Test(timeout = 4000)
  public void test14()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertNotNull(naiveBayesMultinomialText0);
      
      TestInstances testInstances0 = new TestInstances();
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(0, testInstances0.getNumRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(0, testInstances0.getNumString());
      assertFalse(testInstances0.getNoClass());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals((-1), testInstances0.getClassIndex());
      assertNotNull(testInstances0);
      
      Instances instances0 = testInstances0.generate("s:D=ZV\"BVCX");
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(0, testInstances0.getNumRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(0, testInstances0.getNumString());
      assertFalse(testInstances0.getNoClass());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(1, instances0.classIndex());
      assertEquals(2, instances0.numClasses());
      assertEquals(2, instances0.numAttributes());
      assertEquals(20, instances0.numInstances());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(20, instances0.size());
      assertNotNull(instances0);
      
      NullStemmer nullStemmer0 = new NullStemmer();
      assertEquals("A dummy stemmer that performs no stemming at all.", nullStemmer0.globalInfo());
      assertNotNull(nullStemmer0);
      
      boolean boolean0 = instances0.remove((Object) null);
      assertFalse(boolean0);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(0, testInstances0.getNumRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(0, testInstances0.getNumString());
      assertFalse(testInstances0.getNoClass());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(1, instances0.classIndex());
      assertEquals(2, instances0.numClasses());
      assertEquals(2, instances0.numAttributes());
      assertEquals(20, instances0.numInstances());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(20, instances0.size());
      
      naiveBayesMultinomialText0.buildClassifier(instances0);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(0, testInstances0.getNumRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(0, testInstances0.getNumString());
      assertFalse(testInstances0.getNoClass());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(1, instances0.classIndex());
      assertEquals(2, instances0.numClasses());
      assertEquals(2, instances0.numAttributes());
      assertEquals(20, instances0.numInstances());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(20, instances0.size());
      
      naiveBayesMultinomialText0.setPeriodicPruning(90);
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(90, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      
      NaiveBayesMultinomialText.main(testInstances0.DEFAULT_WORDS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(0, testInstances0.getNumRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(0, testInstances0.getNumString());
      assertFalse(testInstances0.getNoClass());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals((-1), testInstances0.getClassIndex());
      
      String string0 = naiveBayesMultinomialText0.tokenizerTipText();
      assertEquals("The tokenizing algorithm to use on the strings.", string0);
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(90, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertNotNull(string0);
      
      naiveBayesMultinomialText0.setNorm(380.4198);
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals(380.4198, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(90, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      
      String string1 = naiveBayesMultinomialText0.lowercaseTokensTipText();
      assertEquals("Whether to convert all tokens to lowercase", string1);
      assertFalse(string1.equals((Object)string0));
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals(380.4198, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(90, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertNotNull(string1);
      
      naiveBayesMultinomialText0.pruneDictionary();
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals(380.4198, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(90, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      
      String string2 = naiveBayesMultinomialText0.stopwordsTipText();
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", string2);
      assertFalse(string2.equals((Object)string0));
      assertFalse(string2.equals((Object)string1));
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals(380.4198, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(90, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertNotNull(string2);
      
      SGDText sGDText0 = new SGDText();
      assertEquals(1, SGDText.LOGLOSS);
      assertEquals(0, SGDText.HINGE);
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText0.lossFunctionTipText());
      assertFalse(sGDText0.getOutputProbsForSVM());
      assertFalse(sGDText0.getUseStopList());
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText0.globalInfo());
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText0.useWordFrequenciesTipText());
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText0.outputProbsForSVMTipText());
      assertEquals(1.0, sGDText0.getNorm(), 0.01);
      assertFalse(sGDText0.getLowercaseTokens());
      assertEquals("Whether to convert all tokens to lowercase", sGDText0.lowercaseTokensTipText());
      assertEquals("The regularization constant. (default = 0.0001)", sGDText0.lambdaTipText());
      assertFalse(sGDText0.getUseWordFrequencies());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText0.stopwordsTipText());
      assertEquals(500, sGDText0.getEpochs());
      assertEquals("The stemming algorithm to use on the words.", sGDText0.stemmerTipText());
      assertEquals(2.0, sGDText0.getLNorm(), 0.01);
      assertEquals(0, sGDText0.getPeriodicPruning());
      assertEquals(3.0, sGDText0.getMinWordFrequency(), 0.01);
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText0.epochsTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText0.debugTipText());
      assertEquals(0.01, sGDText0.getLearningRate(), 0.01);
      assertEquals("The random number seed to be used.", sGDText0.seedTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText0.minWordFrequencyTipText());
      assertEquals("The learning rate.", sGDText0.learningRateTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText0.tokenizerTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText0.periodicPruningTipText());
      assertEquals(1.0E-4, sGDText0.getLambda(), 0.01);
      assertFalse(sGDText0.getDebug());
      assertFalse(sGDText0.getNormalizeDocLength());
      assertEquals("The norm of the instances after normalization.", sGDText0.normTipText());
      assertEquals(1, sGDText0.getSeed());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText0.normalizeDocLengthTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText0.useStopListTipText());
      assertEquals("The LNorm to use for document length normalization.", sGDText0.LNormTipText());
      assertNotNull(sGDText0);
      
      sGDText0.setPeriodicPruning(68);
      assertEquals(1, SGDText.LOGLOSS);
      assertEquals(0, SGDText.HINGE);
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText0.lossFunctionTipText());
      assertFalse(sGDText0.getOutputProbsForSVM());
      assertFalse(sGDText0.getUseStopList());
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText0.globalInfo());
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText0.useWordFrequenciesTipText());
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText0.outputProbsForSVMTipText());
      assertEquals(1.0, sGDText0.getNorm(), 0.01);
      assertFalse(sGDText0.getLowercaseTokens());
      assertEquals("Whether to convert all tokens to lowercase", sGDText0.lowercaseTokensTipText());
      assertEquals("The regularization constant. (default = 0.0001)", sGDText0.lambdaTipText());
      assertFalse(sGDText0.getUseWordFrequencies());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText0.stopwordsTipText());
      assertEquals(500, sGDText0.getEpochs());
      assertEquals("The stemming algorithm to use on the words.", sGDText0.stemmerTipText());
      assertEquals(2.0, sGDText0.getLNorm(), 0.01);
      assertEquals(3.0, sGDText0.getMinWordFrequency(), 0.01);
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText0.epochsTipText());
      assertEquals(68, sGDText0.getPeriodicPruning());
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText0.debugTipText());
      assertEquals(0.01, sGDText0.getLearningRate(), 0.01);
      assertEquals("The random number seed to be used.", sGDText0.seedTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText0.minWordFrequencyTipText());
      assertEquals("The learning rate.", sGDText0.learningRateTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText0.tokenizerTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText0.periodicPruningTipText());
      assertEquals(1.0E-4, sGDText0.getLambda(), 0.01);
      assertFalse(sGDText0.getDebug());
      assertFalse(sGDText0.getNormalizeDocLength());
      assertEquals("The norm of the instances after normalization.", sGDText0.normTipText());
      assertEquals(1, sGDText0.getSeed());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText0.normalizeDocLengthTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText0.useStopListTipText());
      assertEquals("The LNorm to use for document length normalization.", sGDText0.LNormTipText());
      
      String string3 = naiveBayesMultinomialText0.periodicPruningTipText();
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", string3);
      assertFalse(string3.equals((Object)string0));
      assertFalse(string3.equals((Object)string1));
      assertFalse(string3.equals((Object)string2));
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals(380.4198, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(90, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertNotNull(string3);
      
      WordTokenizer wordTokenizer0 = (WordTokenizer)naiveBayesMultinomialText0.getTokenizer();
      assertEquals(" \r\n\t.,;:'\"()?!", wordTokenizer0.getDelimiters());
      assertEquals("Set of delimiter characters to use in tokenizing (\\r, \\n and \\t can be used for carriage-return, line-feed and tab)", wordTokenizer0.delimitersTipText());
      assertEquals("A simple tokenizer that is using the java.util.StringTokenizer class to tokenize the strings.", wordTokenizer0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals(380.4198, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(90, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertNotNull(wordTokenizer0);
      
      String string4 = naiveBayesMultinomialText0.debugTipText();
      assertEquals("If set to true, classifier may output additional info to the console.", string4);
      assertFalse(string4.equals((Object)string0));
      assertFalse(string4.equals((Object)string2));
      assertFalse(string4.equals((Object)string1));
      assertFalse(string4.equals((Object)string3));
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals(380.4198, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(90, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertNotNull(string4);
      
      NaiveBayesMultinomialText naiveBayesMultinomialText1 = new NaiveBayesMultinomialText();
      assertFalse(naiveBayesMultinomialText1.equals((Object)naiveBayesMultinomialText0));
      assertFalse(naiveBayesMultinomialText1.getUseStopList());
      assertEquals(1.0, naiveBayesMultinomialText1.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText1.getLowercaseTokens());
      assertEquals(0, naiveBayesMultinomialText1.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText1.getDebug());
      assertFalse(naiveBayesMultinomialText1.getUseWordFrequencies());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText1.useWordFrequenciesTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText1.globalInfo());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText1.normTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText1.stopwordsTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText1.debugTipText());
      assertEquals(2.0, naiveBayesMultinomialText1.getLNorm(), 0.01);
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText1.lowercaseTokensTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText1.minWordFrequencyTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText1.LNormTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText1.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText1.getNormalizeDocLength());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText1.stemmerTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText1.tokenizerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText1.useStopListTipText());
      assertEquals(3.0, naiveBayesMultinomialText1.getMinWordFrequency(), 0.01);
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText1.periodicPruningTipText());
      assertNotNull(naiveBayesMultinomialText1);
      
      String string5 = naiveBayesMultinomialText1.minWordFrequencyTipText();
      assertNotSame(naiveBayesMultinomialText1, naiveBayesMultinomialText0);
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", string5);
      assertFalse(naiveBayesMultinomialText1.equals((Object)naiveBayesMultinomialText0));
      assertFalse(string5.equals((Object)string3));
      assertFalse(string5.equals((Object)string4));
      assertFalse(string5.equals((Object)string2));
      assertFalse(string5.equals((Object)string1));
      assertFalse(string5.equals((Object)string0));
      assertFalse(naiveBayesMultinomialText1.getUseStopList());
      assertEquals(1.0, naiveBayesMultinomialText1.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText1.getLowercaseTokens());
      assertEquals(0, naiveBayesMultinomialText1.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText1.getDebug());
      assertFalse(naiveBayesMultinomialText1.getUseWordFrequencies());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText1.useWordFrequenciesTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText1.globalInfo());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText1.normTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText1.stopwordsTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText1.debugTipText());
      assertEquals(2.0, naiveBayesMultinomialText1.getLNorm(), 0.01);
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText1.lowercaseTokensTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText1.minWordFrequencyTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText1.LNormTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText1.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText1.getNormalizeDocLength());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText1.stemmerTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText1.tokenizerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText1.useStopListTipText());
      assertEquals(3.0, naiveBayesMultinomialText1.getMinWordFrequency(), 0.01);
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText1.periodicPruningTipText());
      assertNotNull(string5);
      
      naiveBayesMultinomialText1.setLNorm(68);
      assertNotSame(naiveBayesMultinomialText1, naiveBayesMultinomialText0);
      assertFalse(naiveBayesMultinomialText1.equals((Object)naiveBayesMultinomialText0));
      assertFalse(naiveBayesMultinomialText1.getUseStopList());
      assertEquals(1.0, naiveBayesMultinomialText1.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText1.getLowercaseTokens());
      assertEquals(0, naiveBayesMultinomialText1.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText1.getDebug());
      assertFalse(naiveBayesMultinomialText1.getUseWordFrequencies());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText1.useWordFrequenciesTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText1.globalInfo());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText1.normTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText1.stopwordsTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText1.debugTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText1.lowercaseTokensTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText1.minWordFrequencyTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText1.LNormTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText1.normalizeDocLengthTipText());
      assertEquals(68.0, naiveBayesMultinomialText1.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText1.getNormalizeDocLength());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText1.stemmerTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText1.tokenizerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText1.useStopListTipText());
      assertEquals(3.0, naiveBayesMultinomialText1.getMinWordFrequency(), 0.01);
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText1.periodicPruningTipText());
      
      CoverTree coverTree0 = new CoverTree();
      assertEquals(0.0, coverTree0.measureTreeSize(), 0.01);
      assertEquals(0.0, coverTree0.measureMaxDepth(), 0.01);
      assertFalse(coverTree0.getMeasurePerformance());
      assertEquals("Whether to calculate performance statistics for the NN search or not", coverTree0.measurePerformanceTipText());
      assertEquals(0.0, coverTree0.measureNumLeaves(), 0.01);
      assertEquals("The distance function to use for finding neighbours (default: weka.core.EuclideanDistance). ", coverTree0.distanceFunctionTipText());
      assertEquals(1.3, coverTree0.getBase(), 0.01);
      assertEquals("The base for the expansion constant.", coverTree0.baseTipText());
      assertNotNull(coverTree0);
      
      SparseInstance sparseInstance0 = new SparseInstance(68);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertEquals(1.0, sparseInstance0.weight(), 0.01);
      assertEquals(68, sparseInstance0.numValues());
      assertEquals(68, sparseInstance0.numAttributes());
      assertNotNull(sparseInstance0);
      
      try { 
        coverTree0.nearestNeighbour(sparseInstance0);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("weka.core.Instances", e);
      }
  }

  /**
  //Test case number: 15
  /*Coverage entropy=1.9356732692787166
  */
  @Test(timeout = 4000)
  public void test15()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertNotNull(naiveBayesMultinomialText0);
      
      TestInstances testInstances0 = new TestInstances();
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertFalse(testInstances0.getNoClass());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(2, testInstances0.getNumAttributes());
      assertNotNull(testInstances0);
      
      testInstances0.setRelation("RG1=e66u2");
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertFalse(testInstances0.getNoClass());
      assertEquals("RG1=e66u2", testInstances0.getRelation());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(2, testInstances0.getNumAttributes());
      
      NaiveBayesMultinomialText naiveBayesMultinomialText1 = new NaiveBayesMultinomialText();
      assertFalse(naiveBayesMultinomialText1.equals((Object)naiveBayesMultinomialText0));
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText1.lowercaseTokensTipText());
      assertEquals(3.0, naiveBayesMultinomialText1.getMinWordFrequency(), 0.01);
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText1.minWordFrequencyTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText1.tokenizerTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText1.useWordFrequenciesTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText1.stemmerTipText());
      assertFalse(naiveBayesMultinomialText1.getLowercaseTokens());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText1.useStopListTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText1.LNormTipText());
      assertFalse(naiveBayesMultinomialText1.getUseStopList());
      assertFalse(naiveBayesMultinomialText1.getDebug());
      assertEquals(0, naiveBayesMultinomialText1.getPeriodicPruning());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText1.periodicPruningTipText());
      assertFalse(naiveBayesMultinomialText1.getNormalizeDocLength());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText1.normTipText());
      assertFalse(naiveBayesMultinomialText1.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText1.globalInfo());
      assertEquals(2.0, naiveBayesMultinomialText1.getLNorm(), 0.01);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText1.normalizeDocLengthTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText1.stopwordsTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText1.debugTipText());
      assertEquals(1.0, naiveBayesMultinomialText1.getNorm(), 0.01);
      assertNotNull(naiveBayesMultinomialText1);
      
      LinkedHashMap<String, NaiveBayesMultinomialText.Count> linkedHashMap0 = naiveBayesMultinomialText1.m_inputVector;
      assertNull(linkedHashMap0);
      
      naiveBayesMultinomialText0.m_inputVector = null;
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      
      SGDText sGDText0 = new SGDText();
      assertEquals(1, SGDText.LOGLOSS);
      assertEquals(0, SGDText.HINGE);
      assertEquals(1.0E-4, sGDText0.getLambda(), 0.01);
      assertFalse(sGDText0.getUseStopList());
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText0.outputProbsForSVMTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText0.useWordFrequenciesTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText0.minWordFrequencyTipText());
      assertFalse(sGDText0.getDebug());
      assertFalse(sGDText0.getOutputProbsForSVM());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText0.periodicPruningTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText0.normalizeDocLengthTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText0.stopwordsTipText());
      assertEquals(2.0, sGDText0.getLNorm(), 0.01);
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText0.tokenizerTipText());
      assertEquals(0, sGDText0.getPeriodicPruning());
      assertEquals(500, sGDText0.getEpochs());
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText0.debugTipText());
      assertFalse(sGDText0.getLowercaseTokens());
      assertEquals(3.0, sGDText0.getMinWordFrequency(), 0.01);
      assertFalse(sGDText0.getUseWordFrequencies());
      assertEquals("The regularization constant. (default = 0.0001)", sGDText0.lambdaTipText());
      assertEquals("The learning rate.", sGDText0.learningRateTipText());
      assertEquals("The random number seed to be used.", sGDText0.seedTipText());
      assertEquals(0.01, sGDText0.getLearningRate(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText0.useStopListTipText());
      assertEquals("The stemming algorithm to use on the words.", sGDText0.stemmerTipText());
      assertFalse(sGDText0.getNormalizeDocLength());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText0.epochsTipText());
      assertEquals("The norm of the instances after normalization.", sGDText0.normTipText());
      assertEquals(1, sGDText0.getSeed());
      assertEquals("Whether to convert all tokens to lowercase", sGDText0.lowercaseTokensTipText());
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText0.globalInfo());
      assertEquals(1.0, sGDText0.getNorm(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", sGDText0.LNormTipText());
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText0.lossFunctionTipText());
      assertNotNull(sGDText0);
      
      WordTokenizer wordTokenizer0 = (WordTokenizer)sGDText0.getTokenizer();
      assertEquals(1, SGDText.LOGLOSS);
      assertEquals(0, SGDText.HINGE);
      assertEquals(1.0E-4, sGDText0.getLambda(), 0.01);
      assertFalse(sGDText0.getUseStopList());
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText0.outputProbsForSVMTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText0.useWordFrequenciesTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText0.minWordFrequencyTipText());
      assertFalse(sGDText0.getDebug());
      assertFalse(sGDText0.getOutputProbsForSVM());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText0.periodicPruningTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText0.normalizeDocLengthTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText0.stopwordsTipText());
      assertEquals(2.0, sGDText0.getLNorm(), 0.01);
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText0.tokenizerTipText());
      assertEquals(0, sGDText0.getPeriodicPruning());
      assertEquals(500, sGDText0.getEpochs());
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText0.debugTipText());
      assertFalse(sGDText0.getLowercaseTokens());
      assertEquals(3.0, sGDText0.getMinWordFrequency(), 0.01);
      assertFalse(sGDText0.getUseWordFrequencies());
      assertEquals("The regularization constant. (default = 0.0001)", sGDText0.lambdaTipText());
      assertEquals("The learning rate.", sGDText0.learningRateTipText());
      assertEquals("The random number seed to be used.", sGDText0.seedTipText());
      assertEquals(0.01, sGDText0.getLearningRate(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText0.useStopListTipText());
      assertEquals("The stemming algorithm to use on the words.", sGDText0.stemmerTipText());
      assertFalse(sGDText0.getNormalizeDocLength());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText0.epochsTipText());
      assertEquals("The norm of the instances after normalization.", sGDText0.normTipText());
      assertEquals(1, sGDText0.getSeed());
      assertEquals("Whether to convert all tokens to lowercase", sGDText0.lowercaseTokensTipText());
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText0.globalInfo());
      assertEquals(1.0, sGDText0.getNorm(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", sGDText0.LNormTipText());
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText0.lossFunctionTipText());
      assertEquals(" \r\n\t.,;:'\"()?!", wordTokenizer0.getDelimiters());
      assertEquals("A simple tokenizer that is using the java.util.StringTokenizer class to tokenize the strings.", wordTokenizer0.globalInfo());
      assertEquals("Set of delimiter characters to use in tokenizing (\\r, \\n and \\t can be used for carriage-return, line-feed and tab)", wordTokenizer0.delimitersTipText());
      assertNotNull(wordTokenizer0);
      
      naiveBayesMultinomialText0.m_tokenizer = (Tokenizer) wordTokenizer0;
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals(1.0E-4, sGDText0.getLambda(), 0.01);
      assertFalse(sGDText0.getUseStopList());
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText0.outputProbsForSVMTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText0.useWordFrequenciesTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText0.minWordFrequencyTipText());
      assertFalse(sGDText0.getDebug());
      assertFalse(sGDText0.getOutputProbsForSVM());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText0.periodicPruningTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText0.normalizeDocLengthTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText0.stopwordsTipText());
      assertEquals(2.0, sGDText0.getLNorm(), 0.01);
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText0.tokenizerTipText());
      assertEquals(0, sGDText0.getPeriodicPruning());
      assertEquals(500, sGDText0.getEpochs());
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText0.debugTipText());
      assertFalse(sGDText0.getLowercaseTokens());
      assertEquals(3.0, sGDText0.getMinWordFrequency(), 0.01);
      assertFalse(sGDText0.getUseWordFrequencies());
      assertEquals("The regularization constant. (default = 0.0001)", sGDText0.lambdaTipText());
      assertEquals("The learning rate.", sGDText0.learningRateTipText());
      assertEquals("The random number seed to be used.", sGDText0.seedTipText());
      assertEquals(0.01, sGDText0.getLearningRate(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText0.useStopListTipText());
      assertEquals("The stemming algorithm to use on the words.", sGDText0.stemmerTipText());
      assertFalse(sGDText0.getNormalizeDocLength());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText0.epochsTipText());
      assertEquals("The norm of the instances after normalization.", sGDText0.normTipText());
      assertEquals(1, sGDText0.getSeed());
      assertEquals("Whether to convert all tokens to lowercase", sGDText0.lowercaseTokensTipText());
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText0.globalInfo());
      assertEquals(1.0, sGDText0.getNorm(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", sGDText0.LNormTipText());
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText0.lossFunctionTipText());
      assertEquals(" \r\n\t.,;:'\"()?!", wordTokenizer0.getDelimiters());
      assertEquals("A simple tokenizer that is using the java.util.StringTokenizer class to tokenize the strings.", wordTokenizer0.globalInfo());
      assertEquals("Set of delimiter characters to use in tokenizing (\\r, \\n and \\t can be used for carriage-return, line-feed and tab)", wordTokenizer0.delimitersTipText());
      
      boolean boolean0 = FileSystemHandling.shouldAllThrowIOExceptions();
      assertTrue(boolean0);
      
      naiveBayesMultinomialText0.m_lowercaseTokens = false;
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      
      TestInstances.main(testInstances0.DEFAULT_WORDS);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertFalse(testInstances0.getNoClass());
      assertEquals("RG1=e66u2", testInstances0.getRelation());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(2, testInstances0.getNumAttributes());
      
      naiveBayesMultinomialText1.setLowercaseTokens(true);
      assertNotSame(naiveBayesMultinomialText1, naiveBayesMultinomialText0);
      assertFalse(naiveBayesMultinomialText1.equals((Object)naiveBayesMultinomialText0));
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText1.lowercaseTokensTipText());
      assertEquals(3.0, naiveBayesMultinomialText1.getMinWordFrequency(), 0.01);
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText1.minWordFrequencyTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText1.tokenizerTipText());
      assertTrue(naiveBayesMultinomialText1.getLowercaseTokens());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText1.useWordFrequenciesTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText1.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText1.useStopListTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText1.LNormTipText());
      assertFalse(naiveBayesMultinomialText1.getUseStopList());
      assertFalse(naiveBayesMultinomialText1.getDebug());
      assertEquals(0, naiveBayesMultinomialText1.getPeriodicPruning());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText1.periodicPruningTipText());
      assertFalse(naiveBayesMultinomialText1.getNormalizeDocLength());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText1.normTipText());
      assertFalse(naiveBayesMultinomialText1.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText1.globalInfo());
      assertEquals(2.0, naiveBayesMultinomialText1.getLNorm(), 0.01);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText1.normalizeDocLengthTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText1.stopwordsTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText1.debugTipText());
      assertEquals(1.0, naiveBayesMultinomialText1.getNorm(), 0.01);
      
      Instances instances0 = testInstances0.generate("");
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertFalse(testInstances0.getNoClass());
      assertEquals("RG1=e66u2", testInstances0.getRelation());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(2, instances0.numClasses());
      assertEquals(2, instances0.numAttributes());
      assertEquals(1, instances0.classIndex());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(20, instances0.size());
      assertEquals("RG1=e66u2", instances0.relationName());
      assertEquals(20, instances0.numInstances());
      assertNotNull(instances0);
      
      NullStemmer nullStemmer0 = new NullStemmer();
      assertEquals("A dummy stemmer that performs no stemming at all.", nullStemmer0.globalInfo());
      assertNotNull(nullStemmer0);
      
      boolean boolean1 = instances0.remove((Object) nullStemmer0);
      assertFalse(boolean1);
      assertFalse(boolean1 == boolean0);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertFalse(testInstances0.getNoClass());
      assertEquals("RG1=e66u2", testInstances0.getRelation());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(2, instances0.numClasses());
      assertEquals(2, instances0.numAttributes());
      assertEquals(1, instances0.classIndex());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(20, instances0.size());
      assertEquals("RG1=e66u2", instances0.relationName());
      assertEquals(20, instances0.numInstances());
      assertEquals("A dummy stemmer that performs no stemming at all.", nullStemmer0.globalInfo());
      
      naiveBayesMultinomialText0.buildClassifier(instances0);
      assertNotSame(naiveBayesMultinomialText0, naiveBayesMultinomialText1);
      assertFalse(naiveBayesMultinomialText0.equals((Object)naiveBayesMultinomialText1));
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertFalse(testInstances0.getNoClass());
      assertEquals("RG1=e66u2", testInstances0.getRelation());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(2, instances0.numClasses());
      assertEquals(2, instances0.numAttributes());
      assertEquals(1, instances0.classIndex());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(20, instances0.size());
      assertEquals("RG1=e66u2", instances0.relationName());
      assertEquals(20, instances0.numInstances());
      
      double[] doubleArray0 = new double[0];
      SparseInstance sparseInstance0 = new SparseInstance(0.0, doubleArray0);
      assertArrayEquals(new double[] {}, doubleArray0, 0.01);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertEquals(0, doubleArray0.length);
      assertEquals(0, sparseInstance0.numAttributes());
      assertEquals(0, sparseInstance0.numValues());
      assertEquals(0.0, sparseInstance0.weight(), 0.01);
      assertNotNull(sparseInstance0);
      
      try { 
        naiveBayesMultinomialText1.classifyInstance(sparseInstance0);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("weka.classifiers.bayes.NaiveBayesMultinomialText", e);
      }
  }

  /**
  //Test case number: 16
  /*Coverage entropy=3.107972275978608
  */
  @Test(timeout = 4000)
  public void test16()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertNotNull(naiveBayesMultinomialText0);
      
      naiveBayesMultinomialText0.pruneDictionary();
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      
      String string0 = naiveBayesMultinomialText0.getRevision();
      assertEquals("9122", string0);
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertNotNull(string0);
      
      naiveBayesMultinomialText0.setNormalizeDocLength(true);
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertTrue(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      
      naiveBayesMultinomialText0.setNormalizeDocLength(true);
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertTrue(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      
      naiveBayesMultinomialText0.setLowercaseTokens(false);
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertTrue(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      
      naiveBayesMultinomialText0.setUseStopList(false);
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertTrue(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      
      FileSystemHandling fileSystemHandling0 = new FileSystemHandling();
      assertNotNull(fileSystemHandling0);
      
      naiveBayesMultinomialText0.setUseWordFrequencies(false);
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertTrue(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      
      String string1 = naiveBayesMultinomialText0.toString();
      assertEquals("NaiveBayesMultinomialText: No model built yet.\n", string1);
      assertFalse(string1.equals((Object)string0));
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertTrue(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertNotNull(string1);
      
      String[] stringArray0 = naiveBayesMultinomialText0.getOptions();
      assertEquals(13, stringArray0.length);
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertTrue(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertNotNull(stringArray0);
      
      Instance instance0 = null;
      Enumeration<Option> enumeration0 = naiveBayesMultinomialText0.listOptions();
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertTrue(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertNotNull(enumeration0);
      
      try { 
        naiveBayesMultinomialText0.distributionForInstance((Instance) null);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("weka.classifiers.bayes.NaiveBayesMultinomialText", e);
      }
  }

  /**
  //Test case number: 17
  /*Coverage entropy=2.0346160141680394
  */
  @Test(timeout = 4000)
  public void test17()  throws Throwable  {
      TestInstances testInstances0 = new TestInstances();
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals(1, testInstances0.getClassType());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(0, testInstances0.getNumDate());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(0, testInstances0.getNumNumeric());
      assertFalse(testInstances0.getNoClass());
      assertNotNull(testInstances0);
      
      Instances instances0 = testInstances0.generate("Labels differ at position ");
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals(1, testInstances0.getClassType());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(0, testInstances0.getNumDate());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(0, testInstances0.getNumNumeric());
      assertFalse(testInstances0.getNoClass());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(2, instances0.numAttributes());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(2, instances0.numClasses());
      assertEquals(1, instances0.classIndex());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20, instances0.size());
      assertEquals(20, instances0.numInstances());
      assertNotNull(instances0);
      
      NullStemmer nullStemmer0 = new NullStemmer();
      assertEquals("A dummy stemmer that performs no stemming at all.", nullStemmer0.globalInfo());
      assertNotNull(nullStemmer0);
      
      SparseInstance sparseInstance0 = new SparseInstance(20);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertEquals(1.0, sparseInstance0.weight(), 0.01);
      assertEquals(20, sparseInstance0.numValues());
      assertEquals(20, sparseInstance0.numAttributes());
      assertNotNull(sparseInstance0);
      
      boolean boolean0 = instances0.add((Instance) sparseInstance0);
      assertTrue(boolean0);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertEquals(1, testInstances0.getClassType());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(0, testInstances0.getNumDate());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(0, testInstances0.getNumNumeric());
      assertFalse(testInstances0.getNoClass());
      assertEquals(21.0, instances0.sumOfWeights(), 0.01);
      assertEquals(2, instances0.numAttributes());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(2, instances0.numClasses());
      assertEquals(1, instances0.classIndex());
      assertEquals(21, instances0.size());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(21, instances0.numInstances());
      assertEquals(1.0, sparseInstance0.weight(), 0.01);
      assertEquals(20, sparseInstance0.numValues());
      assertEquals(20, sparseInstance0.numAttributes());
      
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertNotNull(naiveBayesMultinomialText0);
      
      naiveBayesMultinomialText0.buildClassifier(instances0);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals(1, testInstances0.getClassType());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(0, testInstances0.getNumDate());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(0, testInstances0.getNumNumeric());
      assertFalse(testInstances0.getNoClass());
      assertEquals(21.0, instances0.sumOfWeights(), 0.01);
      assertEquals(2, instances0.numAttributes());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(2, instances0.numClasses());
      assertEquals(1, instances0.classIndex());
      assertEquals(21, instances0.size());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(21, instances0.numInstances());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      
      naiveBayesMultinomialText0.setPeriodicPruning((-2));
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals((-2), naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      
      NaiveBayesMultinomialText naiveBayesMultinomialText1 = new NaiveBayesMultinomialText();
      assertFalse(naiveBayesMultinomialText1.equals((Object)naiveBayesMultinomialText0));
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText1.periodicPruningTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText1.stopwordsTipText());
      assertFalse(naiveBayesMultinomialText1.getDebug());
      assertFalse(naiveBayesMultinomialText1.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText1.globalInfo());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText1.normTipText());
      assertEquals(0, naiveBayesMultinomialText1.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText1.useWordFrequenciesTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText1.debugTipText());
      assertEquals(1.0, naiveBayesMultinomialText1.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText1.getUseStopList());
      assertEquals(2.0, naiveBayesMultinomialText1.getLNorm(), 0.01);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText1.tokenizerTipText());
      assertEquals(3.0, naiveBayesMultinomialText1.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText1.LNormTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText1.lowercaseTokensTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText1.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText1.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText1.getNormalizeDocLength());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText1.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText1.useStopListTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText1.minWordFrequencyTipText());
      assertNotNull(naiveBayesMultinomialText1);
      
      String string0 = naiveBayesMultinomialText0.tokenizerTipText();
      assertNotSame(naiveBayesMultinomialText0, naiveBayesMultinomialText1);
      assertEquals("The tokenizing algorithm to use on the strings.", string0);
      assertFalse(naiveBayesMultinomialText0.equals((Object)naiveBayesMultinomialText1));
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals((-2), naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertNotNull(string0);
      
      String string1 = naiveBayesMultinomialText1.toString();
      assertNotSame(naiveBayesMultinomialText1, naiveBayesMultinomialText0);
      assertEquals("NaiveBayesMultinomialText: No model built yet.\n", string1);
      assertFalse(naiveBayesMultinomialText1.equals((Object)naiveBayesMultinomialText0));
      assertFalse(string1.equals((Object)string0));
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText1.periodicPruningTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText1.stopwordsTipText());
      assertFalse(naiveBayesMultinomialText1.getDebug());
      assertFalse(naiveBayesMultinomialText1.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText1.globalInfo());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText1.normTipText());
      assertEquals(0, naiveBayesMultinomialText1.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText1.useWordFrequenciesTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText1.debugTipText());
      assertEquals(1.0, naiveBayesMultinomialText1.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText1.getUseStopList());
      assertEquals(2.0, naiveBayesMultinomialText1.getLNorm(), 0.01);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText1.tokenizerTipText());
      assertEquals(3.0, naiveBayesMultinomialText1.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText1.LNormTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText1.lowercaseTokensTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText1.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText1.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText1.getNormalizeDocLength());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText1.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText1.useStopListTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText1.minWordFrequencyTipText());
      assertNotNull(string1);
      
      NaiveBayesMultinomialText naiveBayesMultinomialText2 = new NaiveBayesMultinomialText();
      assertFalse(naiveBayesMultinomialText2.equals((Object)naiveBayesMultinomialText0));
      assertFalse(naiveBayesMultinomialText2.equals((Object)naiveBayesMultinomialText1));
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText2.periodicPruningTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText2.stopwordsTipText());
      assertFalse(naiveBayesMultinomialText2.getUseStopList());
      assertEquals(0, naiveBayesMultinomialText2.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText2.useWordFrequenciesTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText2.useStopListTipText());
      assertFalse(naiveBayesMultinomialText2.getLowercaseTokens());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText2.stemmerTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText2.tokenizerTipText());
      assertEquals(3.0, naiveBayesMultinomialText2.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText2.LNormTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText2.minWordFrequencyTipText());
      assertEquals(2.0, naiveBayesMultinomialText2.getLNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText2.debugTipText());
      assertEquals(1.0, naiveBayesMultinomialText2.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText2.getNormalizeDocLength());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText2.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText2.getDebug());
      assertFalse(naiveBayesMultinomialText2.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText2.globalInfo());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText2.normalizeDocLengthTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText2.normTipText());
      assertNotNull(naiveBayesMultinomialText2);
      
      naiveBayesMultinomialText2.pruneDictionary();
      assertNotSame(naiveBayesMultinomialText2, naiveBayesMultinomialText0);
      assertNotSame(naiveBayesMultinomialText2, naiveBayesMultinomialText1);
      assertFalse(naiveBayesMultinomialText2.equals((Object)naiveBayesMultinomialText0));
      assertFalse(naiveBayesMultinomialText2.equals((Object)naiveBayesMultinomialText1));
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText2.periodicPruningTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText2.stopwordsTipText());
      assertFalse(naiveBayesMultinomialText2.getUseStopList());
      assertEquals(0, naiveBayesMultinomialText2.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText2.useWordFrequenciesTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText2.useStopListTipText());
      assertFalse(naiveBayesMultinomialText2.getLowercaseTokens());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText2.stemmerTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText2.tokenizerTipText());
      assertEquals(3.0, naiveBayesMultinomialText2.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText2.LNormTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText2.minWordFrequencyTipText());
      assertEquals(2.0, naiveBayesMultinomialText2.getLNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText2.debugTipText());
      assertEquals(1.0, naiveBayesMultinomialText2.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText2.getNormalizeDocLength());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText2.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText2.getDebug());
      assertFalse(naiveBayesMultinomialText2.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText2.globalInfo());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText2.normalizeDocLengthTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText2.normTipText());
      
      Random.setNextRandom(0);
      naiveBayesMultinomialText1.pruneDictionary();
      assertNotSame(naiveBayesMultinomialText1, naiveBayesMultinomialText2);
      assertNotSame(naiveBayesMultinomialText1, naiveBayesMultinomialText0);
      assertFalse(naiveBayesMultinomialText1.equals((Object)naiveBayesMultinomialText2));
      assertFalse(naiveBayesMultinomialText1.equals((Object)naiveBayesMultinomialText0));
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText1.periodicPruningTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText1.stopwordsTipText());
      assertFalse(naiveBayesMultinomialText1.getDebug());
      assertFalse(naiveBayesMultinomialText1.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText1.globalInfo());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText1.normTipText());
      assertEquals(0, naiveBayesMultinomialText1.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText1.useWordFrequenciesTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText1.debugTipText());
      assertEquals(1.0, naiveBayesMultinomialText1.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText1.getUseStopList());
      assertEquals(2.0, naiveBayesMultinomialText1.getLNorm(), 0.01);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText1.tokenizerTipText());
      assertEquals(3.0, naiveBayesMultinomialText1.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText1.LNormTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText1.lowercaseTokensTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText1.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText1.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText1.getNormalizeDocLength());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText1.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText1.useStopListTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText1.minWordFrequencyTipText());
      
      try { 
        naiveBayesMultinomialText1.distributionForInstance(sparseInstance0);
        fail("Expecting exception: RuntimeException");
      
      } catch(RuntimeException e) {
         //
         // DenseInstance doesn't have access to a dataset!
         //
         verifyException("weka.core.AbstractInstance", e);
      }
  }

  /**
  //Test case number: 18
  /*Coverage entropy=3.120291565301284
  */
  @Test(timeout = 4000)
  public void test18()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertNotNull(naiveBayesMultinomialText0);
      
      naiveBayesMultinomialText0.pruneDictionary();
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      
      String string0 = naiveBayesMultinomialText0.getRevision();
      assertEquals("9122", string0);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertNotNull(string0);
      
      naiveBayesMultinomialText0.setNormalizeDocLength(true);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertTrue(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      
      naiveBayesMultinomialText0.setNormalizeDocLength(true);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertTrue(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      
      naiveBayesMultinomialText0.setLowercaseTokens(false);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertTrue(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      
      naiveBayesMultinomialText0.setUseStopList(true);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertTrue(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertTrue(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      
      FileSystemHandling fileSystemHandling0 = new FileSystemHandling();
      assertNotNull(fileSystemHandling0);
      
      naiveBayesMultinomialText0.setUseWordFrequencies(true);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertTrue(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertTrue(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertTrue(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      
      String string1 = naiveBayesMultinomialText0.toString();
      assertEquals("NaiveBayesMultinomialText: No model built yet.\n", string1);
      assertFalse(string1.equals((Object)string0));
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertTrue(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertTrue(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertTrue(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertNotNull(string1);
      
      String[] stringArray0 = naiveBayesMultinomialText0.getOptions();
      assertEquals(15, stringArray0.length);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertTrue(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertTrue(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertTrue(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertNotNull(stringArray0);
      
      Instance instance0 = null;
      Enumeration<Option> enumeration0 = naiveBayesMultinomialText0.listOptions();
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertTrue(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertTrue(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertTrue(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertNotNull(enumeration0);
      
      try { 
        naiveBayesMultinomialText0.distributionForInstance((Instance) null);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("weka.classifiers.bayes.NaiveBayesMultinomialText", e);
      }
  }

  /**
  //Test case number: 19
  /*Coverage entropy=3.076233820970336
  */
  @Test(timeout = 4000)
  public void test19()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertNotNull(naiveBayesMultinomialText0);
      
      String[] stringArray0 = new String[8];
      stringArray0[0] = ":";
      stringArray0[1] = "Set of delimiter characters to use in tokenizing (\r, \n and \t can be used for carriage-return, line-feed and tab)";
      naiveBayesMultinomialText0.setUseStopList(true);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertTrue(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      
      FileSystemHandling fileSystemHandling0 = new FileSystemHandling();
      assertNotNull(fileSystemHandling0);
      
      naiveBayesMultinomialText0.setUseWordFrequencies(true);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertTrue(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertTrue(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      
      String string0 = naiveBayesMultinomialText0.toString();
      assertEquals("NaiveBayesMultinomialText: No model built yet.\n", string0);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertTrue(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertTrue(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertNotNull(string0);
      
      String[] stringArray1 = naiveBayesMultinomialText0.getOptions();
      assertNotSame(stringArray1, stringArray0);
      assertFalse(stringArray1.equals((Object)stringArray0));
      assertEquals(14, stringArray1.length);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertTrue(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertTrue(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertNotNull(stringArray1);
      
      Enumeration<Option> enumeration0 = naiveBayesMultinomialText0.listOptions();
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertTrue(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertTrue(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertNotNull(enumeration0);
      
      TestInstances testInstances0 = new TestInstances();
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals(20, testInstances0.getNumInstances());
      assertFalse(testInstances0.getNoClass());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertNotNull(testInstances0);
      
      Instances instances0 = testInstances0.generate();
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals(20, testInstances0.getNumInstances());
      assertFalse(testInstances0.getNoClass());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(20, instances0.numInstances());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(1, instances0.classIndex());
      assertEquals(2, instances0.numAttributes());
      assertEquals(2, instances0.numClasses());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(20, instances0.size());
      assertNotNull(instances0);
      
      naiveBayesMultinomialText0.buildClassifier(instances0);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertTrue(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertTrue(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals(20, testInstances0.getNumInstances());
      assertFalse(testInstances0.getNoClass());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(20, instances0.numInstances());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(1, instances0.classIndex());
      assertEquals(2, instances0.numAttributes());
      assertEquals(2, instances0.numClasses());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(20, instances0.size());
      
      try { 
        naiveBayesMultinomialText0.distributionForInstance((Instance) null);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("weka.classifiers.bayes.NaiveBayesMultinomialText", e);
      }
  }
}
