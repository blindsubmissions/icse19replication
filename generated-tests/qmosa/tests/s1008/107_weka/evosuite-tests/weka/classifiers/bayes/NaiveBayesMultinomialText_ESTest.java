/*
 * This file was automatically generated by EvoSuite
 * Fri Aug 24 10:32:54 GMT 2018
 */

package weka.classifiers.bayes;

import org.junit.Test;
import static org.junit.Assert.*;
import static org.evosuite.runtime.EvoAssertions.*;
import java.io.File;
import java.util.Collection;
import java.util.LinkedHashMap;
import java.util.LinkedList;
import java.util.List;
import java.util.Locale;
import java.util.Map;
import java.util.Properties;
import java.util.Set;
import org.evosuite.runtime.EvoRunner;
import org.evosuite.runtime.EvoRunnerParameters;
import org.evosuite.runtime.Random;
import org.evosuite.runtime.System;
import org.evosuite.runtime.mock.java.io.MockFile;
import org.evosuite.runtime.testdata.EvoSuiteFile;
import org.evosuite.runtime.testdata.FileSystemHandling;
import org.junit.runner.RunWith;
import weka.classifiers.AbstractClassifier;
import weka.classifiers.bayes.NaiveBayesMultinomialText;
import weka.classifiers.functions.SGDText;
import weka.classifiers.meta.MultiClassClassifier;
import weka.classifiers.meta.MultiClassClassifierUpdateable;
import weka.classifiers.misc.InputMappedClassifier;
import weka.core.AbstractInstance;
import weka.core.Attribute;
import weka.core.BinarySparseInstance;
import weka.core.Capabilities;
import weka.core.DenseInstance;
import weka.core.Instance;
import weka.core.Instances;
import weka.core.ProtectedProperties;
import weka.core.SparseInstance;
import weka.core.TestInstances;
import weka.core.neighboursearch.BallTree;
import weka.core.neighboursearch.balltrees.BallNode;
import weka.core.stemmers.IteratedLovinsStemmer;
import weka.core.stemmers.NullStemmer;
import weka.core.stemmers.SnowballStemmer;
import weka.core.stemmers.Stemmer;
import weka.core.tokenizers.Tokenizer;
import weka.core.tokenizers.WordTokenizer;

@RunWith(EvoRunner.class) @EvoRunnerParameters(mockJVMNonDeterminism = true, useVFS = true, useVNET = true, resetStaticState = true, separateClassLoader = true, useJEE = true) 
public class NaiveBayesMultinomialText_ESTest extends NaiveBayesMultinomialText_ESTest_scaffolding {

  /**
  //Test case number: 0
  /*Coverage entropy=3.2958368660043296
  */
  @Test(timeout = 4000)
  public void test00()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.useWordFrequenciesTipText();
      naiveBayesMultinomialText0.m_periodicP = 200;
      String[] stringArray0 = naiveBayesMultinomialText0.getOptions();
      naiveBayesMultinomialText0.setOptions(stringArray0);
      assertEquals(200, naiveBayesMultinomialText0.getPeriodicPruning());
  }

  /**
  //Test case number: 1
  /*Coverage entropy=2.5649493574615376
  */
  @Test(timeout = 4000)
  public void test01()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      MockFile mockFile0 = (MockFile)naiveBayesMultinomialText0.m_stopwordsFile;
      MockFile mockFile1 = new MockFile(mockFile0, "ZTxJ>0uit-x$|:(zn9");
      mockFile1.setReadable(false);
      naiveBayesMultinomialText0.setStopwords(mockFile1);
      mockFile1.setExecutable(true);
      mockFile1.setExecutable(true);
      naiveBayesMultinomialText0.m_useStopList = true;
      mockFile1.delete();
      naiveBayesMultinomialText0.useWordFrequenciesTipText();
      naiveBayesMultinomialText0.getMinWordFrequency();
      naiveBayesMultinomialText0.getNorm();
      naiveBayesMultinomialText0.globalInfo();
      naiveBayesMultinomialText0.setUseStopList(true);
      naiveBayesMultinomialText0.setStopwords(mockFile1);
      naiveBayesMultinomialText0.globalInfo();
      naiveBayesMultinomialText0.setDebug(true);
      naiveBayesMultinomialText0.getCapabilities();
      naiveBayesMultinomialText0.getUseWordFrequencies();
      naiveBayesMultinomialText0.getUseWordFrequencies();
      naiveBayesMultinomialText0.normalizeDocLengthTipText();
      naiveBayesMultinomialText0.setLowercaseTokens(false);
      naiveBayesMultinomialText0.periodicPruningTipText();
      naiveBayesMultinomialText0.reset();
      naiveBayesMultinomialText0.useWordFrequenciesTipText();
      assertTrue(naiveBayesMultinomialText0.getUseStopList());
  }

  /**
  //Test case number: 2
  /*Coverage entropy=3.0445224377234217
  */
  @Test(timeout = 4000)
  public void test02()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.pruneDictionary();
      naiveBayesMultinomialText0.listOptions();
      naiveBayesMultinomialText0.reset();
      naiveBayesMultinomialText0.tokenizerTipText();
      naiveBayesMultinomialText0.LNormTipText();
      naiveBayesMultinomialText0.setStopwords((File) null);
      naiveBayesMultinomialText0.setUseWordFrequencies(false);
      naiveBayesMultinomialText0.getOptions();
      naiveBayesMultinomialText0.setLowercaseTokens(true);
      Instances instances0 = naiveBayesMultinomialText0.m_data;
      int int0 = 0;
      Instances instances1 = null;
      try {
        instances1 = new Instances((Instances) null, 0, 0);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("weka.core.Instances", e);
      }
  }

  /**
  //Test case number: 3
  /*Coverage entropy=1.6094379124341005
  */
  @Test(timeout = 4000)
  public void test03()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      IteratedLovinsStemmer iteratedLovinsStemmer0 = new IteratedLovinsStemmer();
      naiveBayesMultinomialText0.setStemmer(iteratedLovinsStemmer0);
      naiveBayesMultinomialText0.lowercaseTokensTipText();
      naiveBayesMultinomialText0.setStemmer(iteratedLovinsStemmer0);
      SnowballStemmer snowballStemmer0 = new SnowballStemmer((String) null);
      naiveBayesMultinomialText0.setStemmer(snowballStemmer0);
      naiveBayesMultinomialText0.getCapabilities();
      BinarySparseInstance binarySparseInstance0 = new BinarySparseInstance(2074);
      // Undeclared exception!
      try { 
        naiveBayesMultinomialText0.tokenizeInstance(binarySparseInstance0, true);
        fail("Expecting exception: RuntimeException");
      
      } catch(RuntimeException e) {
         //
         // DenseInstance doesn't have access to a dataset!
         //
         verifyException("weka.core.AbstractInstance", e);
      }
  }

  /**
  //Test case number: 4
  /*Coverage entropy=1.3862943611198906
  */
  @Test(timeout = 4000)
  public void test04()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.m_norm = (double) (-3740);
      double[] doubleArray0 = new double[3];
      doubleArray0[0] = (-3740.0);
      doubleArray0[1] = (-3740.0);
      doubleArray0[2] = (double) (-3740);
      naiveBayesMultinomialText0.m_probOfClass = doubleArray0;
      naiveBayesMultinomialText0.m_periodicP = (-3740);
      naiveBayesMultinomialText0.getStopwords();
      naiveBayesMultinomialText0.getUseStopList();
      naiveBayesMultinomialText0.stopwordsTipText();
      BallTree ballTree0 = new BallTree();
      InputMappedClassifier inputMappedClassifier0 = new InputMappedClassifier();
      BallNode ballNode0 = new BallNode((-3740), (-3127), (-3740), (Instance) null, 50.0);
      BinarySparseInstance binarySparseInstance0 = new BinarySparseInstance(3000);
      binarySparseInstance0.dataset();
      try { 
        BallNode.calcPivot(ballNode0, ballNode0, (Instances) null);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("weka.core.neighboursearch.balltrees.BallNode", e);
      }
  }

  /**
  //Test case number: 5
  /*Coverage entropy=2.553454531530652
  */
  @Test(timeout = 4000)
  public void test05()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      String string0 = naiveBayesMultinomialText0.toString();
      assertEquals("NaiveBayesMultinomialText: No model built yet.\n", string0);
      
      naiveBayesMultinomialText0.setUseStopList(false);
      naiveBayesMultinomialText0.pruneDictionary();
      naiveBayesMultinomialText0.m_lowercaseTokens = false;
      String[] stringArray0 = naiveBayesMultinomialText0.getOptions();
      double double0 = naiveBayesMultinomialText0.getLNorm();
      assertEquals(2.0, double0, 0.01);
      
      NaiveBayesMultinomialText.main(stringArray0);
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals(12, stringArray0.length);
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
  }

  /**
  //Test case number: 6
  /*Coverage entropy=2.0794415416798357
  */
  @Test(timeout = 4000)
  public void test06()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      double[] doubleArray0 = new double[7];
      doubleArray0[0] = 0.0;
      doubleArray0[1] = 0.0;
      doubleArray0[2] = (-1122.643);
      doubleArray0[3] = 7.0;
      doubleArray0[4] = 52.3;
      doubleArray0[5] = 1.0;
      doubleArray0[6] = 3359.5;
      naiveBayesMultinomialText0.m_wordsPerClass = doubleArray0;
      naiveBayesMultinomialText0.reset();
      naiveBayesMultinomialText0.setStemmer((Stemmer) null);
      naiveBayesMultinomialText0.setNormalizeDocLength(true);
      naiveBayesMultinomialText0.reset();
      naiveBayesMultinomialText0.tokenizerTipText();
      naiveBayesMultinomialText0.getLNorm();
      naiveBayesMultinomialText0.getPeriodicPruning();
      naiveBayesMultinomialText0.setMinWordFrequency(0);
      assertEquals(0.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertTrue(naiveBayesMultinomialText0.getNormalizeDocLength());
  }

  /**
  //Test case number: 7
  /*Coverage entropy=2.3978952727983707
  */
  @Test(timeout = 4000)
  public void test07()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.reset();
      AbstractClassifier.makeCopies(naiveBayesMultinomialText0, 110);
      SGDText sGDText0 = new SGDText();
      sGDText0.setLambda((-1616.7439426256));
      sGDText0.setOutputProbsForSVM(true);
      Tokenizer tokenizer0 = sGDText0.getTokenizer();
      String string0 = naiveBayesMultinomialText0.getRevision();
      assertEquals("9122", string0);
      
      SnowballStemmer snowballStemmer0 = new SnowballStemmer();
      naiveBayesMultinomialText0.m_stemmer = (Stemmer) snowballStemmer0;
      naiveBayesMultinomialText0.setTokenizer(tokenizer0);
      String string1 = naiveBayesMultinomialText0.normalizeDocLengthTipText();
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", string1);
      
      String string2 = naiveBayesMultinomialText0.minWordFrequencyTipText();
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", string2);
      
      IteratedLovinsStemmer iteratedLovinsStemmer0 = new IteratedLovinsStemmer();
      iteratedLovinsStemmer0.toString();
      naiveBayesMultinomialText0.setStemmer(iteratedLovinsStemmer0);
      String string3 = naiveBayesMultinomialText0.toString();
      assertEquals("NaiveBayesMultinomialText: No model built yet.\n", string3);
      
      double double0 = naiveBayesMultinomialText0.getMinWordFrequency();
      assertEquals(3.0, double0, 0.01);
      
      naiveBayesMultinomialText0.stemmerTipText();
      String string4 = naiveBayesMultinomialText0.stemmerTipText();
      assertEquals("The stemming algorithm to use on the words.", string4);
      
      boolean boolean0 = naiveBayesMultinomialText0.getUseWordFrequencies();
      assertFalse(boolean0);
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
  }

  /**
  //Test case number: 8
  /*Coverage entropy=2.5649493574615376
  */
  @Test(timeout = 4000)
  public void test08()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      assertNotNull(naiveBayesMultinomialText0);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      
      naiveBayesMultinomialText0.m_useStopList = false;
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      
      String string0 = naiveBayesMultinomialText0.getRevision();
      assertNotNull(string0);
      assertEquals("9122", string0);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      
      naiveBayesMultinomialText0.setLowercaseTokens(true);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertTrue(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      
      naiveBayesMultinomialText0.setPeriodicPruning(0);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertTrue(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      
      naiveBayesMultinomialText0.m_t = (double) 0;
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertTrue(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      
      WordTokenizer wordTokenizer0 = (WordTokenizer)naiveBayesMultinomialText0.m_tokenizer;
      assertNotNull(wordTokenizer0);
      assertEquals("Set of delimiter characters to use in tokenizing (\\r, \\n and \\t can be used for carriage-return, line-feed and tab)", wordTokenizer0.delimitersTipText());
      assertEquals("A simple tokenizer that is using the java.util.StringTokenizer class to tokenize the strings.", wordTokenizer0.globalInfo());
      assertEquals(" \r\n\t.,;:'\"()?!", wordTokenizer0.getDelimiters());
      
      naiveBayesMultinomialText0.setTokenizer(wordTokenizer0);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertTrue(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("Set of delimiter characters to use in tokenizing (\\r, \\n and \\t can be used for carriage-return, line-feed and tab)", wordTokenizer0.delimitersTipText());
      assertEquals("A simple tokenizer that is using the java.util.StringTokenizer class to tokenize the strings.", wordTokenizer0.globalInfo());
      assertEquals(" \r\n\t.,;:'\"()?!", wordTokenizer0.getDelimiters());
      
      naiveBayesMultinomialText0.setLowercaseTokens(true);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertTrue(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      
      naiveBayesMultinomialText0.m_lowercaseTokens = false;
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      
      double double0 = naiveBayesMultinomialText0.getNorm();
      assertEquals(1.0, double0, 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      
      String string1 = naiveBayesMultinomialText0.toString();
      assertNotNull(string1);
      assertFalse(string1.equals((Object)string0));
      assertEquals("NaiveBayesMultinomialText: No model built yet.\n", string1);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      
      naiveBayesMultinomialText0.setNormalizeDocLength(true);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertTrue(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      
      String string2 = naiveBayesMultinomialText0.normTipText();
      assertNotNull(string2);
      assertFalse(string2.equals((Object)string1));
      assertFalse(string2.equals((Object)string0));
      assertEquals("The norm of the instances after normalization.", string2);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertTrue(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      
      String string3 = naiveBayesMultinomialText0.normalizeDocLengthTipText();
      assertNotNull(string3);
      assertFalse(string3.equals((Object)string0));
      assertFalse(string3.equals((Object)string1));
      assertFalse(string3.equals((Object)string2));
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", string3);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertTrue(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      
      int int0 = naiveBayesMultinomialText0.getPeriodicPruning();
      assertEquals(0, int0);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertTrue(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      
      double double1 = naiveBayesMultinomialText0.getLNorm();
      assertNotEquals(double1, double0, 0.01);
      assertEquals(2.0, double1, 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertTrue(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      
      String string4 = naiveBayesMultinomialText0.getRevision();
      assertNotNull(string4);
      assertFalse(string4.equals((Object)string1));
      assertFalse(string4.equals((Object)string3));
      assertFalse(string4.equals((Object)string2));
      assertTrue(string4.equals((Object)string0));
      assertEquals("9122", string4);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertTrue(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      
      naiveBayesMultinomialText0.setUseWordFrequencies(true);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertTrue(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertTrue(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      
      String string5 = naiveBayesMultinomialText0.getRevision();
      assertNotNull(string5);
      assertTrue(string5.equals((Object)string4));
      assertFalse(string5.equals((Object)string2));
      assertTrue(string5.equals((Object)string0));
      assertFalse(string5.equals((Object)string3));
      assertFalse(string5.equals((Object)string1));
      assertEquals("9122", string5);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertTrue(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertTrue(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      
      naiveBayesMultinomialText0.setLowercaseTokens(true);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertTrue(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertTrue(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertTrue(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      
      naiveBayesMultinomialText0.setTokenizer(wordTokenizer0);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertTrue(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertTrue(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertTrue(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("Set of delimiter characters to use in tokenizing (\\r, \\n and \\t can be used for carriage-return, line-feed and tab)", wordTokenizer0.delimitersTipText());
      assertEquals("A simple tokenizer that is using the java.util.StringTokenizer class to tokenize the strings.", wordTokenizer0.globalInfo());
      assertEquals(" \r\n\t.,;:'\"()?!", wordTokenizer0.getDelimiters());
  }

  /**
  //Test case number: 9
  /*Coverage entropy=2.7184734013300127
  */
  @Test(timeout = 4000)
  public void test09()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      assertNotNull(naiveBayesMultinomialText0);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      
      boolean boolean0 = true;
      naiveBayesMultinomialText0.m_useStopList = true;
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertTrue(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      
      String[] stringArray0 = naiveBayesMultinomialText0.getOptions();
      assertNotNull(stringArray0);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertTrue(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(13, stringArray0.length);
      
      naiveBayesMultinomialText0.setNormalizeDocLength(true);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertTrue(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertTrue(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      
      naiveBayesMultinomialText0.m_minWordP = 0.0;
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertTrue(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(0.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertTrue(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      
      naiveBayesMultinomialText0.setLNorm(0.0);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertTrue(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(0.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(0.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertTrue(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      
      String string0 = naiveBayesMultinomialText0.useWordFrequenciesTipText();
      assertNotNull(string0);
      assertEquals("Use word frequencies rather than binary bag of words representation", string0);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertTrue(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(0.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(0.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertTrue(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      
      NullStemmer nullStemmer0 = (NullStemmer)naiveBayesMultinomialText0.m_stemmer;
      assertNotNull(nullStemmer0);
      assertEquals("A dummy stemmer that performs no stemming at all.", nullStemmer0.globalInfo());
      
      naiveBayesMultinomialText0.setStemmer(nullStemmer0);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertTrue(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(0.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(0.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertTrue(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("A dummy stemmer that performs no stemming at all.", nullStemmer0.globalInfo());
      
      naiveBayesMultinomialText0.setLNorm(2343.4);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertTrue(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals(2343.4, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(0.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertTrue(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      
      int int0 = 118;
      SparseInstance sparseInstance0 = new SparseInstance(118);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertNotNull(sparseInstance0);
      assertEquals(1.0, sparseInstance0.weight(), 0.01);
      assertEquals(118, sparseInstance0.numValues());
      assertEquals(118, sparseInstance0.numAttributes());
      
      BinarySparseInstance binarySparseInstance0 = new BinarySparseInstance((Instance) sparseInstance0);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertNotNull(binarySparseInstance0);
      assertEquals(1.0, sparseInstance0.weight(), 0.01);
      assertEquals(118, sparseInstance0.numValues());
      assertEquals(118, sparseInstance0.numAttributes());
      assertEquals(118, binarySparseInstance0.numAttributes());
      assertEquals(118, binarySparseInstance0.numValues());
      assertEquals(1.0, binarySparseInstance0.weight(), 0.01);
      
      // Undeclared exception!
      try { 
        binarySparseInstance0.relationalValue((-1629));
        fail("Expecting exception: RuntimeException");
      
      } catch(RuntimeException e) {
         //
         // DenseInstance doesn't have access to a dataset!
         //
         verifyException("weka.core.AbstractInstance", e);
      }
  }

  /**
  //Test case number: 10
  /*Coverage entropy=2.5665989181145106
  */
  @Test(timeout = 4000)
  public void test10()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      assertNotNull(naiveBayesMultinomialText0);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      
      String[] stringArray0 = new String[9];
      stringArray0[0] = "";
      stringArray0[1] = "-nominal-class";
      stringArray0[2] = "SpQjp&N9wt[t0gk+";
      stringArray0[3] = "";
      stringArray0[4] = "OLmsB(J";
      stringArray0[5] = "-string";
      stringArray0[6] = "Q?`nt_nv{JQKXAG";
      stringArray0[7] = "$Revision: 9122 $";
      stringArray0[8] = "";
      naiveBayesMultinomialText0.setOptions(stringArray0);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(9, stringArray0.length);
      
      naiveBayesMultinomialText0.setNorm(733.1474234);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(733.1474234, naiveBayesMultinomialText0.getNorm(), 0.01);
      
      String string0 = naiveBayesMultinomialText0.tokenizerTipText();
      assertNotNull(string0);
      assertEquals("The tokenizing algorithm to use on the strings.", string0);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(733.1474234, naiveBayesMultinomialText0.getNorm(), 0.01);
      
      Capabilities capabilities0 = naiveBayesMultinomialText0.getCapabilities();
      assertNotNull(capabilities0);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(733.1474234, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(capabilities0.hasDependencies());
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      
      String[] stringArray1 = new String[5];
      assertFalse(stringArray1.equals((Object)stringArray0));
      
      Capabilities capabilities1 = capabilities0.getClassCapabilities();
      assertNotSame(capabilities1, capabilities0);
      assertNotSame(capabilities0, capabilities1);
      assertNotNull(capabilities1);
      assertFalse(capabilities1.equals((Object)capabilities0));
      assertEquals(0, capabilities1.getMinimumNumberInstances());
      assertFalse(capabilities1.hasDependencies());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(733.1474234, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(capabilities0.hasDependencies());
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      
      stringArray1[0] = "weka/core/Capabilities.props";
      stringArray1[1] = "weka/core/Capabilities.props";
      stringArray1[2] = "";
      stringArray1[3] = "The tokenizing algorithm to use on the strings.";
      capabilities0.and(capabilities1);
      assertNotSame(capabilities1, capabilities0);
      assertNotSame(capabilities0, capabilities1);
      assertFalse(capabilities1.equals((Object)capabilities0));
      assertFalse(capabilities0.equals((Object)capabilities1));
      assertEquals(0, capabilities1.getMinimumNumberInstances());
      assertFalse(capabilities1.hasDependencies());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(733.1474234, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(capabilities0.hasDependencies());
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      
      stringArray1[4] = "weka/core/Capabilities.props";
      naiveBayesMultinomialText0.setOptions(stringArray1);
      assertNotSame(stringArray1, stringArray0);
      assertFalse(stringArray1.equals((Object)stringArray0));
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(733.1474234, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals(5, stringArray1.length);
      
      String string1 = naiveBayesMultinomialText0.globalInfo();
      assertNotNull(string1);
      assertFalse(string1.equals((Object)string0));
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", string1);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(733.1474234, naiveBayesMultinomialText0.getNorm(), 0.01);
  }

  /**
  //Test case number: 11
  /*Coverage entropy=1.3862943611198906
  */
  @Test(timeout = 4000)
  public void test11()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      assertNotNull(naiveBayesMultinomialText0);
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      
      String string0 = naiveBayesMultinomialText0.useWordFrequenciesTipText();
      assertNotNull(string0);
      assertEquals("Use word frequencies rather than binary bag of words representation", string0);
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      
      naiveBayesMultinomialText0.m_periodicP = 200;
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(200, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      
      SparseInstance sparseInstance0 = new SparseInstance(200);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertNotNull(sparseInstance0);
      assertEquals(200, sparseInstance0.numAttributes());
      assertEquals(200, sparseInstance0.numValues());
      assertEquals(1.0, sparseInstance0.weight(), 0.01);
      
      try { 
        naiveBayesMultinomialText0.updateClassifier(sparseInstance0);
        fail("Expecting exception: RuntimeException");
      
      } catch(RuntimeException e) {
         //
         // DenseInstance doesn't have access to a dataset!
         //
         verifyException("weka.core.AbstractInstance", e);
      }
  }

  /**
  //Test case number: 12
  /*Coverage entropy=2.9264175554979963
  */
  @Test(timeout = 4000)
  public void test12()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      assertNotNull(naiveBayesMultinomialText0);
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      
      String string0 = naiveBayesMultinomialText0.toString();
      assertNotNull(string0);
      assertEquals("NaiveBayesMultinomialText: No model built yet.\n", string0);
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      
      naiveBayesMultinomialText0.setUseStopList(true);
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertTrue(naiveBayesMultinomialText0.getUseStopList());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      
      naiveBayesMultinomialText0.pruneDictionary();
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertTrue(naiveBayesMultinomialText0.getUseStopList());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      
      String[] stringArray0 = naiveBayesMultinomialText0.getOptions();
      assertNotNull(stringArray0);
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertTrue(naiveBayesMultinomialText0.getUseStopList());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals(13, stringArray0.length);
      
      double double0 = naiveBayesMultinomialText0.getLNorm();
      assertEquals(2.0, double0, 0.01);
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertTrue(naiveBayesMultinomialText0.getUseStopList());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      
      String[] stringArray1 = new String[1];
      assertFalse(stringArray1.equals((Object)stringArray0));
      
      stringArray1[0] = "NaiveBayesMultinomialText: No model built yet.\n";
      NaiveBayesMultinomialText.main(stringArray1);
      assertNotSame(stringArray1, stringArray0);
      assertFalse(stringArray1.equals((Object)stringArray0));
      assertEquals(1, stringArray1.length);
      
      double[] doubleArray0 = new double[7];
      doubleArray0[0] = 2.0;
      doubleArray0[1] = 0.0;
      doubleArray0[2] = 0.0;
      doubleArray0[3] = 2.0;
      doubleArray0[4] = 2.0;
      doubleArray0[5] = (-1.0);
      doubleArray0[6] = 2.0;
      DenseInstance denseInstance0 = new DenseInstance(2.0, doubleArray0);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertNotNull(denseInstance0);
      assertArrayEquals(new double[] {2.0, 0.0, 0.0, 2.0, 2.0, (-1.0), 2.0}, doubleArray0, 0.01);
      assertEquals(2.0, denseInstance0.weight(), 0.01);
      assertEquals(7, denseInstance0.numAttributes());
      assertEquals(7, denseInstance0.numValues());
      assertEquals(7, doubleArray0.length);
      
      // Undeclared exception!
      try { 
        naiveBayesMultinomialText0.tokenizeInstance(denseInstance0, false);
        fail("Expecting exception: RuntimeException");
      
      } catch(RuntimeException e) {
         //
         // DenseInstance doesn't have access to a dataset!
         //
         verifyException("weka.core.AbstractInstance", e);
      }
  }

  /**
  //Test case number: 13
  /*Coverage entropy=3.2913361527953433
  */
  @Test(timeout = 4000)
  public void test13()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      assertNotNull(naiveBayesMultinomialText0);
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      
      naiveBayesMultinomialText0.setLowercaseTokens(true);
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertTrue(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      
      String string0 = naiveBayesMultinomialText0.useWordFrequenciesTipText();
      assertNotNull(string0);
      assertEquals("Use word frequencies rather than binary bag of words representation", string0);
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertTrue(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      
      naiveBayesMultinomialText0.m_periodicP = 200;
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(200, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertTrue(naiveBayesMultinomialText0.getLowercaseTokens());
      
      WordTokenizer wordTokenizer0 = (WordTokenizer)naiveBayesMultinomialText0.m_tokenizer;
      assertNotNull(wordTokenizer0);
      assertEquals("Set of delimiter characters to use in tokenizing (\\r, \\n and \\t can be used for carriage-return, line-feed and tab)", wordTokenizer0.delimitersTipText());
      assertEquals("A simple tokenizer that is using the java.util.StringTokenizer class to tokenize the strings.", wordTokenizer0.globalInfo());
      assertEquals(" \r\n\t.,;:'\"()?!", wordTokenizer0.getDelimiters());
      
      naiveBayesMultinomialText0.m_tokenizer = (Tokenizer) wordTokenizer0;
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(200, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertTrue(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("Set of delimiter characters to use in tokenizing (\\r, \\n and \\t can be used for carriage-return, line-feed and tab)", wordTokenizer0.delimitersTipText());
      assertEquals("A simple tokenizer that is using the java.util.StringTokenizer class to tokenize the strings.", wordTokenizer0.globalInfo());
      assertEquals(" \r\n\t.,;:'\"()?!", wordTokenizer0.getDelimiters());
      
      String[] stringArray0 = naiveBayesMultinomialText0.getOptions();
      assertNotNull(stringArray0);
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(200, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertTrue(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(13, stringArray0.length);
      
      naiveBayesMultinomialText0.setOptions(stringArray0);
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(200, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertTrue(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(13, stringArray0.length);
      
      naiveBayesMultinomialText0.pruneDictionary();
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(200, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertTrue(naiveBayesMultinomialText0.getLowercaseTokens());
  }

  /**
  //Test case number: 14
  /*Coverage entropy=1.3321790402101223
  */
  @Test(timeout = 4000)
  public void test14()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      assertNotNull(naiveBayesMultinomialText0);
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      
      naiveBayesMultinomialText0.setLowercaseTokens(true);
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertTrue(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      
      String string0 = naiveBayesMultinomialText0.useWordFrequenciesTipText();
      assertNotNull(string0);
      assertEquals("Use word frequencies rather than binary bag of words representation", string0);
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertTrue(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      
      naiveBayesMultinomialText0.m_periodicP = 200;
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(200, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertTrue(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      
      WordTokenizer wordTokenizer0 = (WordTokenizer)naiveBayesMultinomialText0.m_tokenizer;
      assertNotNull(wordTokenizer0);
      assertEquals("Set of delimiter characters to use in tokenizing (\\r, \\n and \\t can be used for carriage-return, line-feed and tab)", wordTokenizer0.delimitersTipText());
      assertEquals("A simple tokenizer that is using the java.util.StringTokenizer class to tokenize the strings.", wordTokenizer0.globalInfo());
      assertEquals(" \r\n\t.,;:'\"()?!", wordTokenizer0.getDelimiters());
      
      String[] stringArray0 = new String[6];
      stringArray0[0] = "Use word frequencies rather than binary bag of words representation";
      stringArray0[1] = "Use word frequencies rather than binary bag of words representation";
      stringArray0[2] = "Use word frequencies rather than binary bag of words representation";
      stringArray0[3] = "Use word frequencies rather than binary bag of words representation";
      stringArray0[4] = "Use word frequencies rather than binary bag of words representation";
      stringArray0[5] = "-not-missing-class-values";
      WordTokenizer.main(stringArray0);
      assertEquals(6, stringArray0.length);
      
      naiveBayesMultinomialText0.m_tokenizer = (Tokenizer) wordTokenizer0;
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(200, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertTrue(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("Set of delimiter characters to use in tokenizing (\\r, \\n and \\t can be used for carriage-return, line-feed and tab)", wordTokenizer0.delimitersTipText());
      assertEquals("A simple tokenizer that is using the java.util.StringTokenizer class to tokenize the strings.", wordTokenizer0.globalInfo());
      assertEquals(" \r\n\t.,;:'\"()?!", wordTokenizer0.getDelimiters());
      
      // Undeclared exception!
      try { 
        naiveBayesMultinomialText0.pruneDictionary();
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("weka.classifiers.bayes.NaiveBayesMultinomialText", e);
      }
  }

  /**
  //Test case number: 15
  /*Coverage entropy=3.2293848929426523
  */
  @Test(timeout = 4000)
  public void test15()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      assertNotNull(naiveBayesMultinomialText0);
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      
      File file0 = naiveBayesMultinomialText0.getStopwords();
      assertNotNull(file0);
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertTrue(file0.canWrite());
      assertFalse(file0.isHidden());
      assertEquals(0L, file0.getUsableSpace());
      assertEquals("/home/ubuntu/evosuite_readability_gen/projects", file0.getParent());
      assertTrue(file0.isAbsolute());
      assertTrue(file0.canRead());
      assertTrue(file0.canExecute());
      assertEquals("107_weka", file0.getName());
      assertEquals(0L, file0.length());
      assertTrue(file0.exists());
      assertEquals(0L, file0.getFreeSpace());
      assertEquals(0L, file0.getTotalSpace());
      assertEquals("/home/ubuntu/evosuite_readability_gen/projects/107_weka", file0.toString());
      assertTrue(file0.isDirectory());
      assertEquals(1392409281320L, file0.lastModified());
      assertFalse(file0.isFile());
      
      String string0 = naiveBayesMultinomialText0.getRevision();
      assertNotNull(string0);
      assertEquals("9122", string0);
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      
      String[] stringArray0 = new String[9];
      stringArray0[0] = "9122";
      stringArray0[1] = "9122";
      stringArray0[2] = "9122";
      LinkedHashMap<String, NaiveBayesMultinomialText.Count> linkedHashMap0 = naiveBayesMultinomialText0.m_inputVector;
      assertNull(linkedHashMap0);
      
      naiveBayesMultinomialText0.m_inputVector = null;
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      
      stringArray0[3] = "9122";
      stringArray0[4] = "9122";
      stringArray0[5] = "9122";
      stringArray0[6] = "9122";
      stringArray0[7] = "9122";
      naiveBayesMultinomialText0.m_stopwordsFile = file0;
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertTrue(file0.canWrite());
      assertFalse(file0.isHidden());
      assertEquals(0L, file0.getUsableSpace());
      assertEquals("/home/ubuntu/evosuite_readability_gen/projects", file0.getParent());
      assertTrue(file0.isAbsolute());
      assertTrue(file0.canRead());
      assertTrue(file0.canExecute());
      assertEquals("107_weka", file0.getName());
      assertEquals(0L, file0.length());
      assertTrue(file0.exists());
      assertEquals(0L, file0.getFreeSpace());
      assertEquals(0L, file0.getTotalSpace());
      assertEquals("/home/ubuntu/evosuite_readability_gen/projects/107_weka", file0.toString());
      assertTrue(file0.isDirectory());
      assertEquals(1392409281320L, file0.lastModified());
      assertFalse(file0.isFile());
      assertTrue(naiveBayesMultinomialText0.m_stopwordsFile.canWrite());
      assertEquals(0L, naiveBayesMultinomialText0.m_stopwordsFile.getUsableSpace());
      assertFalse(naiveBayesMultinomialText0.m_stopwordsFile.isFile());
      assertTrue(naiveBayesMultinomialText0.m_stopwordsFile.canRead());
      assertEquals("107_weka", naiveBayesMultinomialText0.m_stopwordsFile.getName());
      assertTrue(naiveBayesMultinomialText0.m_stopwordsFile.exists());
      assertEquals(0L, naiveBayesMultinomialText0.m_stopwordsFile.getTotalSpace());
      assertEquals(0L, naiveBayesMultinomialText0.m_stopwordsFile.getFreeSpace());
      assertEquals(0L, naiveBayesMultinomialText0.m_stopwordsFile.length());
      assertEquals(1392409281320L, naiveBayesMultinomialText0.m_stopwordsFile.lastModified());
      assertFalse(naiveBayesMultinomialText0.m_stopwordsFile.isHidden());
      assertEquals("/home/ubuntu/evosuite_readability_gen/projects/107_weka", naiveBayesMultinomialText0.m_stopwordsFile.toString());
      assertTrue(naiveBayesMultinomialText0.m_stopwordsFile.canExecute());
      assertEquals("/home/ubuntu/evosuite_readability_gen/projects", naiveBayesMultinomialText0.m_stopwordsFile.getParent());
      assertTrue(naiveBayesMultinomialText0.m_stopwordsFile.isDirectory());
      assertTrue(naiveBayesMultinomialText0.m_stopwordsFile.isAbsolute());
      
      stringArray0[8] = "9122";
      NaiveBayesMultinomialText.main(stringArray0);
      assertEquals(9, stringArray0.length);
      
      naiveBayesMultinomialText0.reset();
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      
      String string1 = naiveBayesMultinomialText0.LNormTipText();
      assertNotNull(string1);
      assertFalse(string1.equals((Object)string0));
      assertEquals("The LNorm to use for document length normalization.", string1);
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      
      naiveBayesMultinomialText0.setMinWordFrequency(1.0);
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      
      File file1 = MockFile.createTempFile("9122", "The LNorm to use for document length normalization.");
      assertNotSame(file1, file0);
      assertNotNull(file1);
      assertFalse(file1.equals((Object)file0));
      assertEquals(0L, file1.getTotalSpace());
      assertTrue(file1.exists());
      assertEquals(1392409281320L, file1.lastModified());
      assertEquals(0L, file1.length());
      assertEquals("/tmp/91220The LNorm to use for document length normalization.", file1.toString());
      assertTrue(file1.isAbsolute());
      assertFalse(file1.isDirectory());
      assertTrue(file1.canWrite());
      assertEquals(0L, file1.getFreeSpace());
      assertEquals(0L, file1.getUsableSpace());
      assertFalse(file1.isHidden());
      assertEquals("91220The LNorm to use for document length normalization.", file1.getName());
      assertEquals("/tmp", file1.getParent());
      assertTrue(file1.canExecute());
      assertTrue(file1.isFile());
      assertTrue(file1.canRead());
      
      naiveBayesMultinomialText0.setStopwords(file1);
      assertNotSame(file1, file0);
      assertFalse(file1.equals((Object)file0));
      assertTrue(naiveBayesMultinomialText0.getUseStopList());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(0L, file1.getTotalSpace());
      assertTrue(file1.exists());
      assertEquals(1392409281320L, file1.lastModified());
      assertEquals(0L, file1.length());
      assertEquals("/tmp/91220The LNorm to use for document length normalization.", file1.toString());
      assertTrue(file1.isAbsolute());
      assertFalse(file1.isDirectory());
      assertTrue(file1.canWrite());
      assertEquals(0L, file1.getFreeSpace());
      assertEquals(0L, file1.getUsableSpace());
      assertFalse(file1.isHidden());
      assertEquals("91220The LNorm to use for document length normalization.", file1.getName());
      assertEquals("/tmp", file1.getParent());
      assertTrue(file1.canExecute());
      assertTrue(file1.isFile());
      assertTrue(file1.canRead());
      
      naiveBayesMultinomialText0.setLNorm(1018.89739);
      assertTrue(naiveBayesMultinomialText0.getUseStopList());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals(1018.89739, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      
      String string2 = naiveBayesMultinomialText0.normalizeDocLengthTipText();
      assertNotNull(string2);
      assertFalse(string2.equals((Object)string0));
      assertFalse(string2.equals((Object)string1));
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", string2);
      assertTrue(naiveBayesMultinomialText0.getUseStopList());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals(1018.89739, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      
      WordTokenizer wordTokenizer0 = (WordTokenizer)naiveBayesMultinomialText0.getTokenizer();
      assertNotNull(wordTokenizer0);
      assertTrue(naiveBayesMultinomialText0.getUseStopList());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals(1018.89739, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("Set of delimiter characters to use in tokenizing (\\r, \\n and \\t can be used for carriage-return, line-feed and tab)", wordTokenizer0.delimitersTipText());
      assertEquals("A simple tokenizer that is using the java.util.StringTokenizer class to tokenize the strings.", wordTokenizer0.globalInfo());
      assertEquals(" \r\n\t.,;:'\"()?!", wordTokenizer0.getDelimiters());
      
      double double0 = naiveBayesMultinomialText0.getLNorm();
      assertEquals(1018.89739, double0, 0.01);
      assertTrue(naiveBayesMultinomialText0.getUseStopList());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals(1018.89739, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      
      String string3 = naiveBayesMultinomialText0.useWordFrequenciesTipText();
      assertNotNull(string3);
      assertFalse(string3.equals((Object)string0));
      assertFalse(string3.equals((Object)string2));
      assertFalse(string3.equals((Object)string1));
      assertEquals("Use word frequencies rather than binary bag of words representation", string3);
      assertTrue(naiveBayesMultinomialText0.getUseStopList());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals(1018.89739, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      
      String[] stringArray1 = naiveBayesMultinomialText0.getOptions();
      assertNotSame(stringArray1, stringArray0);
      assertNotNull(stringArray1);
      assertFalse(stringArray1.equals((Object)stringArray0));
      assertTrue(naiveBayesMultinomialText0.getUseStopList());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals(1018.89739, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(15, stringArray1.length);
      
      int int0 = naiveBayesMultinomialText0.getPeriodicPruning();
      assertEquals(0, int0);
      assertTrue(naiveBayesMultinomialText0.getUseStopList());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals(1018.89739, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      
      String string4 = naiveBayesMultinomialText0.useStopListTipText();
      assertNotNull(string4);
      assertFalse(string4.equals((Object)string0));
      assertFalse(string4.equals((Object)string3));
      assertFalse(string4.equals((Object)string2));
      assertFalse(string4.equals((Object)string1));
      assertEquals("If true, ignores all words that are on the stoplist.", string4);
      assertTrue(naiveBayesMultinomialText0.getUseStopList());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals(1018.89739, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      
      NullStemmer nullStemmer0 = (NullStemmer)naiveBayesMultinomialText0.getStemmer();
      assertNotNull(nullStemmer0);
      assertTrue(naiveBayesMultinomialText0.getUseStopList());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals(1018.89739, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("A dummy stemmer that performs no stemming at all.", nullStemmer0.globalInfo());
      
      naiveBayesMultinomialText0.pruneDictionary();
      assertTrue(naiveBayesMultinomialText0.getUseStopList());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals(1018.89739, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      
      String string5 = naiveBayesMultinomialText0.lowercaseTokensTipText();
      assertNotNull(string5);
      assertFalse(string5.equals((Object)string3));
      assertFalse(string5.equals((Object)string2));
      assertFalse(string5.equals((Object)string1));
      assertFalse(string5.equals((Object)string0));
      assertFalse(string5.equals((Object)string4));
      assertEquals("Whether to convert all tokens to lowercase", string5);
      assertTrue(naiveBayesMultinomialText0.getUseStopList());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals(1018.89739, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      
      naiveBayesMultinomialText0.setOptions(stringArray1);
      assertNotSame(stringArray1, stringArray0);
      assertFalse(stringArray1.equals((Object)stringArray0));
      assertTrue(naiveBayesMultinomialText0.getUseStopList());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals(1018.89739, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(15, stringArray1.length);
      
      naiveBayesMultinomialText0.setOptions(stringArray1);
      assertNotSame(stringArray1, stringArray0);
      assertFalse(stringArray1.equals((Object)stringArray0));
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals(1018.89739, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(15, stringArray1.length);
  }

  /**
  //Test case number: 16
  /*Coverage entropy=2.6136886560571986
  */
  @Test(timeout = 4000)
  public void test16()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      assertNotNull(naiveBayesMultinomialText0);
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      
      NaiveBayesMultinomialText naiveBayesMultinomialText1 = new NaiveBayesMultinomialText();
      assertNotNull(naiveBayesMultinomialText1);
      assertFalse(naiveBayesMultinomialText1.equals((Object)naiveBayesMultinomialText0));
      assertFalse(naiveBayesMultinomialText1.getUseStopList());
      assertFalse(naiveBayesMultinomialText1.getDebug());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText1.useWordFrequenciesTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText1.periodicPruningTipText());
      assertEquals(0, naiveBayesMultinomialText1.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText1.getNormalizeDocLength());
      assertEquals(3.0, naiveBayesMultinomialText1.getMinWordFrequency(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText1.useStopListTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText1.stemmerTipText());
      assertFalse(naiveBayesMultinomialText1.getLowercaseTokens());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText1.LNormTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText1.lowercaseTokensTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText1.tokenizerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText1.minWordFrequencyTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText1.stopwordsTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText1.normTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText1.globalInfo());
      assertFalse(naiveBayesMultinomialText1.getUseWordFrequencies());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText1.normalizeDocLengthTipText());
      assertEquals(2.0, naiveBayesMultinomialText1.getLNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText1.debugTipText());
      assertEquals(1.0, naiveBayesMultinomialText1.getNorm(), 0.01);
      
      String string0 = naiveBayesMultinomialText1.toString();
      assertNotSame(naiveBayesMultinomialText1, naiveBayesMultinomialText0);
      assertNotNull(string0);
      assertFalse(naiveBayesMultinomialText1.equals((Object)naiveBayesMultinomialText0));
      assertEquals("NaiveBayesMultinomialText: No model built yet.\n", string0);
      assertFalse(naiveBayesMultinomialText1.getUseStopList());
      assertFalse(naiveBayesMultinomialText1.getDebug());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText1.useWordFrequenciesTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText1.periodicPruningTipText());
      assertEquals(0, naiveBayesMultinomialText1.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText1.getNormalizeDocLength());
      assertEquals(3.0, naiveBayesMultinomialText1.getMinWordFrequency(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText1.useStopListTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText1.stemmerTipText());
      assertFalse(naiveBayesMultinomialText1.getLowercaseTokens());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText1.LNormTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText1.lowercaseTokensTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText1.tokenizerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText1.minWordFrequencyTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText1.stopwordsTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText1.normTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText1.globalInfo());
      assertFalse(naiveBayesMultinomialText1.getUseWordFrequencies());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText1.normalizeDocLengthTipText());
      assertEquals(2.0, naiveBayesMultinomialText1.getLNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText1.debugTipText());
      assertEquals(1.0, naiveBayesMultinomialText1.getNorm(), 0.01);
      
      naiveBayesMultinomialText0.setUseStopList(true);
      assertNotSame(naiveBayesMultinomialText0, naiveBayesMultinomialText1);
      assertFalse(naiveBayesMultinomialText0.equals((Object)naiveBayesMultinomialText1));
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertTrue(naiveBayesMultinomialText0.getUseStopList());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      
      naiveBayesMultinomialText0.pruneDictionary();
      assertNotSame(naiveBayesMultinomialText0, naiveBayesMultinomialText1);
      assertFalse(naiveBayesMultinomialText0.equals((Object)naiveBayesMultinomialText1));
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertTrue(naiveBayesMultinomialText0.getUseStopList());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      
      String[] stringArray0 = naiveBayesMultinomialText1.getOptions();
      assertNotSame(naiveBayesMultinomialText1, naiveBayesMultinomialText0);
      assertNotNull(stringArray0);
      assertFalse(naiveBayesMultinomialText1.equals((Object)naiveBayesMultinomialText0));
      assertFalse(naiveBayesMultinomialText1.getUseStopList());
      assertFalse(naiveBayesMultinomialText1.getDebug());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText1.useWordFrequenciesTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText1.periodicPruningTipText());
      assertEquals(0, naiveBayesMultinomialText1.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText1.getNormalizeDocLength());
      assertEquals(3.0, naiveBayesMultinomialText1.getMinWordFrequency(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText1.useStopListTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText1.stemmerTipText());
      assertFalse(naiveBayesMultinomialText1.getLowercaseTokens());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText1.LNormTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText1.lowercaseTokensTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText1.tokenizerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText1.minWordFrequencyTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText1.stopwordsTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText1.normTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText1.globalInfo());
      assertFalse(naiveBayesMultinomialText1.getUseWordFrequencies());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText1.normalizeDocLengthTipText());
      assertEquals(2.0, naiveBayesMultinomialText1.getLNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText1.debugTipText());
      assertEquals(1.0, naiveBayesMultinomialText1.getNorm(), 0.01);
      assertEquals(12, stringArray0.length);
      
      double double0 = naiveBayesMultinomialText1.getLNorm();
      assertNotSame(naiveBayesMultinomialText1, naiveBayesMultinomialText0);
      assertFalse(naiveBayesMultinomialText1.equals((Object)naiveBayesMultinomialText0));
      assertEquals(2.0, double0, 0.01);
      assertFalse(naiveBayesMultinomialText1.getUseStopList());
      assertFalse(naiveBayesMultinomialText1.getDebug());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText1.useWordFrequenciesTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText1.periodicPruningTipText());
      assertEquals(0, naiveBayesMultinomialText1.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText1.getNormalizeDocLength());
      assertEquals(3.0, naiveBayesMultinomialText1.getMinWordFrequency(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText1.useStopListTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText1.stemmerTipText());
      assertFalse(naiveBayesMultinomialText1.getLowercaseTokens());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText1.LNormTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText1.lowercaseTokensTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText1.tokenizerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText1.minWordFrequencyTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText1.stopwordsTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText1.normTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText1.globalInfo());
      assertFalse(naiveBayesMultinomialText1.getUseWordFrequencies());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText1.normalizeDocLengthTipText());
      assertEquals(2.0, naiveBayesMultinomialText1.getLNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText1.debugTipText());
      assertEquals(1.0, naiveBayesMultinomialText1.getNorm(), 0.01);
      
      TestInstances testInstances0 = new TestInstances();
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertNotNull(testInstances0);
      assertEquals(0, testInstances0.getNumString());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(20, testInstances0.getNumInstances());
      assertFalse(testInstances0.getNoClass());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      
      Instances instances0 = testInstances0.generate("NaiveBayesMultinomialText: No model built yet.\n");
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertNotNull(instances0);
      assertEquals(0, testInstances0.getNumString());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(20, testInstances0.getNumInstances());
      assertFalse(testInstances0.getNoClass());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(2, instances0.numClasses());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20, instances0.size());
      assertEquals(20, instances0.numInstances());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(1, instances0.classIndex());
      assertEquals(2, instances0.numAttributes());
      
      naiveBayesMultinomialText1.buildClassifier(instances0);
      assertNotSame(naiveBayesMultinomialText1, naiveBayesMultinomialText0);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertFalse(naiveBayesMultinomialText1.equals((Object)naiveBayesMultinomialText0));
      assertFalse(naiveBayesMultinomialText1.getUseStopList());
      assertFalse(naiveBayesMultinomialText1.getDebug());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText1.useWordFrequenciesTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText1.periodicPruningTipText());
      assertEquals(0, naiveBayesMultinomialText1.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText1.getNormalizeDocLength());
      assertEquals(3.0, naiveBayesMultinomialText1.getMinWordFrequency(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText1.useStopListTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText1.stemmerTipText());
      assertFalse(naiveBayesMultinomialText1.getLowercaseTokens());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText1.LNormTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText1.lowercaseTokensTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText1.tokenizerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText1.minWordFrequencyTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText1.stopwordsTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText1.normTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText1.globalInfo());
      assertFalse(naiveBayesMultinomialText1.getUseWordFrequencies());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText1.normalizeDocLengthTipText());
      assertEquals(2.0, naiveBayesMultinomialText1.getLNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText1.debugTipText());
      assertEquals(1.0, naiveBayesMultinomialText1.getNorm(), 0.01);
      assertEquals(0, testInstances0.getNumString());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(20, testInstances0.getNumInstances());
      assertFalse(testInstances0.getNoClass());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(2, instances0.numClasses());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20, instances0.size());
      assertEquals(20, instances0.numInstances());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(1, instances0.classIndex());
      assertEquals(2, instances0.numAttributes());
      
      String[] stringArray1 = naiveBayesMultinomialText0.getOptions();
      assertNotSame(naiveBayesMultinomialText0, naiveBayesMultinomialText1);
      assertNotSame(stringArray1, stringArray0);
      assertNotNull(stringArray1);
      assertFalse(naiveBayesMultinomialText0.equals((Object)naiveBayesMultinomialText1));
      assertFalse(stringArray1.equals((Object)stringArray0));
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertTrue(naiveBayesMultinomialText0.getUseStopList());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals(13, stringArray1.length);
      
      Random.setNextRandom(10000);
      String string1 = naiveBayesMultinomialText1.toString();
      assertNotSame(naiveBayesMultinomialText1, naiveBayesMultinomialText0);
      assertNotNull(string1);
      assertFalse(naiveBayesMultinomialText1.equals((Object)naiveBayesMultinomialText0));
      assertFalse(string1.equals((Object)string0));
      assertEquals("The independent probability of a class\n--------------------------------------\nclass1\t12.0\nclass2\t10.0\n\nThe probability of a word given the class\n-----------------------------------------\n\tclass1\tclass2\t\n", string1);
      assertFalse(naiveBayesMultinomialText1.getUseStopList());
      assertFalse(naiveBayesMultinomialText1.getDebug());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText1.useWordFrequenciesTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText1.periodicPruningTipText());
      assertEquals(0, naiveBayesMultinomialText1.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText1.getNormalizeDocLength());
      assertEquals(3.0, naiveBayesMultinomialText1.getMinWordFrequency(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText1.useStopListTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText1.stemmerTipText());
      assertFalse(naiveBayesMultinomialText1.getLowercaseTokens());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText1.LNormTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText1.lowercaseTokensTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText1.tokenizerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText1.minWordFrequencyTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText1.stopwordsTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText1.normTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText1.globalInfo());
      assertFalse(naiveBayesMultinomialText1.getUseWordFrequencies());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText1.normalizeDocLengthTipText());
      assertEquals(2.0, naiveBayesMultinomialText1.getLNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText1.debugTipText());
      assertEquals(1.0, naiveBayesMultinomialText1.getNorm(), 0.01);
  }

  /**
  //Test case number: 17
  /*Coverage entropy=2.685945324669788
  */
  @Test(timeout = 4000)
  public void test17()  throws Throwable  {
      byte[] byteArray0 = new byte[2];
      byteArray0[0] = (byte)29;
      byteArray0[1] = (byte)119;
      boolean boolean0 = FileSystemHandling.appendDataToFile((EvoSuiteFile) null, byteArray0);
      assertArrayEquals(new byte[] {(byte)29, (byte)119}, byteArray0);
      assertFalse(boolean0);
      assertEquals(2, byteArray0.length);
      
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      assertNotNull(naiveBayesMultinomialText0);
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      
      naiveBayesMultinomialText0.pruneDictionary();
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      
      LinkedHashMap<Integer, LinkedHashMap<String, NaiveBayesMultinomialText.Count>> linkedHashMap0 = new LinkedHashMap<Integer, LinkedHashMap<String, NaiveBayesMultinomialText.Count>>();
      assertNotNull(linkedHashMap0);
      assertTrue(linkedHashMap0.isEmpty());
      assertEquals(0, linkedHashMap0.size());
      
      naiveBayesMultinomialText0.m_probOfWordGivenClass = (Map<Integer, LinkedHashMap<String, NaiveBayesMultinomialText.Count>>) linkedHashMap0;
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertTrue(linkedHashMap0.isEmpty());
      assertEquals(0, linkedHashMap0.size());
      assertEquals(0, naiveBayesMultinomialText0.m_probOfWordGivenClass.size());
      assertTrue(naiveBayesMultinomialText0.m_probOfWordGivenClass.isEmpty());
      
      String string0 = naiveBayesMultinomialText0.toString();
      assertNotNull(string0);
      assertEquals("NaiveBayesMultinomialText: No model built yet.\n", string0);
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      
      naiveBayesMultinomialText0.m_normalize = true;
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertTrue(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      
      Random.setNextRandom((byte)119);
      IteratedLovinsStemmer iteratedLovinsStemmer0 = new IteratedLovinsStemmer();
      assertNotNull(iteratedLovinsStemmer0);
      
      naiveBayesMultinomialText0.m_stemmer = (Stemmer) iteratedLovinsStemmer0;
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertTrue(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      
      String[] stringArray0 = naiveBayesMultinomialText0.getOptions();
      assertNotNull(stringArray0);
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertTrue(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals(13, stringArray0.length);
      
      String string1 = naiveBayesMultinomialText0.toString();
      assertNotNull(string1);
      assertEquals("NaiveBayesMultinomialText: No model built yet.\n", string1);
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertTrue(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
  }

  /**
  //Test case number: 18
  /*Coverage entropy=2.964177170412887
  */
  @Test(timeout = 4000)
  public void test18()  throws Throwable  {
      boolean boolean0 = FileSystemHandling.appendLineToFile((EvoSuiteFile) null, "L?!\"n|4m3B2");
      assertFalse(boolean0);
      
      BinarySparseInstance binarySparseInstance0 = new BinarySparseInstance(5);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertNotNull(binarySparseInstance0);
      assertEquals(1.0, binarySparseInstance0.weight(), 0.01);
      assertEquals(5, binarySparseInstance0.numValues());
      assertEquals(5, binarySparseInstance0.numAttributes());
      
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      assertNotNull(naiveBayesMultinomialText0);
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      
      naiveBayesMultinomialText0.setUseStopList(true);
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertTrue(naiveBayesMultinomialText0.getUseStopList());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      
      naiveBayesMultinomialText0.pruneDictionary();
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertTrue(naiveBayesMultinomialText0.getUseStopList());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      
      String[] stringArray0 = naiveBayesMultinomialText0.getOptions();
      assertNotNull(stringArray0);
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertTrue(naiveBayesMultinomialText0.getUseStopList());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals(13, stringArray0.length);
      
      double double0 = naiveBayesMultinomialText0.getLNorm();
      assertEquals(2.0, double0, 0.01);
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertTrue(naiveBayesMultinomialText0.getUseStopList());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      
      TestInstances testInstances0 = new TestInstances();
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertNotNull(testInstances0);
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertFalse(testInstances0.getNoClass());
      
      Instances instances0 = testInstances0.generate("weka.filters.unsupervised.attribute.MakeIndicator");
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertNotNull(instances0);
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertFalse(testInstances0.getNoClass());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20, instances0.size());
      assertEquals(2, instances0.numClasses());
      assertEquals(2, instances0.numAttributes());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals("Testdata", instances0.relationName());
      assertEquals(1, instances0.classIndex());
      assertEquals(20, instances0.numInstances());
      
      naiveBayesMultinomialText0.buildClassifier(instances0);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertTrue(naiveBayesMultinomialText0.getUseStopList());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertFalse(testInstances0.getNoClass());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20, instances0.size());
      assertEquals(2, instances0.numClasses());
      assertEquals(2, instances0.numAttributes());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals("Testdata", instances0.relationName());
      assertEquals(1, instances0.classIndex());
      assertEquals(20, instances0.numInstances());
      
      String[] stringArray1 = naiveBayesMultinomialText0.getOptions();
      assertNotSame(stringArray1, stringArray0);
      assertNotNull(stringArray1);
      assertFalse(stringArray1.equals((Object)stringArray0));
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertTrue(naiveBayesMultinomialText0.getUseStopList());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals(13, stringArray1.length);
      
      Random.setNextRandom(1936);
      String string0 = naiveBayesMultinomialText0.toString();
      assertNotNull(string0);
      assertEquals("The independent probability of a class\n--------------------------------------\nclass1\t12.0\nclass2\t10.0\n\nThe probability of a word given the class\n-----------------------------------------\n\tclass1\tclass2\t\n", string0);
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertTrue(naiveBayesMultinomialText0.getUseStopList());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      
      naiveBayesMultinomialText0.m_wordFrequencies = true;
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertTrue(naiveBayesMultinomialText0.getUseStopList());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertTrue(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      
      String[] stringArray2 = naiveBayesMultinomialText0.getOptions();
      assertNotSame(stringArray2, stringArray0);
      assertNotSame(stringArray2, stringArray1);
      assertNotNull(stringArray2);
      assertFalse(stringArray2.equals((Object)stringArray0));
      assertFalse(stringArray2.equals((Object)stringArray1));
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertTrue(naiveBayesMultinomialText0.getUseStopList());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertTrue(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals(14, stringArray2.length);
      
      System.setCurrentTimeMillis(5);
      naiveBayesMultinomialText0.setOptions(stringArray0);
      assertNotSame(stringArray0, stringArray1);
      assertNotSame(stringArray0, stringArray2);
      assertFalse(stringArray0.equals((Object)stringArray1));
      assertFalse(stringArray0.equals((Object)stringArray2));
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertTrue(naiveBayesMultinomialText0.getUseStopList());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals(13, stringArray0.length);
  }

  /**
  //Test case number: 19
  /*Coverage entropy=1.611796945455623
  */
  @Test(timeout = 4000)
  public void test19()  throws Throwable  {
      BinarySparseInstance binarySparseInstance0 = new BinarySparseInstance(5);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertNotNull(binarySparseInstance0);
      assertEquals(1.0, binarySparseInstance0.weight(), 0.01);
      assertEquals(5, binarySparseInstance0.numValues());
      assertEquals(5, binarySparseInstance0.numAttributes());
      
      SparseInstance sparseInstance0 = new SparseInstance(5);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertNotNull(sparseInstance0);
      assertEquals(1.0, sparseInstance0.weight(), 0.01);
      assertEquals(5, sparseInstance0.numAttributes());
      assertEquals(5, sparseInstance0.numValues());
      
      LinkedList<Locale.LanguageRange> linkedList0 = new LinkedList<Locale.LanguageRange>();
      assertNotNull(linkedList0);
      assertEquals(0, linkedList0.size());
      
      Locale locale0 = Locale.FRANCE;
      assertNotNull(locale0);
      assertEquals("fra", locale0.getISO3Language());
      assertEquals("", locale0.getVariant());
      assertEquals("FRA", locale0.getISO3Country());
      assertEquals("fr_FR", locale0.toString());
      assertEquals("fr", locale0.getLanguage());
      assertEquals("FR", locale0.getCountry());
      
      Set<String> set0 = locale0.getUnicodeLocaleKeys();
      assertNotNull(set0);
      assertEquals("fra", locale0.getISO3Language());
      assertEquals("", locale0.getVariant());
      assertEquals("FRA", locale0.getISO3Country());
      assertEquals("fr_FR", locale0.toString());
      assertEquals("fr", locale0.getLanguage());
      assertEquals("FR", locale0.getCountry());
      assertTrue(set0.isEmpty());
      assertEquals(0, set0.size());
      
      List<String> list0 = Locale.filterTags((List<Locale.LanguageRange>) linkedList0, (Collection<String>) set0);
      assertNotNull(list0);
      assertEquals(0, linkedList0.size());
      assertEquals("fra", locale0.getISO3Language());
      assertEquals("", locale0.getVariant());
      assertEquals("FRA", locale0.getISO3Country());
      assertEquals("fr_FR", locale0.toString());
      assertEquals("fr", locale0.getLanguage());
      assertEquals("FR", locale0.getCountry());
      assertTrue(set0.isEmpty());
      assertEquals(0, set0.size());
      assertEquals(0, list0.size());
      assertTrue(list0.isEmpty());
      
      Properties properties0 = new Properties();
      assertNotNull(properties0);
      assertTrue(properties0.isEmpty());
      assertEquals(0, properties0.size());
      
      ProtectedProperties protectedProperties0 = new ProtectedProperties(properties0);
      assertNotNull(protectedProperties0);
      assertTrue(properties0.isEmpty());
      assertEquals(0, properties0.size());
      assertEquals(0, protectedProperties0.size());
      assertTrue(protectedProperties0.isEmpty());
      
      ProtectedProperties protectedProperties1 = new ProtectedProperties(protectedProperties0);
      assertNotNull(protectedProperties1);
      assertTrue(protectedProperties1.equals((Object)protectedProperties0));
      assertTrue(properties0.isEmpty());
      assertEquals(0, properties0.size());
      assertEquals(0, protectedProperties0.size());
      assertTrue(protectedProperties0.isEmpty());
      assertTrue(protectedProperties1.isEmpty());
      assertEquals(0, protectedProperties1.size());
      
      ProtectedProperties protectedProperties2 = new ProtectedProperties(protectedProperties1);
      assertNotNull(protectedProperties2);
      assertTrue(protectedProperties0.equals((Object)protectedProperties1));
      assertTrue(protectedProperties1.equals((Object)protectedProperties0));
      assertTrue(protectedProperties2.equals((Object)protectedProperties0));
      assertTrue(protectedProperties2.equals((Object)protectedProperties1));
      assertTrue(properties0.isEmpty());
      assertEquals(0, properties0.size());
      assertEquals(0, protectedProperties0.size());
      assertTrue(protectedProperties0.isEmpty());
      assertTrue(protectedProperties1.isEmpty());
      assertEquals(0, protectedProperties1.size());
      assertTrue(protectedProperties2.isEmpty());
      assertEquals(0, protectedProperties2.size());
      
      Attribute attribute0 = new Attribute("stopwords", list0, protectedProperties2);
      assertEquals(2, Attribute.ORDERING_MODULO);
      assertEquals(4, Attribute.RELATIONAL);
      assertEquals(2, Attribute.STRING);
      assertEquals(1, Attribute.NOMINAL);
      assertEquals(3, Attribute.DATE);
      assertEquals(1, Attribute.ORDERING_ORDERED);
      assertEquals(0, Attribute.ORDERING_SYMBOLIC);
      assertEquals(0, Attribute.NUMERIC);
      assertNotNull(attribute0);
      assertFalse(set0.contains("stopwords"));
      assertFalse(list0.contains("stopwords"));
      assertTrue(protectedProperties0.equals((Object)protectedProperties1));
      assertTrue(protectedProperties0.equals((Object)protectedProperties2));
      assertTrue(protectedProperties1.equals((Object)protectedProperties0));
      assertTrue(protectedProperties1.equals((Object)protectedProperties2));
      assertTrue(protectedProperties2.equals((Object)protectedProperties0));
      assertTrue(protectedProperties2.equals((Object)protectedProperties1));
      assertEquals(0, linkedList0.size());
      assertEquals("fra", locale0.getISO3Language());
      assertEquals("", locale0.getVariant());
      assertEquals("FRA", locale0.getISO3Country());
      assertEquals("fr_FR", locale0.toString());
      assertEquals("fr", locale0.getLanguage());
      assertEquals("FR", locale0.getCountry());
      assertTrue(set0.isEmpty());
      assertEquals(0, set0.size());
      assertEquals(0, list0.size());
      assertTrue(list0.isEmpty());
      assertTrue(properties0.isEmpty());
      assertEquals(0, properties0.size());
      assertEquals(0, protectedProperties0.size());
      assertTrue(protectedProperties0.isEmpty());
      assertTrue(protectedProperties1.isEmpty());
      assertEquals(0, protectedProperties1.size());
      assertTrue(protectedProperties2.isEmpty());
      assertEquals(0, protectedProperties2.size());
      assertFalse(attribute0.isNumeric());
      assertEquals("stopwords", attribute0.name());
      assertFalse(attribute0.isAveragable());
      assertFalse(attribute0.hasZeropoint());
      assertEquals((-1), attribute0.index());
      assertEquals("", attribute0.getDateFormat());
      assertEquals(0, attribute0.ordering());
      assertEquals(1.0, attribute0.weight(), 0.01);
      assertEquals(0.0, attribute0.getLowerNumericBound(), 0.01);
      assertEquals(0.0, attribute0.getUpperNumericBound(), 0.01);
      assertFalse(attribute0.isString());
      assertFalse(attribute0.isDate());
      assertFalse(attribute0.isRelationValued());
      assertTrue(attribute0.isNominal());
      assertEquals(0, attribute0.numValues());
      assertFalse(attribute0.upperNumericBoundIsOpen());
      assertFalse(attribute0.lowerNumericBoundIsOpen());
      assertEquals(1, attribute0.type());
      assertFalse(attribute0.isRegular());
      
      sparseInstance0.setValue(attribute0, (double) 5);
      assertNotSame(protectedProperties0, protectedProperties2);
      assertNotSame(protectedProperties0, protectedProperties1);
      assertNotSame(protectedProperties1, protectedProperties2);
      assertNotSame(protectedProperties1, protectedProperties0);
      assertNotSame(protectedProperties2, protectedProperties0);
      assertNotSame(protectedProperties2, protectedProperties1);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertEquals(2, Attribute.ORDERING_MODULO);
      assertEquals(4, Attribute.RELATIONAL);
      assertEquals(2, Attribute.STRING);
      assertEquals(1, Attribute.NOMINAL);
      assertEquals(3, Attribute.DATE);
      assertEquals(1, Attribute.ORDERING_ORDERED);
      assertEquals(0, Attribute.ORDERING_SYMBOLIC);
      assertEquals(0, Attribute.NUMERIC);
      assertFalse(set0.contains("stopwords"));
      assertFalse(list0.contains("stopwords"));
      assertTrue(protectedProperties0.equals((Object)protectedProperties1));
      assertTrue(protectedProperties0.equals((Object)protectedProperties2));
      assertTrue(protectedProperties1.equals((Object)protectedProperties0));
      assertTrue(protectedProperties1.equals((Object)protectedProperties2));
      assertTrue(protectedProperties2.equals((Object)protectedProperties0));
      assertTrue(protectedProperties2.equals((Object)protectedProperties1));
      assertEquals(1.0, sparseInstance0.weight(), 0.01);
      assertEquals(6, sparseInstance0.numValues());
      assertEquals(5, sparseInstance0.numAttributes());
      assertEquals(0, linkedList0.size());
      assertEquals("fra", locale0.getISO3Language());
      assertEquals("", locale0.getVariant());
      assertEquals("FRA", locale0.getISO3Country());
      assertEquals("fr_FR", locale0.toString());
      assertEquals("fr", locale0.getLanguage());
      assertEquals("FR", locale0.getCountry());
      assertTrue(set0.isEmpty());
      assertEquals(0, set0.size());
      assertEquals(0, list0.size());
      assertTrue(list0.isEmpty());
      assertTrue(properties0.isEmpty());
      assertEquals(0, properties0.size());
      assertEquals(0, protectedProperties0.size());
      assertTrue(protectedProperties0.isEmpty());
      assertTrue(protectedProperties1.isEmpty());
      assertEquals(0, protectedProperties1.size());
      assertTrue(protectedProperties2.isEmpty());
      assertEquals(0, protectedProperties2.size());
      assertFalse(attribute0.isNumeric());
      assertEquals("stopwords", attribute0.name());
      assertFalse(attribute0.isAveragable());
      assertFalse(attribute0.hasZeropoint());
      assertEquals((-1), attribute0.index());
      assertEquals("", attribute0.getDateFormat());
      assertEquals(0, attribute0.ordering());
      assertEquals(1.0, attribute0.weight(), 0.01);
      assertEquals(0.0, attribute0.getLowerNumericBound(), 0.01);
      assertEquals(0.0, attribute0.getUpperNumericBound(), 0.01);
      assertFalse(attribute0.isString());
      assertFalse(attribute0.isDate());
      assertFalse(attribute0.isRelationValued());
      assertTrue(attribute0.isNominal());
      assertEquals(0, attribute0.numValues());
      assertFalse(attribute0.upperNumericBoundIsOpen());
      assertFalse(attribute0.lowerNumericBoundIsOpen());
      assertEquals(1, attribute0.type());
      assertFalse(attribute0.isRegular());
      
      String string0 = sparseInstance0.toStringNoWeight();
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertNotNull(string0);
      assertEquals("{-1 5,0 ?,1 ?,2 ?,3 ?,4 ?}", string0);
      assertEquals(1.0, sparseInstance0.weight(), 0.01);
      assertEquals(6, sparseInstance0.numValues());
      assertEquals(5, sparseInstance0.numAttributes());
      
      TestInstances testInstances0 = new TestInstances();
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertNotNull(testInstances0);
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertFalse(testInstances0.getNoClass());
      assertEquals(1, testInstances0.getClassType());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertFalse(testInstances0.getMultiInstance());
      
      Instances instances0 = testInstances0.generate("");
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertNotNull(instances0);
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertFalse(testInstances0.getNoClass());
      assertEquals(1, testInstances0.getClassType());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(20, instances0.numInstances());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(2, instances0.numAttributes());
      assertEquals(2, instances0.numClasses());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20, instances0.size());
      assertEquals(1, instances0.classIndex());
      
      boolean boolean0 = instances0.add((Instance) sparseInstance0);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertTrue(boolean0);
      assertEquals(1.0, sparseInstance0.weight(), 0.01);
      assertEquals(6, sparseInstance0.numValues());
      assertEquals(5, sparseInstance0.numAttributes());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertFalse(testInstances0.getNoClass());
      assertEquals(1, testInstances0.getClassType());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(2, instances0.numAttributes());
      assertEquals(21, instances0.size());
      assertEquals(2, instances0.numClasses());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(21.0, instances0.sumOfWeights(), 0.01);
      assertEquals(1, instances0.classIndex());
      assertEquals(21, instances0.numInstances());
      
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      assertNotNull(naiveBayesMultinomialText0);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      
      naiveBayesMultinomialText0.buildClassifier(instances0);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertFalse(testInstances0.getNoClass());
      assertEquals(1, testInstances0.getClassType());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(2, instances0.numAttributes());
      assertEquals(21, instances0.size());
      assertEquals(2, instances0.numClasses());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(21.0, instances0.sumOfWeights(), 0.01);
      assertEquals(1, instances0.classIndex());
      assertEquals(21, instances0.numInstances());
      
      Random.setNextRandom(6);
      Random.setNextRandom(5);
  }

  /**
  //Test case number: 20
  /*Coverage entropy=1.9505268740410797
  */
  @Test(timeout = 4000)
  public void test20()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      assertNotNull(naiveBayesMultinomialText0);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      
      String string0 = naiveBayesMultinomialText0.toString();
      assertNotNull(string0);
      assertEquals("NaiveBayesMultinomialText: No model built yet.\n", string0);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      
      naiveBayesMultinomialText0.setUseStopList(true);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertTrue(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      
      naiveBayesMultinomialText0.pruneDictionary();
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertTrue(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      
      NaiveBayesMultinomialText naiveBayesMultinomialText1 = new NaiveBayesMultinomialText();
      assertNotNull(naiveBayesMultinomialText1);
      assertFalse(naiveBayesMultinomialText1.equals((Object)naiveBayesMultinomialText0));
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText1.globalInfo());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText1.stopwordsTipText());
      assertFalse(naiveBayesMultinomialText1.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText1.getUseStopList());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText1.periodicPruningTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText1.useStopListTipText());
      assertEquals(0, naiveBayesMultinomialText1.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText1.useWordFrequenciesTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText1.stemmerTipText());
      assertFalse(naiveBayesMultinomialText1.getLowercaseTokens());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText1.LNormTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText1.tokenizerTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText1.normalizeDocLengthTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText1.lowercaseTokensTipText());
      assertEquals(3.0, naiveBayesMultinomialText1.getMinWordFrequency(), 0.01);
      assertFalse(naiveBayesMultinomialText1.getNormalizeDocLength());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText1.minWordFrequencyTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText1.debugTipText());
      assertEquals(2.0, naiveBayesMultinomialText1.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText1.getDebug());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText1.normTipText());
      assertEquals(1.0, naiveBayesMultinomialText1.getNorm(), 0.01);
      
      WordTokenizer wordTokenizer0 = (WordTokenizer)naiveBayesMultinomialText1.m_tokenizer;
      assertNotNull(wordTokenizer0);
      assertEquals(" \r\n\t.,;:'\"()?!", wordTokenizer0.getDelimiters());
      assertEquals("A simple tokenizer that is using the java.util.StringTokenizer class to tokenize the strings.", wordTokenizer0.globalInfo());
      assertEquals("Set of delimiter characters to use in tokenizing (\\r, \\n and \\t can be used for carriage-return, line-feed and tab)", wordTokenizer0.delimitersTipText());
      
      String[] stringArray0 = new String[8];
      stringArray0[0] = "";
      stringArray0[1] = "";
      stringArray0[2] = "B;lKWv6~PY-";
      stringArray0[3] = "Rq[@OKGG=k'jZSS";
      stringArray0[4] = "-tokenizer";
      stringArray0[5] = " ";
      stringArray0[6] = "(cPF|b4?qs>,'";
      stringArray0[7] = "(cPF|b4?qs>,'";
      try { 
        naiveBayesMultinomialText1.setOptions(stringArray0);
        fail("Expecting exception: Exception");
      
      } catch(Exception e) {
         //
         // Invalid tokenizer specification string
         //
         verifyException("weka.classifiers.bayes.NaiveBayesMultinomialText", e);
      }
  }

  /**
  //Test case number: 21
  /*Coverage entropy=2.8309404302537744
  */
  @Test(timeout = 4000)
  public void test21()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      assertNotNull(naiveBayesMultinomialText0);
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      
      MultiClassClassifierUpdateable multiClassClassifierUpdateable0 = new MultiClassClassifierUpdateable();
      assertEquals(3, MultiClassClassifier.METHOD_1_AGAINST_1);
      assertEquals(1, MultiClassClassifier.METHOD_ERROR_RANDOM);
      assertEquals(0, MultiClassClassifier.METHOD_1_AGAINST_ALL);
      assertEquals(2, MultiClassClassifier.METHOD_ERROR_EXHAUSTIVE);
      assertNotNull(multiClassClassifierUpdateable0);
      assertEquals("Sets the method to use for transforming the multi-class problem into several 2-class ones.", multiClassClassifierUpdateable0.methodTipText());
      assertEquals(2.0, multiClassClassifierUpdateable0.getRandomWidthFactor(), 0.01);
      assertEquals("The base classifier to be used.", multiClassClassifierUpdateable0.classifierTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", multiClassClassifierUpdateable0.debugTipText());
      assertEquals(1, multiClassClassifierUpdateable0.getSeed());
      assertFalse(multiClassClassifierUpdateable0.getUsePairwiseCoupling());
      assertEquals("Use pairwise coupling (only has an effect for 1-against-1).", multiClassClassifierUpdateable0.usePairwiseCouplingTipText());
      assertEquals("The random number seed to be used.", multiClassClassifierUpdateable0.seedTipText());
      assertFalse(multiClassClassifierUpdateable0.getDebug());
      assertEquals("A metaclassifier for handling multi-class datasets with 2-class classifiers. This classifier is also capable of applying error correcting output codes for increased accuracy. The base classifier must be an updateable classifier", multiClassClassifierUpdateable0.globalInfo());
      assertEquals("Sets the width multiplier when using random codes. The number of codes generated will be thus number multiplied by the number of classes.", multiClassClassifierUpdateable0.randomWidthFactorTipText());
      
      Capabilities capabilities0 = naiveBayesMultinomialText0.getCapabilities();
      assertNotNull(capabilities0);
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      assertFalse(capabilities0.hasDependencies());
      
      TestInstances testInstances0 = TestInstances.forCapabilities(capabilities0);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertNotNull(testInstances0);
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      assertFalse(capabilities0.hasDependencies());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(4, testInstances0.getNumClasses());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getNumRelationalString());
      assertEquals(5, testInstances0.getNumAttributes());
      assertEquals(1, testInstances0.getNumString());
      assertFalse(testInstances0.getNoClass());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      
      Instances instances0 = testInstances0.generate("weka/core/Capabilities.props");
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertNotNull(instances0);
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      assertFalse(capabilities0.hasDependencies());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(4, testInstances0.getNumClasses());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getNumRelationalString());
      assertEquals(5, testInstances0.getNumAttributes());
      assertEquals(1, testInstances0.getNumString());
      assertFalse(testInstances0.getNoClass());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertTrue(instances0.checkForStringAttributes());
      assertEquals(4, instances0.numClasses());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(4, instances0.classIndex());
      assertEquals(20, instances0.size());
      assertEquals(20, instances0.numInstances());
      assertEquals(5, instances0.numAttributes());
      
      naiveBayesMultinomialText0.buildClassifier(instances0);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      assertFalse(capabilities0.hasDependencies());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(4, testInstances0.getNumClasses());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getNumRelationalString());
      assertEquals(5, testInstances0.getNumAttributes());
      assertEquals(1, testInstances0.getNumString());
      assertFalse(testInstances0.getNoClass());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertTrue(instances0.checkForStringAttributes());
      assertEquals(4, instances0.numClasses());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(4, instances0.classIndex());
      assertEquals(20, instances0.size());
      assertEquals(20, instances0.numInstances());
      assertEquals(5, instances0.numAttributes());
      
      Attribute attribute0 = new Attribute("weka/core/Capabilities.props", "", 3);
      assertEquals(4, Attribute.RELATIONAL);
      assertEquals(3, Attribute.DATE);
      assertEquals(1, Attribute.ORDERING_ORDERED);
      assertEquals(0, Attribute.ORDERING_SYMBOLIC);
      assertEquals(0, Attribute.NUMERIC);
      assertEquals(2, Attribute.STRING);
      assertEquals(2, Attribute.ORDERING_MODULO);
      assertEquals(1, Attribute.NOMINAL);
      assertNotNull(attribute0);
      assertFalse(attribute0.isString());
      assertEquals(1, attribute0.ordering());
      assertEquals("weka/core/Capabilities.props", attribute0.name());
      assertEquals(3, attribute0.index());
      assertEquals(3, attribute0.type());
      assertFalse(attribute0.lowerNumericBoundIsOpen());
      assertTrue(attribute0.isDate());
      assertFalse(attribute0.hasZeropoint());
      assertFalse(attribute0.upperNumericBoundIsOpen());
      assertTrue(attribute0.isRegular());
      assertEquals(0.0, attribute0.getUpperNumericBound(), 0.01);
      assertEquals(0.0, attribute0.getLowerNumericBound(), 0.01);
      assertFalse(attribute0.isNominal());
      assertFalse(attribute0.isAveragable());
      assertEquals("", attribute0.getDateFormat());
      assertEquals(0, attribute0.numValues());
      assertTrue(attribute0.isNumeric());
      assertEquals(1.0, attribute0.weight(), 0.01);
      assertFalse(attribute0.isRelationValued());
      
      naiveBayesMultinomialText0.setOptions(testInstances0.DEFAULT_WORDS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      assertFalse(capabilities0.hasDependencies());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(4, testInstances0.getNumClasses());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getNumRelationalString());
      assertEquals(5, testInstances0.getNumAttributes());
      assertEquals(1, testInstances0.getNumString());
      assertFalse(testInstances0.getNoClass());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      
      System.setCurrentTimeMillis((-1));
      double[] doubleArray0 = new double[8];
      doubleArray0[0] = (double) 3;
      doubleArray0[1] = (double) 2;
      doubleArray0[2] = (double) (-1);
      doubleArray0[3] = (double) 2;
      doubleArray0[4] = 4665.215351101808;
      doubleArray0[5] = (double) 1;
      doubleArray0[6] = (double) 0;
      doubleArray0[7] = (double) 0;
      int[] intArray0 = new int[8];
      intArray0[0] = 1;
      intArray0[1] = 4;
      intArray0[2] = 1;
      intArray0[3] = 1;
      intArray0[4] = 2317;
      intArray0[5] = 2;
      intArray0[6] = (-1);
      intArray0[7] = 3;
      SparseInstance sparseInstance0 = new SparseInstance(2, doubleArray0, intArray0, 0);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertNotNull(sparseInstance0);
      assertArrayEquals(new int[] {1, 4, 1, 1, 2317, 2, (-1), 3}, intArray0);
      assertArrayEquals(new double[] {3.0, 2.0, (-1.0), 2.0, 4665.215351101808, 1.0, 0.0, 0.0}, doubleArray0, 0.01);
      assertEquals(2.0, sparseInstance0.weight(), 0.01);
      assertEquals(6, sparseInstance0.numValues());
      assertEquals(0, sparseInstance0.numAttributes());
      assertEquals(8, intArray0.length);
      assertEquals(8, doubleArray0.length);
      
      try { 
        naiveBayesMultinomialText0.distributionForInstance(sparseInstance0);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("weka.classifiers.bayes.NaiveBayesMultinomialText", e);
      }
  }

  /**
  //Test case number: 22
  /*Coverage entropy=2.6712364135622684
  */
  @Test(timeout = 4000)
  public void test22()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      assertNotNull(naiveBayesMultinomialText0);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      
      MultiClassClassifierUpdateable multiClassClassifierUpdateable0 = new MultiClassClassifierUpdateable();
      assertEquals(2, MultiClassClassifier.METHOD_ERROR_EXHAUSTIVE);
      assertEquals(1, MultiClassClassifier.METHOD_ERROR_RANDOM);
      assertEquals(3, MultiClassClassifier.METHOD_1_AGAINST_1);
      assertEquals(0, MultiClassClassifier.METHOD_1_AGAINST_ALL);
      assertNotNull(multiClassClassifierUpdateable0);
      assertEquals("Use pairwise coupling (only has an effect for 1-against-1).", multiClassClassifierUpdateable0.usePairwiseCouplingTipText());
      assertFalse(multiClassClassifierUpdateable0.getUsePairwiseCoupling());
      assertEquals("A metaclassifier for handling multi-class datasets with 2-class classifiers. This classifier is also capable of applying error correcting output codes for increased accuracy. The base classifier must be an updateable classifier", multiClassClassifierUpdateable0.globalInfo());
      assertEquals("Sets the width multiplier when using random codes. The number of codes generated will be thus number multiplied by the number of classes.", multiClassClassifierUpdateable0.randomWidthFactorTipText());
      assertFalse(multiClassClassifierUpdateable0.getDebug());
      assertEquals("The base classifier to be used.", multiClassClassifierUpdateable0.classifierTipText());
      assertEquals(1, multiClassClassifierUpdateable0.getSeed());
      assertEquals("If set to true, classifier may output additional info to the console.", multiClassClassifierUpdateable0.debugTipText());
      assertEquals("Sets the method to use for transforming the multi-class problem into several 2-class ones.", multiClassClassifierUpdateable0.methodTipText());
      assertEquals("The random number seed to be used.", multiClassClassifierUpdateable0.seedTipText());
      assertEquals(2.0, multiClassClassifierUpdateable0.getRandomWidthFactor(), 0.01);
      
      Capabilities capabilities0 = naiveBayesMultinomialText0.getCapabilities();
      assertNotNull(capabilities0);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      assertFalse(capabilities0.hasDependencies());
      
      boolean boolean0 = FileSystemHandling.appendLineToFile((EvoSuiteFile) null, "weka.classifiers.bayes.NaiveBayesMultinomialText");
      assertFalse(boolean0);
      
      BinarySparseInstance binarySparseInstance0 = new BinarySparseInstance(2408);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertNotNull(binarySparseInstance0);
      assertEquals(2408, binarySparseInstance0.numValues());
      assertEquals(1.0, binarySparseInstance0.weight(), 0.01);
      assertEquals(2408, binarySparseInstance0.numAttributes());
      
      NaiveBayesMultinomialText naiveBayesMultinomialText1 = new NaiveBayesMultinomialText();
      assertNotNull(naiveBayesMultinomialText1);
      assertFalse(naiveBayesMultinomialText1.equals((Object)naiveBayesMultinomialText0));
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText1.tokenizerTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText1.normalizeDocLengthTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText1.lowercaseTokensTipText());
      assertEquals(3.0, naiveBayesMultinomialText1.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText1.LNormTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText1.debugTipText());
      assertEquals(0, naiveBayesMultinomialText1.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText1.useWordFrequenciesTipText());
      assertEquals(2.0, naiveBayesMultinomialText1.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText1.getUseStopList());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText1.globalInfo());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText1.stopwordsTipText());
      assertFalse(naiveBayesMultinomialText1.getUseWordFrequencies());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText1.normTipText());
      assertFalse(naiveBayesMultinomialText1.getLowercaseTokens());
      assertEquals(1.0, naiveBayesMultinomialText1.getNorm(), 0.01);
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText1.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText1.useStopListTipText());
      assertFalse(naiveBayesMultinomialText1.getDebug());
      assertFalse(naiveBayesMultinomialText1.getNormalizeDocLength());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText1.minWordFrequencyTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText1.periodicPruningTipText());
      
      NaiveBayesMultinomialText naiveBayesMultinomialText2 = new NaiveBayesMultinomialText();
      assertNotNull(naiveBayesMultinomialText2);
      assertFalse(naiveBayesMultinomialText2.equals((Object)naiveBayesMultinomialText1));
      assertFalse(naiveBayesMultinomialText2.equals((Object)naiveBayesMultinomialText0));
      assertFalse(naiveBayesMultinomialText2.getNormalizeDocLength());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText2.useStopListTipText());
      assertEquals(3.0, naiveBayesMultinomialText2.getMinWordFrequency(), 0.01);
      assertEquals(2.0, naiveBayesMultinomialText2.getLNorm(), 0.01);
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText2.lowercaseTokensTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText2.minWordFrequencyTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText2.periodicPruningTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText2.tokenizerTipText());
      assertEquals(1.0, naiveBayesMultinomialText2.getNorm(), 0.01);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText2.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText2.getDebug());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText2.globalInfo());
      assertFalse(naiveBayesMultinomialText2.getUseWordFrequencies());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText2.normTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText2.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText2.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText2.stopwordsTipText());
      assertFalse(naiveBayesMultinomialText2.getUseStopList());
      assertFalse(naiveBayesMultinomialText2.getLowercaseTokens());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText2.LNormTipText());
      assertEquals(0, naiveBayesMultinomialText2.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText2.useWordFrequenciesTipText());
      
      naiveBayesMultinomialText2.setUseStopList(false);
      assertNotSame(naiveBayesMultinomialText2, naiveBayesMultinomialText1);
      assertNotSame(naiveBayesMultinomialText2, naiveBayesMultinomialText0);
      assertFalse(naiveBayesMultinomialText2.equals((Object)naiveBayesMultinomialText1));
      assertFalse(naiveBayesMultinomialText2.equals((Object)naiveBayesMultinomialText0));
      assertFalse(naiveBayesMultinomialText2.getNormalizeDocLength());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText2.useStopListTipText());
      assertEquals(3.0, naiveBayesMultinomialText2.getMinWordFrequency(), 0.01);
      assertEquals(2.0, naiveBayesMultinomialText2.getLNorm(), 0.01);
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText2.lowercaseTokensTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText2.minWordFrequencyTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText2.periodicPruningTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText2.tokenizerTipText());
      assertEquals(1.0, naiveBayesMultinomialText2.getNorm(), 0.01);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText2.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText2.getDebug());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText2.globalInfo());
      assertFalse(naiveBayesMultinomialText2.getUseWordFrequencies());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText2.normTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText2.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText2.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText2.stopwordsTipText());
      assertFalse(naiveBayesMultinomialText2.getUseStopList());
      assertFalse(naiveBayesMultinomialText2.getLowercaseTokens());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText2.LNormTipText());
      assertEquals(0, naiveBayesMultinomialText2.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText2.useWordFrequenciesTipText());
      
      naiveBayesMultinomialText2.pruneDictionary();
      assertNotSame(naiveBayesMultinomialText2, naiveBayesMultinomialText1);
      assertNotSame(naiveBayesMultinomialText2, naiveBayesMultinomialText0);
      assertFalse(naiveBayesMultinomialText2.equals((Object)naiveBayesMultinomialText1));
      assertFalse(naiveBayesMultinomialText2.equals((Object)naiveBayesMultinomialText0));
      assertFalse(naiveBayesMultinomialText2.getNormalizeDocLength());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText2.useStopListTipText());
      assertEquals(3.0, naiveBayesMultinomialText2.getMinWordFrequency(), 0.01);
      assertEquals(2.0, naiveBayesMultinomialText2.getLNorm(), 0.01);
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText2.lowercaseTokensTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText2.minWordFrequencyTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText2.periodicPruningTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText2.tokenizerTipText());
      assertEquals(1.0, naiveBayesMultinomialText2.getNorm(), 0.01);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText2.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText2.getDebug());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText2.globalInfo());
      assertFalse(naiveBayesMultinomialText2.getUseWordFrequencies());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText2.normTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText2.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText2.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText2.stopwordsTipText());
      assertFalse(naiveBayesMultinomialText2.getUseStopList());
      assertFalse(naiveBayesMultinomialText2.getLowercaseTokens());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText2.LNormTipText());
      assertEquals(0, naiveBayesMultinomialText2.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText2.useWordFrequenciesTipText());
      
      String[] stringArray0 = naiveBayesMultinomialText1.getOptions();
      assertNotSame(naiveBayesMultinomialText1, naiveBayesMultinomialText2);
      assertNotSame(naiveBayesMultinomialText1, naiveBayesMultinomialText0);
      assertNotNull(stringArray0);
      assertFalse(naiveBayesMultinomialText1.equals((Object)naiveBayesMultinomialText2));
      assertFalse(naiveBayesMultinomialText1.equals((Object)naiveBayesMultinomialText0));
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText1.tokenizerTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText1.normalizeDocLengthTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText1.lowercaseTokensTipText());
      assertEquals(3.0, naiveBayesMultinomialText1.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText1.LNormTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText1.debugTipText());
      assertEquals(0, naiveBayesMultinomialText1.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText1.useWordFrequenciesTipText());
      assertEquals(2.0, naiveBayesMultinomialText1.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText1.getUseStopList());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText1.globalInfo());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText1.stopwordsTipText());
      assertFalse(naiveBayesMultinomialText1.getUseWordFrequencies());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText1.normTipText());
      assertFalse(naiveBayesMultinomialText1.getLowercaseTokens());
      assertEquals(1.0, naiveBayesMultinomialText1.getNorm(), 0.01);
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText1.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText1.useStopListTipText());
      assertFalse(naiveBayesMultinomialText1.getDebug());
      assertFalse(naiveBayesMultinomialText1.getNormalizeDocLength());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText1.minWordFrequencyTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText1.periodicPruningTipText());
      assertEquals(12, stringArray0.length);
      
      double double0 = naiveBayesMultinomialText0.getLNorm();
      assertNotSame(naiveBayesMultinomialText0, naiveBayesMultinomialText1);
      assertNotSame(naiveBayesMultinomialText0, naiveBayesMultinomialText2);
      assertFalse(naiveBayesMultinomialText0.equals((Object)naiveBayesMultinomialText1));
      assertFalse(naiveBayesMultinomialText0.equals((Object)naiveBayesMultinomialText2));
      assertEquals(2.0, double0, 0.01);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      
      TestInstances testInstances0 = new TestInstances();
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertNotNull(testInstances0);
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(20, testInstances0.getNumInstances());
      assertFalse(testInstances0.getNoClass());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals((-1), testInstances0.getClassIndex());
      
      TestInstances testInstances1 = TestInstances.forCapabilities(capabilities0);
      assertNotSame(naiveBayesMultinomialText0, naiveBayesMultinomialText1);
      assertNotSame(naiveBayesMultinomialText0, naiveBayesMultinomialText2);
      assertNotSame(testInstances1, testInstances0);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertNotNull(testInstances1);
      assertFalse(naiveBayesMultinomialText0.equals((Object)naiveBayesMultinomialText1));
      assertFalse(naiveBayesMultinomialText0.equals((Object)naiveBayesMultinomialText2));
      assertFalse(testInstances1.equals((Object)testInstances0));
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      assertFalse(capabilities0.hasDependencies());
      assertEquals(1, testInstances1.getNumRelationalString());
      assertEquals(2, testInstances1.getNumNominalValues());
      assertEquals(1, testInstances1.getNumDate());
      assertEquals(10, testInstances1.getNumInstancesRelational());
      assertEquals(20, testInstances1.getNumInstances());
      assertFalse(testInstances1.getNoClass());
      assertEquals((-1), testInstances1.getClassIndex());
      assertEquals(5, testInstances1.getNumAttributes());
      assertEquals(1, testInstances1.getNumNominal());
      assertEquals("Testdata", testInstances1.getRelation());
      assertEquals(" ", testInstances1.getWordSeparators());
      assertEquals(1, testInstances1.getNumRelationalDate());
      assertEquals(1, testInstances1.getClassType());
      assertEquals(1, testInstances1.getNumNumeric());
      assertEquals(4, testInstances1.getNumClasses());
      assertEquals(0, testInstances1.getNumRelational());
      assertEquals(1, testInstances1.getSeed());
      assertEquals(1, testInstances1.getNumRelationalNominal());
      assertEquals(2, testInstances1.getNumRelationalNominalValues());
      assertEquals(1, testInstances1.getNumString());
      assertFalse(testInstances1.getMultiInstance());
      assertEquals(1, testInstances1.getNumRelationalNumeric());
      
      Instances instances0 = testInstances1.generate(" ");
      assertNotSame(naiveBayesMultinomialText0, naiveBayesMultinomialText1);
      assertNotSame(naiveBayesMultinomialText0, naiveBayesMultinomialText2);
      assertNotSame(testInstances1, testInstances0);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertNotNull(instances0);
      assertFalse(naiveBayesMultinomialText0.equals((Object)naiveBayesMultinomialText1));
      assertFalse(naiveBayesMultinomialText0.equals((Object)naiveBayesMultinomialText2));
      assertFalse(testInstances1.equals((Object)testInstances0));
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      assertFalse(capabilities0.hasDependencies());
      assertEquals(1, testInstances1.getNumRelationalString());
      assertEquals(2, testInstances1.getNumNominalValues());
      assertEquals(1, testInstances1.getNumDate());
      assertEquals(10, testInstances1.getNumInstancesRelational());
      assertEquals(20, testInstances1.getNumInstances());
      assertFalse(testInstances1.getNoClass());
      assertEquals((-1), testInstances1.getClassIndex());
      assertEquals(5, testInstances1.getNumAttributes());
      assertEquals(1, testInstances1.getNumNominal());
      assertEquals("Testdata", testInstances1.getRelation());
      assertEquals(" ", testInstances1.getWordSeparators());
      assertEquals(1, testInstances1.getNumRelationalDate());
      assertEquals(1, testInstances1.getClassType());
      assertEquals(1, testInstances1.getNumNumeric());
      assertEquals(4, testInstances1.getNumClasses());
      assertEquals(0, testInstances1.getNumRelational());
      assertEquals(1, testInstances1.getSeed());
      assertEquals(1, testInstances1.getNumRelationalNominal());
      assertEquals(2, testInstances1.getNumRelationalNominalValues());
      assertEquals(1, testInstances1.getNumString());
      assertFalse(testInstances1.getMultiInstance());
      assertEquals(1, testInstances1.getNumRelationalNumeric());
      assertEquals(4, instances0.classIndex());
      assertEquals(20, instances0.numInstances());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(20, instances0.size());
      assertEquals(4, instances0.numClasses());
      assertEquals(5, instances0.numAttributes());
      assertTrue(instances0.checkForStringAttributes());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      
      naiveBayesMultinomialText0.buildClassifier(instances0);
      assertNotSame(naiveBayesMultinomialText0, naiveBayesMultinomialText1);
      assertNotSame(naiveBayesMultinomialText0, naiveBayesMultinomialText2);
      assertNotSame(testInstances1, testInstances0);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertFalse(naiveBayesMultinomialText0.equals((Object)naiveBayesMultinomialText1));
      assertFalse(naiveBayesMultinomialText0.equals((Object)naiveBayesMultinomialText2));
      assertFalse(testInstances1.equals((Object)testInstances0));
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      assertFalse(capabilities0.hasDependencies());
      assertEquals(1, testInstances1.getNumRelationalString());
      assertEquals(2, testInstances1.getNumNominalValues());
      assertEquals(1, testInstances1.getNumDate());
      assertEquals(10, testInstances1.getNumInstancesRelational());
      assertEquals(20, testInstances1.getNumInstances());
      assertFalse(testInstances1.getNoClass());
      assertEquals((-1), testInstances1.getClassIndex());
      assertEquals(5, testInstances1.getNumAttributes());
      assertEquals(1, testInstances1.getNumNominal());
      assertEquals("Testdata", testInstances1.getRelation());
      assertEquals(" ", testInstances1.getWordSeparators());
      assertEquals(1, testInstances1.getNumRelationalDate());
      assertEquals(1, testInstances1.getClassType());
      assertEquals(1, testInstances1.getNumNumeric());
      assertEquals(4, testInstances1.getNumClasses());
      assertEquals(0, testInstances1.getNumRelational());
      assertEquals(1, testInstances1.getSeed());
      assertEquals(1, testInstances1.getNumRelationalNominal());
      assertEquals(2, testInstances1.getNumRelationalNominalValues());
      assertEquals(1, testInstances1.getNumString());
      assertFalse(testInstances1.getMultiInstance());
      assertEquals(1, testInstances1.getNumRelationalNumeric());
      assertEquals(4, instances0.classIndex());
      assertEquals(20, instances0.numInstances());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(20, instances0.size());
      assertEquals(4, instances0.numClasses());
      assertEquals(5, instances0.numAttributes());
      assertTrue(instances0.checkForStringAttributes());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      
      NaiveBayesMultinomialText naiveBayesMultinomialText3 = new NaiveBayesMultinomialText();
      assertNotNull(naiveBayesMultinomialText3);
      assertFalse(naiveBayesMultinomialText3.equals((Object)naiveBayesMultinomialText2));
      assertFalse(naiveBayesMultinomialText3.equals((Object)naiveBayesMultinomialText1));
      assertFalse(naiveBayesMultinomialText3.equals((Object)naiveBayesMultinomialText0));
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText3.lowercaseTokensTipText());
      assertEquals(3.0, naiveBayesMultinomialText3.getMinWordFrequency(), 0.01);
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText3.stemmerTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText3.normalizeDocLengthTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText3.debugTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText3.LNormTipText());
      assertEquals(2.0, naiveBayesMultinomialText3.getLNorm(), 0.01);
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText3.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText3.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText3.getUseStopList());
      assertFalse(naiveBayesMultinomialText3.getDebug());
      assertFalse(naiveBayesMultinomialText3.getLowercaseTokens());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText3.globalInfo());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText3.useStopListTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText3.stopwordsTipText());
      assertEquals(1.0, naiveBayesMultinomialText3.getNorm(), 0.01);
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText3.normTipText());
      assertFalse(naiveBayesMultinomialText3.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText3.getUseWordFrequencies());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText3.minWordFrequencyTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText3.periodicPruningTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText3.tokenizerTipText());
      
      String[] stringArray1 = naiveBayesMultinomialText3.getOptions();
      assertNotSame(stringArray1, stringArray0);
      assertNotSame(naiveBayesMultinomialText3, naiveBayesMultinomialText2);
      assertNotSame(naiveBayesMultinomialText3, naiveBayesMultinomialText1);
      assertNotSame(naiveBayesMultinomialText3, naiveBayesMultinomialText0);
      assertNotNull(stringArray1);
      assertFalse(stringArray1.equals((Object)stringArray0));
      assertFalse(naiveBayesMultinomialText3.equals((Object)naiveBayesMultinomialText2));
      assertFalse(naiveBayesMultinomialText3.equals((Object)naiveBayesMultinomialText1));
      assertFalse(naiveBayesMultinomialText3.equals((Object)naiveBayesMultinomialText0));
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText3.lowercaseTokensTipText());
      assertEquals(3.0, naiveBayesMultinomialText3.getMinWordFrequency(), 0.01);
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText3.stemmerTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText3.normalizeDocLengthTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText3.debugTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText3.LNormTipText());
      assertEquals(2.0, naiveBayesMultinomialText3.getLNorm(), 0.01);
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText3.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText3.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText3.getUseStopList());
      assertFalse(naiveBayesMultinomialText3.getDebug());
      assertFalse(naiveBayesMultinomialText3.getLowercaseTokens());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText3.globalInfo());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText3.useStopListTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText3.stopwordsTipText());
      assertEquals(1.0, naiveBayesMultinomialText3.getNorm(), 0.01);
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText3.normTipText());
      assertFalse(naiveBayesMultinomialText3.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText3.getUseWordFrequencies());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText3.minWordFrequencyTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText3.periodicPruningTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText3.tokenizerTipText());
      assertEquals(12, stringArray1.length);
      
      Random.setNextRandom(2);
      FileSystemHandling fileSystemHandling0 = new FileSystemHandling();
      assertNotNull(fileSystemHandling0);
      
      String string0 = naiveBayesMultinomialText0.toString();
      assertNotSame(naiveBayesMultinomialText0, naiveBayesMultinomialText1);
      assertNotSame(naiveBayesMultinomialText0, naiveBayesMultinomialText2);
      assertNotSame(naiveBayesMultinomialText0, naiveBayesMultinomialText3);
      assertNotNull(string0);
      assertFalse(naiveBayesMultinomialText0.equals((Object)naiveBayesMultinomialText1));
      assertFalse(naiveBayesMultinomialText0.equals((Object)naiveBayesMultinomialText2));
      assertFalse(naiveBayesMultinomialText0.equals((Object)naiveBayesMultinomialText3));
      assertEquals("The independent probability of a class\n--------------------------------------\nclass1\t9.0\nclass2\t4.0\nclass3\t7.0\nclass4\t4.0\n\nThe probability of a word given the class\n-----------------------------------------\n\tclass1\tclass2\tclass3\tclass4\t\nover\t2.718281828459045\t2.718281828459045\t7.38905609893065\t7.38905609893065\t\nthe\t7.38905609893065\t7.38905609893065\t2.718281828459045\t2.718281828459045\t\nThe\t7.38905609893065\t2.718281828459045\t7.38905609893065\t2.718281828459045\t\nquick\t7.38905609893065\t2.718281828459045\t7.38905609893065\t7.38905609893065\t\nlazy\t7.38905609893065\t2.718281828459045\t7.38905609893065\t2.718281828459045\t\njumps\t20.085536923187668\t7.38905609893065\t2.718281828459045\t2.718281828459045\t\nbrown\t7.38905609893065\t7.38905609893065\t2.718281828459045\t2.718281828459045\t\ndog\t7.38905609893065\t2.718281828459045\t7.38905609893065\t2.718281828459045\t\nfox\t2.718281828459045\t2.718281828459045\t7.38905609893065\t7.38905609893065\t\n", string0);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      
      String[] stringArray2 = naiveBayesMultinomialText1.getOptions();
      assertNotSame(stringArray2, stringArray1);
      assertNotSame(stringArray2, stringArray0);
      assertNotSame(naiveBayesMultinomialText1, naiveBayesMultinomialText2);
      assertNotSame(naiveBayesMultinomialText1, naiveBayesMultinomialText3);
      assertNotSame(naiveBayesMultinomialText1, naiveBayesMultinomialText0);
      assertNotNull(stringArray2);
      assertFalse(stringArray2.equals((Object)stringArray1));
      assertFalse(stringArray2.equals((Object)stringArray0));
      assertFalse(naiveBayesMultinomialText1.equals((Object)naiveBayesMultinomialText2));
      assertFalse(naiveBayesMultinomialText1.equals((Object)naiveBayesMultinomialText3));
      assertFalse(naiveBayesMultinomialText1.equals((Object)naiveBayesMultinomialText0));
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText1.tokenizerTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText1.normalizeDocLengthTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText1.lowercaseTokensTipText());
      assertEquals(3.0, naiveBayesMultinomialText1.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText1.LNormTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText1.debugTipText());
      assertEquals(0, naiveBayesMultinomialText1.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText1.useWordFrequenciesTipText());
      assertEquals(2.0, naiveBayesMultinomialText1.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText1.getUseStopList());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText1.globalInfo());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText1.stopwordsTipText());
      assertFalse(naiveBayesMultinomialText1.getUseWordFrequencies());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText1.normTipText());
      assertFalse(naiveBayesMultinomialText1.getLowercaseTokens());
      assertEquals(1.0, naiveBayesMultinomialText1.getNorm(), 0.01);
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText1.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText1.useStopListTipText());
      assertFalse(naiveBayesMultinomialText1.getDebug());
      assertFalse(naiveBayesMultinomialText1.getNormalizeDocLength());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText1.minWordFrequencyTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText1.periodicPruningTipText());
      assertEquals(12, stringArray2.length);
      
      System.setCurrentTimeMillis(0L);
      NaiveBayesMultinomialText naiveBayesMultinomialText4 = new NaiveBayesMultinomialText();
      assertNotNull(naiveBayesMultinomialText4);
      assertFalse(naiveBayesMultinomialText4.equals((Object)naiveBayesMultinomialText2));
      assertFalse(naiveBayesMultinomialText4.equals((Object)naiveBayesMultinomialText0));
      assertFalse(naiveBayesMultinomialText4.equals((Object)naiveBayesMultinomialText3));
      assertFalse(naiveBayesMultinomialText4.equals((Object)naiveBayesMultinomialText1));
      assertFalse(naiveBayesMultinomialText4.getUseStopList());
      assertFalse(naiveBayesMultinomialText4.getDebug());
      assertEquals(0, naiveBayesMultinomialText4.getPeriodicPruning());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText4.periodicPruningTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText4.normTipText());
      assertFalse(naiveBayesMultinomialText4.getNormalizeDocLength());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText4.globalInfo());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText4.stopwordsTipText());
      assertEquals(2.0, naiveBayesMultinomialText4.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText4.getUseWordFrequencies());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText4.debugTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText4.normalizeDocLengthTipText());
      assertEquals(1.0, naiveBayesMultinomialText4.getNorm(), 0.01);
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText4.lowercaseTokensTipText());
      assertEquals(3.0, naiveBayesMultinomialText4.getMinWordFrequency(), 0.01);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText4.tokenizerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText4.minWordFrequencyTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText4.useStopListTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText4.useWordFrequenciesTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText4.stemmerTipText());
      assertFalse(naiveBayesMultinomialText4.getLowercaseTokens());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText4.LNormTipText());
      
      naiveBayesMultinomialText4.setOptions(testInstances0.DEFAULT_WORDS);
      assertNotSame(naiveBayesMultinomialText4, naiveBayesMultinomialText2);
      assertNotSame(naiveBayesMultinomialText4, naiveBayesMultinomialText0);
      assertNotSame(naiveBayesMultinomialText4, naiveBayesMultinomialText3);
      assertNotSame(naiveBayesMultinomialText4, naiveBayesMultinomialText1);
      assertNotSame(testInstances0, testInstances1);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertFalse(naiveBayesMultinomialText4.equals((Object)naiveBayesMultinomialText2));
      assertFalse(naiveBayesMultinomialText4.equals((Object)naiveBayesMultinomialText0));
      assertFalse(naiveBayesMultinomialText4.equals((Object)naiveBayesMultinomialText3));
      assertFalse(naiveBayesMultinomialText4.equals((Object)naiveBayesMultinomialText1));
      assertFalse(testInstances0.equals((Object)testInstances1));
      assertFalse(naiveBayesMultinomialText4.getUseStopList());
      assertFalse(naiveBayesMultinomialText4.getDebug());
      assertEquals(0, naiveBayesMultinomialText4.getPeriodicPruning());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText4.periodicPruningTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText4.normTipText());
      assertFalse(naiveBayesMultinomialText4.getNormalizeDocLength());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText4.globalInfo());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText4.stopwordsTipText());
      assertEquals(2.0, naiveBayesMultinomialText4.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText4.getUseWordFrequencies());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText4.debugTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText4.normalizeDocLengthTipText());
      assertEquals(1.0, naiveBayesMultinomialText4.getNorm(), 0.01);
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText4.lowercaseTokensTipText());
      assertEquals(3.0, naiveBayesMultinomialText4.getMinWordFrequency(), 0.01);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText4.tokenizerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText4.minWordFrequencyTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText4.useStopListTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText4.useWordFrequenciesTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText4.stemmerTipText());
      assertFalse(naiveBayesMultinomialText4.getLowercaseTokens());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText4.LNormTipText());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(20, testInstances0.getNumInstances());
      assertFalse(testInstances0.getNoClass());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals((-1), testInstances0.getClassIndex());
      
      String string1 = naiveBayesMultinomialText2.toString();
      assertNotSame(naiveBayesMultinomialText2, naiveBayesMultinomialText1);
      assertNotSame(naiveBayesMultinomialText2, naiveBayesMultinomialText0);
      assertNotSame(naiveBayesMultinomialText2, naiveBayesMultinomialText4);
      assertNotSame(naiveBayesMultinomialText2, naiveBayesMultinomialText3);
      assertNotNull(string1);
      assertFalse(naiveBayesMultinomialText2.equals((Object)naiveBayesMultinomialText1));
      assertFalse(naiveBayesMultinomialText2.equals((Object)naiveBayesMultinomialText0));
      assertFalse(naiveBayesMultinomialText2.equals((Object)naiveBayesMultinomialText4));
      assertFalse(naiveBayesMultinomialText2.equals((Object)naiveBayesMultinomialText3));
      assertFalse(string1.equals((Object)string0));
      assertEquals("NaiveBayesMultinomialText: No model built yet.\n", string1);
      assertFalse(naiveBayesMultinomialText2.getNormalizeDocLength());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText2.useStopListTipText());
      assertEquals(3.0, naiveBayesMultinomialText2.getMinWordFrequency(), 0.01);
      assertEquals(2.0, naiveBayesMultinomialText2.getLNorm(), 0.01);
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText2.lowercaseTokensTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText2.minWordFrequencyTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText2.periodicPruningTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText2.tokenizerTipText());
      assertEquals(1.0, naiveBayesMultinomialText2.getNorm(), 0.01);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText2.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText2.getDebug());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText2.globalInfo());
      assertFalse(naiveBayesMultinomialText2.getUseWordFrequencies());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText2.normTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText2.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText2.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText2.stopwordsTipText());
      assertFalse(naiveBayesMultinomialText2.getUseStopList());
      assertFalse(naiveBayesMultinomialText2.getLowercaseTokens());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText2.LNormTipText());
      assertEquals(0, naiveBayesMultinomialText2.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText2.useWordFrequenciesTipText());
  }

  /**
  //Test case number: 23
  /*Coverage entropy=2.3978952727983707
  */
  @Test(timeout = 4000)
  public void test23()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      assertNotNull(naiveBayesMultinomialText0);
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      
      MultiClassClassifierUpdateable multiClassClassifierUpdateable0 = new MultiClassClassifierUpdateable();
      assertEquals(1, MultiClassClassifier.METHOD_ERROR_RANDOM);
      assertEquals(3, MultiClassClassifier.METHOD_1_AGAINST_1);
      assertEquals(0, MultiClassClassifier.METHOD_1_AGAINST_ALL);
      assertEquals(2, MultiClassClassifier.METHOD_ERROR_EXHAUSTIVE);
      assertNotNull(multiClassClassifierUpdateable0);
      assertEquals("Sets the width multiplier when using random codes. The number of codes generated will be thus number multiplied by the number of classes.", multiClassClassifierUpdateable0.randomWidthFactorTipText());
      assertEquals("Use pairwise coupling (only has an effect for 1-against-1).", multiClassClassifierUpdateable0.usePairwiseCouplingTipText());
      assertEquals("The random number seed to be used.", multiClassClassifierUpdateable0.seedTipText());
      assertFalse(multiClassClassifierUpdateable0.getDebug());
      assertEquals("Sets the method to use for transforming the multi-class problem into several 2-class ones.", multiClassClassifierUpdateable0.methodTipText());
      assertEquals(1, multiClassClassifierUpdateable0.getSeed());
      assertEquals("If set to true, classifier may output additional info to the console.", multiClassClassifierUpdateable0.debugTipText());
      assertEquals("A metaclassifier for handling multi-class datasets with 2-class classifiers. This classifier is also capable of applying error correcting output codes for increased accuracy. The base classifier must be an updateable classifier", multiClassClassifierUpdateable0.globalInfo());
      assertEquals(2.0, multiClassClassifierUpdateable0.getRandomWidthFactor(), 0.01);
      assertEquals("The base classifier to be used.", multiClassClassifierUpdateable0.classifierTipText());
      assertFalse(multiClassClassifierUpdateable0.getUsePairwiseCoupling());
      
      Capabilities capabilities0 = naiveBayesMultinomialText0.getCapabilities();
      assertNotNull(capabilities0);
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertFalse(capabilities0.hasDependencies());
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      
      TestInstances testInstances0 = TestInstances.forCapabilities(capabilities0);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertNotNull(testInstances0);
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertFalse(capabilities0.hasDependencies());
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      assertEquals(1, testInstances0.getNumString());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(5, testInstances0.getNumAttributes());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(4, testInstances0.getNumClasses());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertFalse(testInstances0.getNoClass());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(1, testInstances0.getNumRelationalString());
      
      naiveBayesMultinomialText0.setNormalizeDocLength(true);
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertTrue(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      
      Instances instances0 = testInstances0.generate("weka/core/Capabilities.props");
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertNotNull(instances0);
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertTrue(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertFalse(capabilities0.hasDependencies());
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      assertEquals(1, testInstances0.getNumString());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(5, testInstances0.getNumAttributes());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(4, testInstances0.getNumClasses());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertFalse(testInstances0.getNoClass());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(1, testInstances0.getNumRelationalString());
      assertEquals(4, instances0.numClasses());
      assertEquals(20, instances0.numInstances());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(20, instances0.size());
      assertEquals(5, instances0.numAttributes());
      assertTrue(instances0.checkForStringAttributes());
      assertEquals(4, instances0.classIndex());
      assertEquals("Testdata", instances0.relationName());
      
      naiveBayesMultinomialText0.buildClassifier(instances0);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertTrue(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertFalse(capabilities0.hasDependencies());
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      assertEquals(1, testInstances0.getNumString());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(5, testInstances0.getNumAttributes());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(4, testInstances0.getNumClasses());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertFalse(testInstances0.getMultiInstance());
      assertFalse(testInstances0.getNoClass());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(1, testInstances0.getNumRelationalString());
      assertEquals(4, instances0.numClasses());
      assertEquals(20, instances0.numInstances());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(20, instances0.size());
      assertEquals(5, instances0.numAttributes());
      assertTrue(instances0.checkForStringAttributes());
      assertEquals(4, instances0.classIndex());
      assertEquals("Testdata", instances0.relationName());
      
      System.setCurrentTimeMillis(0L);
      double[] doubleArray0 = new double[1];
      doubleArray0[0] = (double) 0;
      int[] intArray0 = new int[5];
      intArray0[0] = (-2);
      intArray0[1] = (-2);
      intArray0[2] = (-2654);
      intArray0[3] = (-2);
      intArray0[4] = 0;
      SparseInstance sparseInstance0 = new SparseInstance(6.283185307179586, doubleArray0, intArray0, (-424));
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertNotNull(sparseInstance0);
      assertArrayEquals(new double[] {0.0}, doubleArray0, 0.01);
      assertArrayEquals(new int[] {(-2), (-2), (-2654), (-2), 0}, intArray0);
      assertEquals(0, sparseInstance0.numValues());
      assertEquals(6.283185307179586, sparseInstance0.weight(), 0.01);
      assertEquals((-424), sparseInstance0.numAttributes());
      assertEquals(1, doubleArray0.length);
      assertEquals(5, intArray0.length);
      
      double[] doubleArray1 = naiveBayesMultinomialText0.distributionForInstance(sparseInstance0);
      assertNotSame(doubleArray1, doubleArray0);
      assertNotSame(doubleArray0, doubleArray1);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertNotNull(doubleArray1);
      assertArrayEquals(new double[] {0.3750000000000001, 0.16666666666666669, 0.29166666666666663, 0.16666666666666669}, doubleArray1, 0.01);
      assertArrayEquals(new double[] {0.0}, doubleArray0, 0.01);
      assertArrayEquals(new int[] {(-2), (-2), (-2654), (-2), 0}, intArray0);
      assertFalse(doubleArray1.equals((Object)doubleArray0));
      assertEquals(0, sparseInstance0.numValues());
      assertEquals(6.283185307179586, sparseInstance0.weight(), 0.01);
      assertEquals((-424), sparseInstance0.numAttributes());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertTrue(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals(4, doubleArray1.length);
      assertEquals(1, doubleArray0.length);
      assertEquals(5, intArray0.length);
  }

  /**
  //Test case number: 24
  /*Coverage entropy=0.9502705392332347
  */
  @Test(timeout = 4000)
  public void test24()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      assertNotNull(naiveBayesMultinomialText0);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      
      LinkedHashMap<Integer, LinkedHashMap<String, NaiveBayesMultinomialText.Count>> linkedHashMap0 = new LinkedHashMap<Integer, LinkedHashMap<String, NaiveBayesMultinomialText.Count>>();
      assertNotNull(linkedHashMap0);
      assertEquals(0, linkedHashMap0.size());
      assertTrue(linkedHashMap0.isEmpty());
      
      naiveBayesMultinomialText0.m_probOfWordGivenClass = (Map<Integer, LinkedHashMap<String, NaiveBayesMultinomialText.Count>>) linkedHashMap0;
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals(0, linkedHashMap0.size());
      assertTrue(linkedHashMap0.isEmpty());
      assertTrue(naiveBayesMultinomialText0.m_probOfWordGivenClass.isEmpty());
      assertEquals(0, naiveBayesMultinomialText0.m_probOfWordGivenClass.size());
      
      naiveBayesMultinomialText0.m_periodicP = 1849;
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(1849, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      
      naiveBayesMultinomialText0.pruneDictionary();
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(1849, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      
      int[] intArray0 = new int[8];
      intArray0[0] = (-759);
      Capabilities capabilities0 = naiveBayesMultinomialText0.getCapabilities();
      assertNotNull(capabilities0);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(1849, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(capabilities0.hasDependencies());
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      
      intArray0[1] = 1849;
      intArray0[2] = 1849;
      intArray0[3] = 1849;
      intArray0[4] = 1849;
      intArray0[5] = 499;
      intArray0[6] = 1849;
      intArray0[7] = 1849;
      SparseInstance sparseInstance0 = null;
      try {
        sparseInstance0 = new SparseInstance((-1935.052252205), (double[]) null, intArray0, 1849);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("weka.core.SparseInstance", e);
      }
  }

  /**
  //Test case number: 25
  /*Coverage entropy=2.1006789212792607
  */
  @Test(timeout = 4000)
  public void test25()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      assertNotNull(naiveBayesMultinomialText0);
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      
      MultiClassClassifierUpdateable multiClassClassifierUpdateable0 = new MultiClassClassifierUpdateable();
      assertEquals(3, MultiClassClassifier.METHOD_1_AGAINST_1);
      assertEquals(1, MultiClassClassifier.METHOD_ERROR_RANDOM);
      assertEquals(2, MultiClassClassifier.METHOD_ERROR_EXHAUSTIVE);
      assertEquals(0, MultiClassClassifier.METHOD_1_AGAINST_ALL);
      assertNotNull(multiClassClassifierUpdateable0);
      assertEquals("Sets the method to use for transforming the multi-class problem into several 2-class ones.", multiClassClassifierUpdateable0.methodTipText());
      assertEquals("Sets the width multiplier when using random codes. The number of codes generated will be thus number multiplied by the number of classes.", multiClassClassifierUpdateable0.randomWidthFactorTipText());
      assertEquals("The base classifier to be used.", multiClassClassifierUpdateable0.classifierTipText());
      assertEquals("A metaclassifier for handling multi-class datasets with 2-class classifiers. This classifier is also capable of applying error correcting output codes for increased accuracy. The base classifier must be an updateable classifier", multiClassClassifierUpdateable0.globalInfo());
      assertFalse(multiClassClassifierUpdateable0.getDebug());
      assertFalse(multiClassClassifierUpdateable0.getUsePairwiseCoupling());
      assertEquals("Use pairwise coupling (only has an effect for 1-against-1).", multiClassClassifierUpdateable0.usePairwiseCouplingTipText());
      assertEquals(1, multiClassClassifierUpdateable0.getSeed());
      assertEquals("The random number seed to be used.", multiClassClassifierUpdateable0.seedTipText());
      assertEquals(2.0, multiClassClassifierUpdateable0.getRandomWidthFactor(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", multiClassClassifierUpdateable0.debugTipText());
      
      Capabilities capabilities0 = naiveBayesMultinomialText0.getCapabilities();
      assertNotNull(capabilities0);
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      assertFalse(capabilities0.hasDependencies());
      
      TestInstances testInstances0 = TestInstances.forCapabilities(capabilities0);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertNotNull(testInstances0);
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      assertFalse(capabilities0.hasDependencies());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(1, testInstances0.getSeed());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(1, testInstances0.getNumString());
      assertEquals(4, testInstances0.getNumClasses());
      assertFalse(testInstances0.getNoClass());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(5, testInstances0.getNumAttributes());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(2, testInstances0.getNumNominalValues());
      
      naiveBayesMultinomialText0.m_lnorm = 0.0;
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals(0.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      
      naiveBayesMultinomialText0.setNormalizeDocLength(true);
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals(0.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertTrue(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      
      Instances instances0 = testInstances0.generate("weka/core/Capabilities.props");
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertNotNull(instances0);
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals(0.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertTrue(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      assertFalse(capabilities0.hasDependencies());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(1, testInstances0.getSeed());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(1, testInstances0.getNumString());
      assertEquals(4, testInstances0.getNumClasses());
      assertFalse(testInstances0.getNoClass());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(5, testInstances0.getNumAttributes());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(20, instances0.numInstances());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(5, instances0.numAttributes());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(20, instances0.size());
      assertEquals(4, instances0.numClasses());
      assertEquals(4, instances0.classIndex());
      assertTrue(instances0.checkForStringAttributes());
      
      naiveBayesMultinomialText0.buildClassifier(instances0);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals(0.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertTrue(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      assertFalse(capabilities0.hasDependencies());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(1, testInstances0.getSeed());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(1, testInstances0.getNumString());
      assertEquals(4, testInstances0.getNumClasses());
      assertFalse(testInstances0.getNoClass());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(5, testInstances0.getNumAttributes());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(1, testInstances0.getNumDate());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(20, instances0.numInstances());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(5, instances0.numAttributes());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(20, instances0.size());
      assertEquals(4, instances0.numClasses());
      assertEquals(4, instances0.classIndex());
      assertTrue(instances0.checkForStringAttributes());
      
      System.setCurrentTimeMillis(0L);
      double[] doubleArray0 = new double[1];
      doubleArray0[0] = (double) 0;
      int[] intArray0 = new int[5];
      intArray0[0] = (-2);
      intArray0[1] = (-2);
      intArray0[2] = (-2654);
      intArray0[3] = (-2);
      intArray0[4] = 0;
      SparseInstance sparseInstance0 = new SparseInstance(6.283185307179586, doubleArray0, intArray0, (-424));
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertNotNull(sparseInstance0);
      assertArrayEquals(new double[] {0.0}, doubleArray0, 0.01);
      assertArrayEquals(new int[] {(-2), (-2), (-2654), (-2), 0}, intArray0);
      assertEquals(6.283185307179586, sparseInstance0.weight(), 0.01);
      assertEquals((-424), sparseInstance0.numAttributes());
      assertEquals(0, sparseInstance0.numValues());
      assertEquals(1, doubleArray0.length);
      assertEquals(5, intArray0.length);
      
      double[] doubleArray1 = naiveBayesMultinomialText0.distributionForInstance(sparseInstance0);
      assertNotSame(doubleArray1, doubleArray0);
      assertNotSame(doubleArray0, doubleArray1);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertNotNull(doubleArray1);
      assertArrayEquals(new double[] {0.3750000000000001, 0.16666666666666669, 0.29166666666666663, 0.16666666666666669}, doubleArray1, 0.01);
      assertArrayEquals(new double[] {0.0}, doubleArray0, 0.01);
      assertArrayEquals(new int[] {(-2), (-2), (-2654), (-2), 0}, intArray0);
      assertFalse(doubleArray1.equals((Object)doubleArray0));
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals(0.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertTrue(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals(6.283185307179586, sparseInstance0.weight(), 0.01);
      assertEquals((-424), sparseInstance0.numAttributes());
      assertEquals(0, sparseInstance0.numValues());
      assertEquals(4, doubleArray1.length);
      assertEquals(1, doubleArray0.length);
      assertEquals(5, intArray0.length);
  }

  /**
  //Test case number: 26
  /*Coverage entropy=1.9483122222153415
  */
  @Test(timeout = 4000)
  public void test26()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      assertNotNull(naiveBayesMultinomialText0);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      
      MultiClassClassifierUpdateable multiClassClassifierUpdateable0 = new MultiClassClassifierUpdateable();
      assertEquals(2, MultiClassClassifier.METHOD_ERROR_EXHAUSTIVE);
      assertEquals(0, MultiClassClassifier.METHOD_1_AGAINST_ALL);
      assertEquals(1, MultiClassClassifier.METHOD_ERROR_RANDOM);
      assertEquals(3, MultiClassClassifier.METHOD_1_AGAINST_1);
      assertNotNull(multiClassClassifierUpdateable0);
      assertEquals(1, multiClassClassifierUpdateable0.getSeed());
      assertEquals("If set to true, classifier may output additional info to the console.", multiClassClassifierUpdateable0.debugTipText());
      assertEquals("The random number seed to be used.", multiClassClassifierUpdateable0.seedTipText());
      assertFalse(multiClassClassifierUpdateable0.getUsePairwiseCoupling());
      assertEquals(2.0, multiClassClassifierUpdateable0.getRandomWidthFactor(), 0.01);
      assertEquals("Sets the method to use for transforming the multi-class problem into several 2-class ones.", multiClassClassifierUpdateable0.methodTipText());
      assertEquals("Sets the width multiplier when using random codes. The number of codes generated will be thus number multiplied by the number of classes.", multiClassClassifierUpdateable0.randomWidthFactorTipText());
      assertFalse(multiClassClassifierUpdateable0.getDebug());
      assertEquals("Use pairwise coupling (only has an effect for 1-against-1).", multiClassClassifierUpdateable0.usePairwiseCouplingTipText());
      assertEquals("The base classifier to be used.", multiClassClassifierUpdateable0.classifierTipText());
      assertEquals("A metaclassifier for handling multi-class datasets with 2-class classifiers. This classifier is also capable of applying error correcting output codes for increased accuracy. The base classifier must be an updateable classifier", multiClassClassifierUpdateable0.globalInfo());
      
      Capabilities capabilities0 = naiveBayesMultinomialText0.getCapabilities();
      assertNotNull(capabilities0);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      assertFalse(capabilities0.hasDependencies());
      
      naiveBayesMultinomialText0.setLowercaseTokens(true);
      assertTrue(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      
      TestInstances testInstances0 = TestInstances.forCapabilities(capabilities0);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertNotNull(testInstances0);
      assertTrue(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      assertFalse(capabilities0.hasDependencies());
      assertEquals(5, testInstances0.getNumAttributes());
      assertEquals(1, testInstances0.getNumString());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(4, testInstances0.getNumClasses());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getSeed());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertFalse(testInstances0.getNoClass());
      assertEquals(1, testInstances0.getNumDate());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(1, testInstances0.getNumRelationalString());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      
      naiveBayesMultinomialText0.setNormalizeDocLength(true);
      assertTrue(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertTrue(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      
      Instances instances0 = testInstances0.generate("weka/core/Capabilities.props");
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertNotNull(instances0);
      assertTrue(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertTrue(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      assertFalse(capabilities0.hasDependencies());
      assertEquals(5, testInstances0.getNumAttributes());
      assertEquals(1, testInstances0.getNumString());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(4, testInstances0.getNumClasses());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getSeed());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertFalse(testInstances0.getNoClass());
      assertEquals(1, testInstances0.getNumDate());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(1, testInstances0.getNumRelationalString());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertTrue(instances0.checkForStringAttributes());
      assertEquals(20, instances0.numInstances());
      assertEquals(5, instances0.numAttributes());
      assertEquals(4, instances0.numClasses());
      assertEquals(20, instances0.size());
      assertEquals(4, instances0.classIndex());
      
      naiveBayesMultinomialText0.buildClassifier(instances0);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertTrue(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertTrue(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      assertFalse(capabilities0.hasDependencies());
      assertEquals(5, testInstances0.getNumAttributes());
      assertEquals(1, testInstances0.getNumString());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(4, testInstances0.getNumClasses());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getSeed());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(1, testInstances0.getNumRelationalDate());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertFalse(testInstances0.getNoClass());
      assertEquals(1, testInstances0.getNumDate());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(1, testInstances0.getNumRelationalNumeric());
      assertEquals(1, testInstances0.getNumRelationalString());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getNumNumeric());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertTrue(instances0.checkForStringAttributes());
      assertEquals(20, instances0.numInstances());
      assertEquals(5, instances0.numAttributes());
      assertEquals(4, instances0.numClasses());
      assertEquals(20, instances0.size());
      assertEquals(4, instances0.classIndex());
      
      System.setCurrentTimeMillis(0L);
      double[] doubleArray0 = new double[1];
      doubleArray0[0] = (double) 0;
      int[] intArray0 = new int[5];
      intArray0[0] = (-2);
      intArray0[1] = (-2);
      intArray0[2] = (-2654);
      intArray0[3] = (-2);
      intArray0[4] = 0;
      SparseInstance sparseInstance0 = new SparseInstance(6.283185307179586, doubleArray0, intArray0, (-424));
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertNotNull(sparseInstance0);
      assertArrayEquals(new double[] {0.0}, doubleArray0, 0.01);
      assertArrayEquals(new int[] {(-2), (-2), (-2654), (-2), 0}, intArray0);
      assertEquals(6.283185307179586, sparseInstance0.weight(), 0.01);
      assertEquals((-424), sparseInstance0.numAttributes());
      assertEquals(0, sparseInstance0.numValues());
      assertEquals(1, doubleArray0.length);
      assertEquals(5, intArray0.length);
      
      double[] doubleArray1 = naiveBayesMultinomialText0.distributionForInstance(sparseInstance0);
      assertNotSame(doubleArray1, doubleArray0);
      assertNotSame(doubleArray0, doubleArray1);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertNotNull(doubleArray1);
      assertArrayEquals(new double[] {0.3750000000000001, 0.16666666666666669, 0.29166666666666663, 0.16666666666666669}, doubleArray1, 0.01);
      assertArrayEquals(new double[] {0.0}, doubleArray0, 0.01);
      assertArrayEquals(new int[] {(-2), (-2), (-2654), (-2), 0}, intArray0);
      assertFalse(doubleArray1.equals((Object)doubleArray0));
      assertTrue(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertTrue(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(6.283185307179586, sparseInstance0.weight(), 0.01);
      assertEquals((-424), sparseInstance0.numAttributes());
      assertEquals(0, sparseInstance0.numValues());
      assertEquals(4, doubleArray1.length);
      assertEquals(1, doubleArray0.length);
      assertEquals(5, intArray0.length);
  }
}
