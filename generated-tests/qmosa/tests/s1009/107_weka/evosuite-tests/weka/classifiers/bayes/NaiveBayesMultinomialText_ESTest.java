/*
 * This file was automatically generated by EvoSuite
 * Fri Aug 24 10:44:30 GMT 2018
 */

package weka.classifiers.bayes;

import org.junit.Test;
import static org.junit.Assert.*;
import static org.evosuite.shaded.org.mockito.Mockito.*;
import static org.evosuite.runtime.EvoAssertions.*;
import java.io.File;
import java.util.ArrayList;
import java.util.Enumeration;
import java.util.LinkedHashMap;
import java.util.Map;
import java.util.function.Consumer;
import org.evosuite.runtime.EvoRunner;
import org.evosuite.runtime.EvoRunnerParameters;
import org.evosuite.runtime.System;
import org.evosuite.runtime.ViolatedAssumptionAnswer;
import org.evosuite.runtime.mock.java.io.MockFile;
import org.evosuite.runtime.testdata.FileSystemHandling;
import org.junit.runner.RunWith;
import weka.attributeSelection.PrincipalComponents;
import weka.classifiers.AbstractClassifier;
import weka.classifiers.Classifier;
import weka.classifiers.bayes.NaiveBayesMultinomial;
import weka.classifiers.bayes.NaiveBayesMultinomialText;
import weka.classifiers.functions.SGDText;
import weka.classifiers.functions.VotedPerceptron;
import weka.classifiers.functions.supportVector.PrecomputedKernelMatrixKernel;
import weka.classifiers.lazy.IBk;
import weka.classifiers.misc.InputMappedClassifier;
import weka.core.AbstractInstance;
import weka.core.Attribute;
import weka.core.BinarySparseInstance;
import weka.core.Capabilities;
import weka.core.DenseInstance;
import weka.core.Instance;
import weka.core.Instances;
import weka.core.Option;
import weka.core.SparseInstance;
import weka.core.TestInstances;
import weka.core.neighboursearch.LinearNNSearch;
import weka.core.stemmers.IteratedLovinsStemmer;
import weka.core.stemmers.NullStemmer;
import weka.core.stemmers.SnowballStemmer;
import weka.core.stemmers.Stemmer;
import weka.core.tokenizers.NGramTokenizer;
import weka.core.tokenizers.WordTokenizer;

@RunWith(EvoRunner.class) @EvoRunnerParameters(mockJVMNonDeterminism = true, useVFS = true, useVNET = true, resetStaticState = true, separateClassLoader = true, useJEE = true) 
public class NaiveBayesMultinomialText_ESTest extends NaiveBayesMultinomialText_ESTest_scaffolding {

  /**
  //Test case number: 0
  /*Coverage entropy=1.0986122886681096
  */
  @Test(timeout = 4000)
  public void test00()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.reset();
      double[] doubleArray0 = new double[6];
      doubleArray0[0] = (-11.0);
      doubleArray0[1] = 0.7;
      doubleArray0[2] = 0.0;
      doubleArray0[3] = 4657.97;
      doubleArray0[4] = 0.0;
      doubleArray0[5] = (-940.608909);
      naiveBayesMultinomialText0.m_wordsPerClass = doubleArray0;
      String string0 = naiveBayesMultinomialText0.LNormTipText();
      assertEquals("The LNorm to use for document length normalization.", string0);
      
      naiveBayesMultinomialText0.reset();
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
  }

  /**
  //Test case number: 1
  /*Coverage entropy=2.995732273553991
  */
  @Test(timeout = 4000)
  public void test01()  throws Throwable  {
      String[] stringArray0 = new String[9];
      stringArray0[0] = "";
      stringArray0[1] = "";
      stringArray0[2] = "SVM not initialized in optimizer. Use RegOptimizer.setSVMReg()";
      stringArray0[3] = "x0T3XZ-qLnInSiL";
      stringArray0[4] = "OsM";
      stringArray0[5] = "%6g{wy5];";
      stringArray0[6] = "/=FB|";
      stringArray0[7] = "MultiScheme: No model built yet.";
      stringArray0[8] = "";
      NaiveBayesMultinomialText.main(stringArray0);
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.getRevision();
      naiveBayesMultinomialText0.minWordFrequencyTipText();
      SGDText sGDText0 = new SGDText();
      Stemmer stemmer0 = sGDText0.getStemmer();
      naiveBayesMultinomialText0.m_stemmer = stemmer0;
      sGDText0.setUseWordFrequencies(false);
      naiveBayesMultinomialText0.LNormTipText();
      naiveBayesMultinomialText0.getNorm();
      naiveBayesMultinomialText0.getPeriodicPruning();
      naiveBayesMultinomialText0.tokenizerTipText();
      naiveBayesMultinomialText0.getOptions();
      int[] intArray0 = new int[2];
      intArray0[0] = 0;
      intArray0[1] = 0;
      BinarySparseInstance binarySparseInstance0 = new BinarySparseInstance(0, intArray0, 1);
      // Undeclared exception!
      try { 
        naiveBayesMultinomialText0.tokenizeInstance(binarySparseInstance0, false);
        fail("Expecting exception: RuntimeException");
      
      } catch(RuntimeException e) {
         //
         // DenseInstance doesn't have access to a dataset!
         //
         verifyException("weka.core.AbstractInstance", e);
      }
  }

  /**
  //Test case number: 2
  /*Coverage entropy=1.3862943611198906
  */
  @Test(timeout = 4000)
  public void test02()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.setNormalizeDocLength(true);
      naiveBayesMultinomialText0.reset();
      naiveBayesMultinomialText0.stemmerTipText();
      naiveBayesMultinomialText0.setNormalizeDocLength(false);
      ArrayList<Attribute> arrayList0 = new ArrayList<Attribute>();
      arrayList0.add((Attribute) null);
      arrayList0.add((Attribute) null);
      Instances instances0 = null;
      try {
        instances0 = new Instances("q\"oip)cV-gZ E%qCZd", arrayList0, 118);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("weka.core.Instances", e);
      }
  }

  /**
  //Test case number: 3
  /*Coverage entropy=2.9444389791664403
  */
  @Test(timeout = 4000)
  public void test03()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.m_wordsPerClass = null;
      double[] doubleArray0 = new double[4];
      naiveBayesMultinomialText0.getOptions();
      doubleArray0[0] = 2470.19;
      naiveBayesMultinomialText0.getRevision();
      naiveBayesMultinomialText0.m_norm = (-1264.0);
      doubleArray0[1] = 1.0E10;
      boolean boolean0 = true;
      naiveBayesMultinomialText0.setUseStopList(true);
      doubleArray0[2] = (-861.0);
      doubleArray0[3] = (-347.85368339678695);
      naiveBayesMultinomialText0.m_probOfClass = doubleArray0;
      naiveBayesMultinomialText0.getCapabilities();
      naiveBayesMultinomialText0.pruneDictionary();
      Instance instance0 = null;
      try { 
        naiveBayesMultinomialText0.distributionForInstance((Instance) null);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("weka.classifiers.bayes.NaiveBayesMultinomialText", e);
      }
  }

  /**
  //Test case number: 4
  /*Coverage entropy=1.7917594692280547
  */
  @Test(timeout = 4000)
  public void test04()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      String[] stringArray0 = new String[2];
      naiveBayesMultinomialText0.m_lnorm = (-3118.752690056);
      stringArray0[0] = "BiZ{iV7{";
      stringArray0[1] = "";
      NaiveBayesMultinomialText.main(stringArray0);
      LinkedHashMap<String, NaiveBayesMultinomialText.Count> linkedHashMap0 = new LinkedHashMap<String, NaiveBayesMultinomialText.Count>(0, 830.23364F);
      naiveBayesMultinomialText0.m_inputVector = linkedHashMap0;
      naiveBayesMultinomialText0.setPeriodicPruning((-1));
      naiveBayesMultinomialText0.getUseWordFrequencies();
      naiveBayesMultinomialText0.getNorm();
      PrincipalComponents principalComponents0 = new PrincipalComponents();
      try { 
        principalComponents0.transformedHeader();
        fail("Expecting exception: Exception");
      
      } catch(Exception e) {
         //
         // Principal components hasn't been built yet
         //
         verifyException("weka.attributeSelection.PrincipalComponents", e);
      }
  }

  /**
  //Test case number: 5
  /*Coverage entropy=1.6094379124341005
  */
  @Test(timeout = 4000)
  public void test05()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      AbstractClassifier.makeCopy(naiveBayesMultinomialText0);
      naiveBayesMultinomialText0.listOptions();
      SnowballStemmer snowballStemmer0 = new SnowballStemmer();
      naiveBayesMultinomialText0.setStemmer(snowballStemmer0);
      naiveBayesMultinomialText0.stemmerTipText();
      double[] doubleArray0 = new double[8];
      doubleArray0[0] = (-1101.0);
      doubleArray0[1] = (-1101.0);
      doubleArray0[2] = (-1101.0);
      doubleArray0[3] = (-1101.0);
      doubleArray0[4] = (-1101.0);
      doubleArray0[5] = (-1101.0);
      doubleArray0[6] = (-1101.0);
      doubleArray0[7] = (-1101.0);
      SparseInstance sparseInstance0 = new SparseInstance((-1101.0), doubleArray0);
      BinarySparseInstance binarySparseInstance0 = new BinarySparseInstance(sparseInstance0);
      boolean boolean0 = true;
      try { 
        naiveBayesMultinomialText0.updateClassifier((Instance) binarySparseInstance0, true);
        fail("Expecting exception: RuntimeException");
      
      } catch(RuntimeException e) {
         //
         // DenseInstance doesn't have access to a dataset!
         //
         verifyException("weka.core.AbstractInstance", e);
      }
  }

  /**
  //Test case number: 6
  /*Coverage entropy=1.6094379124341005
  */
  @Test(timeout = 4000)
  public void test06()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.setNormalizeDocLength(false);
      LinkedHashMap<String, NaiveBayesMultinomialText.Count> linkedHashMap0 = new LinkedHashMap<String, NaiveBayesMultinomialText.Count>();
      naiveBayesMultinomialText0.m_inputVector = linkedHashMap0;
      naiveBayesMultinomialText0.getRevision();
      naiveBayesMultinomialText0.stopwordsTipText();
      int[] intArray0 = new int[4];
      intArray0[0] = 0;
      intArray0[1] = (-2167);
      intArray0[2] = 0;
      intArray0[3] = 943;
      int int0 = (-1307);
      BinarySparseInstance binarySparseInstance0 = new BinarySparseInstance((-109.49498948695), intArray0, (-1307));
      String[] stringArray0 = new String[1];
      stringArray0[0] = "The file containing the stopwords (if this is a directory then the default ones are used).";
      BinarySparseInstance.main(stringArray0);
      try { 
        naiveBayesMultinomialText0.updateClassifier((Instance) binarySparseInstance0, true);
        fail("Expecting exception: RuntimeException");
      
      } catch(RuntimeException e) {
         //
         // DenseInstance doesn't have access to a dataset!
         //
         verifyException("weka.core.AbstractInstance", e);
      }
  }

  /**
  //Test case number: 7
  /*Coverage entropy=3.0910424533583143
  */
  @Test(timeout = 4000)
  public void test07()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      naiveBayesMultinomialText0.listOptions();
      naiveBayesMultinomialText0.toString();
      naiveBayesMultinomialText0.m_probOfWordGivenClass = null;
      naiveBayesMultinomialText0.m_probOfWordGivenClass = null;
      String[] stringArray0 = new String[1];
      stringArray0[0] = "";
      NaiveBayesMultinomialText.main(stringArray0);
      naiveBayesMultinomialText0.setNormalizeDocLength(false);
      naiveBayesMultinomialText0.setLNorm(3.0);
      naiveBayesMultinomialText0.setNorm(5721.639);
      Stemmer stemmer0 = naiveBayesMultinomialText0.getStemmer();
      naiveBayesMultinomialText0.getStopwords();
      naiveBayesMultinomialText0.setUseWordFrequencies(false);
      naiveBayesMultinomialText0.setStemmer(stemmer0);
      naiveBayesMultinomialText0.getOptions();
      naiveBayesMultinomialText0.getUseStopList();
      naiveBayesMultinomialText0.setNormalizeDocLength(false);
      naiveBayesMultinomialText0.getRevision();
      naiveBayesMultinomialText0.getPeriodicPruning();
      naiveBayesMultinomialText0.m_stemmer = stemmer0;
      naiveBayesMultinomialText0.setNormalizeDocLength(true);
      assertTrue(naiveBayesMultinomialText0.getNormalizeDocLength());
  }

  /**
  //Test case number: 8
  /*Coverage entropy=2.3978952727983707
  */
  @Test(timeout = 4000)
  public void test08()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertNotNull(naiveBayesMultinomialText0);
      
      naiveBayesMultinomialText0.m_leplace = 0.0;
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      
      LinkedHashMap<String, NaiveBayesMultinomialText.Count> linkedHashMap0 = new LinkedHashMap<String, NaiveBayesMultinomialText.Count>();
      assertTrue(linkedHashMap0.isEmpty());
      assertEquals(0, linkedHashMap0.size());
      assertNotNull(linkedHashMap0);
      
      naiveBayesMultinomialText0.m_inputVector = linkedHashMap0;
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertTrue(linkedHashMap0.isEmpty());
      assertEquals(0, linkedHashMap0.size());
      assertTrue(naiveBayesMultinomialText0.m_inputVector.isEmpty());
      assertEquals(0, naiveBayesMultinomialText0.m_inputVector.size());
      
      int int0 = naiveBayesMultinomialText0.getPeriodicPruning();
      assertEquals(0, int0);
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      
      String string0 = naiveBayesMultinomialText0.periodicPruningTipText();
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", string0);
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertNotNull(string0);
      
      SGDText sGDText0 = new SGDText();
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText0.normalizeDocLengthTipText());
      assertEquals("Whether to convert all tokens to lowercase", sGDText0.lowercaseTokensTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText0.stopwordsTipText());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText0.epochsTipText());
      assertEquals("The stemming algorithm to use on the words.", sGDText0.stemmerTipText());
      assertEquals(500, sGDText0.getEpochs());
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText0.useStopListTipText());
      assertEquals("The regularization constant. (default = 0.0001)", sGDText0.lambdaTipText());
      assertFalse(sGDText0.getUseWordFrequencies());
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText0.outputProbsForSVMTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText0.useWordFrequenciesTipText());
      assertEquals(1.0E-4, sGDText0.getLambda(), 0.01);
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText0.lossFunctionTipText());
      assertEquals(1.0, sGDText0.getNorm(), 0.01);
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText0.globalInfo());
      assertEquals("The norm of the instances after normalization.", sGDText0.normTipText());
      assertEquals("The LNorm to use for document length normalization.", sGDText0.LNormTipText());
      assertEquals(1, sGDText0.getSeed());
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText0.tokenizerTipText());
      assertFalse(sGDText0.getNormalizeDocLength());
      assertFalse(sGDText0.getOutputProbsForSVM());
      assertEquals(0.01, sGDText0.getLearningRate(), 0.01);
      assertEquals("The learning rate.", sGDText0.learningRateTipText());
      assertFalse(sGDText0.getLowercaseTokens());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText0.periodicPruningTipText());
      assertEquals("The random number seed to be used.", sGDText0.seedTipText());
      assertFalse(sGDText0.getDebug());
      assertEquals(0, sGDText0.getPeriodicPruning());
      assertEquals(3.0, sGDText0.getMinWordFrequency(), 0.01);
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText0.minWordFrequencyTipText());
      assertEquals(2.0, sGDText0.getLNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText0.debugTipText());
      assertFalse(sGDText0.getUseStopList());
      assertEquals(0, SGDText.HINGE);
      assertEquals(1, SGDText.LOGLOSS);
      assertNotNull(sGDText0);
      
      Capabilities capabilities0 = sGDText0.getCapabilities();
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText0.normalizeDocLengthTipText());
      assertEquals("Whether to convert all tokens to lowercase", sGDText0.lowercaseTokensTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText0.stopwordsTipText());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText0.epochsTipText());
      assertEquals("The stemming algorithm to use on the words.", sGDText0.stemmerTipText());
      assertEquals(500, sGDText0.getEpochs());
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText0.useStopListTipText());
      assertEquals("The regularization constant. (default = 0.0001)", sGDText0.lambdaTipText());
      assertFalse(sGDText0.getUseWordFrequencies());
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText0.outputProbsForSVMTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText0.useWordFrequenciesTipText());
      assertEquals(1.0E-4, sGDText0.getLambda(), 0.01);
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText0.lossFunctionTipText());
      assertEquals(1.0, sGDText0.getNorm(), 0.01);
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText0.globalInfo());
      assertEquals("The norm of the instances after normalization.", sGDText0.normTipText());
      assertEquals("The LNorm to use for document length normalization.", sGDText0.LNormTipText());
      assertEquals(1, sGDText0.getSeed());
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText0.tokenizerTipText());
      assertFalse(sGDText0.getNormalizeDocLength());
      assertFalse(sGDText0.getOutputProbsForSVM());
      assertEquals(0.01, sGDText0.getLearningRate(), 0.01);
      assertEquals("The learning rate.", sGDText0.learningRateTipText());
      assertFalse(sGDText0.getLowercaseTokens());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText0.periodicPruningTipText());
      assertEquals("The random number seed to be used.", sGDText0.seedTipText());
      assertFalse(sGDText0.getDebug());
      assertEquals(0, sGDText0.getPeriodicPruning());
      assertEquals(3.0, sGDText0.getMinWordFrequency(), 0.01);
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText0.minWordFrequencyTipText());
      assertEquals(2.0, sGDText0.getLNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText0.debugTipText());
      assertFalse(sGDText0.getUseStopList());
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      assertFalse(capabilities0.hasDependencies());
      assertEquals(0, SGDText.HINGE);
      assertEquals(1, SGDText.LOGLOSS);
      assertNotNull(capabilities0);
      
      NullStemmer nullStemmer0 = (NullStemmer)sGDText0.getStemmer();
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText0.normalizeDocLengthTipText());
      assertEquals("Whether to convert all tokens to lowercase", sGDText0.lowercaseTokensTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText0.stopwordsTipText());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText0.epochsTipText());
      assertEquals("The stemming algorithm to use on the words.", sGDText0.stemmerTipText());
      assertEquals(500, sGDText0.getEpochs());
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText0.useStopListTipText());
      assertEquals("The regularization constant. (default = 0.0001)", sGDText0.lambdaTipText());
      assertFalse(sGDText0.getUseWordFrequencies());
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText0.outputProbsForSVMTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText0.useWordFrequenciesTipText());
      assertEquals(1.0E-4, sGDText0.getLambda(), 0.01);
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText0.lossFunctionTipText());
      assertEquals(1.0, sGDText0.getNorm(), 0.01);
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText0.globalInfo());
      assertEquals("The norm of the instances after normalization.", sGDText0.normTipText());
      assertEquals("The LNorm to use for document length normalization.", sGDText0.LNormTipText());
      assertEquals(1, sGDText0.getSeed());
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText0.tokenizerTipText());
      assertFalse(sGDText0.getNormalizeDocLength());
      assertFalse(sGDText0.getOutputProbsForSVM());
      assertEquals(0.01, sGDText0.getLearningRate(), 0.01);
      assertEquals("The learning rate.", sGDText0.learningRateTipText());
      assertFalse(sGDText0.getLowercaseTokens());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText0.periodicPruningTipText());
      assertEquals("The random number seed to be used.", sGDText0.seedTipText());
      assertFalse(sGDText0.getDebug());
      assertEquals(0, sGDText0.getPeriodicPruning());
      assertEquals(3.0, sGDText0.getMinWordFrequency(), 0.01);
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText0.minWordFrequencyTipText());
      assertEquals(2.0, sGDText0.getLNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText0.debugTipText());
      assertFalse(sGDText0.getUseStopList());
      assertEquals("A dummy stemmer that performs no stemming at all.", nullStemmer0.globalInfo());
      assertEquals(0, SGDText.HINGE);
      assertEquals(1, SGDText.LOGLOSS);
      assertNotNull(nullStemmer0);
      
      naiveBayesMultinomialText0.setStemmer(nullStemmer0);
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", sGDText0.normalizeDocLengthTipText());
      assertEquals("Whether to convert all tokens to lowercase", sGDText0.lowercaseTokensTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", sGDText0.stopwordsTipText());
      assertEquals("The number of epochs to perform (batch learning). The total number of iterations is epochs * num instances.", sGDText0.epochsTipText());
      assertEquals("The stemming algorithm to use on the words.", sGDText0.stemmerTipText());
      assertEquals(500, sGDText0.getEpochs());
      assertEquals("If true, ignores all words that are on the stoplist.", sGDText0.useStopListTipText());
      assertEquals("The regularization constant. (default = 0.0001)", sGDText0.lambdaTipText());
      assertFalse(sGDText0.getUseWordFrequencies());
      assertEquals("Fit a logistic regression to the output of SVM for producing probability estimates", sGDText0.outputProbsForSVMTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", sGDText0.useWordFrequenciesTipText());
      assertEquals(1.0E-4, sGDText0.getLambda(), 0.01);
      assertEquals("The loss function to use. Hinge loss (SVM), log loss (logistic regression) or squared loss (regression).", sGDText0.lossFunctionTipText());
      assertEquals(1.0, sGDText0.getNorm(), 0.01);
      assertEquals("Implements stochastic gradient descent for learning a linear binary class SVM or binary class logistic regression on text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification.", sGDText0.globalInfo());
      assertEquals("The norm of the instances after normalization.", sGDText0.normTipText());
      assertEquals("The LNorm to use for document length normalization.", sGDText0.LNormTipText());
      assertEquals(1, sGDText0.getSeed());
      assertEquals("The tokenizing algorithm to use on the strings.", sGDText0.tokenizerTipText());
      assertFalse(sGDText0.getNormalizeDocLength());
      assertFalse(sGDText0.getOutputProbsForSVM());
      assertEquals(0.01, sGDText0.getLearningRate(), 0.01);
      assertEquals("The learning rate.", sGDText0.learningRateTipText());
      assertFalse(sGDText0.getLowercaseTokens());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", sGDText0.periodicPruningTipText());
      assertEquals("The random number seed to be used.", sGDText0.seedTipText());
      assertFalse(sGDText0.getDebug());
      assertEquals(0, sGDText0.getPeriodicPruning());
      assertEquals(3.0, sGDText0.getMinWordFrequency(), 0.01);
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", sGDText0.minWordFrequencyTipText());
      assertEquals(2.0, sGDText0.getLNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", sGDText0.debugTipText());
      assertFalse(sGDText0.getUseStopList());
      assertEquals("A dummy stemmer that performs no stemming at all.", nullStemmer0.globalInfo());
      assertEquals(0, SGDText.HINGE);
      assertEquals(1, SGDText.LOGLOSS);
      
      String string1 = naiveBayesMultinomialText0.stopwordsTipText();
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", string1);
      assertFalse(string1.equals((Object)string0));
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertNotNull(string1);
      
      double double0 = naiveBayesMultinomialText0.getLNorm();
      assertEquals(2.0, double0, 0.01);
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      
      double double1 = naiveBayesMultinomialText0.getMinWordFrequency();
      assertEquals(3.0, double1, 0.01);
      assertNotEquals(double1, double0, 0.01);
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      
      boolean boolean0 = naiveBayesMultinomialText0.getLowercaseTokens();
      assertFalse(boolean0);
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      
      MockFile mockFile0 = new MockFile("weka/core/Capabilities.props");
      assertNotNull(mockFile0);
      
      boolean boolean1 = mockFile0.delete();
      assertFalse(boolean1);
      assertTrue(boolean1 == boolean0);
      assertEquals(0L, mockFile0.getFreeSpace());
      assertFalse(mockFile0.isAbsolute());
      assertEquals("weka/core", mockFile0.getParent());
      assertEquals(0L, mockFile0.length());
      assertEquals(0L, mockFile0.getTotalSpace());
      assertEquals("Capabilities.props", mockFile0.getName());
      assertFalse(mockFile0.canRead());
      assertEquals("weka/core/Capabilities.props", mockFile0.toString());
      assertFalse(mockFile0.canWrite());
      assertFalse(mockFile0.isDirectory());
      assertEquals(0L, mockFile0.getUsableSpace());
      assertEquals(0L, mockFile0.lastModified());
      assertFalse(mockFile0.isHidden());
      assertFalse(mockFile0.exists());
      assertTrue(mockFile0.isFile());
      assertFalse(mockFile0.canExecute());
      
      naiveBayesMultinomialText0.setStopwords(mockFile0);
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(0L, mockFile0.getFreeSpace());
      assertFalse(mockFile0.isAbsolute());
      assertEquals("weka/core", mockFile0.getParent());
      assertEquals(0L, mockFile0.length());
      assertEquals(0L, mockFile0.getTotalSpace());
      assertEquals("Capabilities.props", mockFile0.getName());
      assertFalse(mockFile0.canRead());
      assertEquals("weka/core/Capabilities.props", mockFile0.toString());
      assertFalse(mockFile0.canWrite());
      assertFalse(mockFile0.isDirectory());
      assertEquals(0L, mockFile0.getUsableSpace());
      assertEquals(0L, mockFile0.lastModified());
      assertFalse(mockFile0.isHidden());
      assertFalse(mockFile0.exists());
      assertTrue(mockFile0.isFile());
      assertFalse(mockFile0.canExecute());
      
      String string2 = naiveBayesMultinomialText0.LNormTipText();
      assertEquals("The LNorm to use for document length normalization.", string2);
      assertFalse(string2.equals((Object)string1));
      assertFalse(string2.equals((Object)string0));
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertNotNull(string2);
      
      String string3 = naiveBayesMultinomialText0.getRevision();
      assertEquals("9122", string3);
      assertFalse(string3.equals((Object)string2));
      assertFalse(string3.equals((Object)string1));
      assertFalse(string3.equals((Object)string0));
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertNotNull(string3);
      
      naiveBayesMultinomialText0.setStopwords(mockFile0);
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(0L, mockFile0.getFreeSpace());
      assertFalse(mockFile0.isAbsolute());
      assertEquals("weka/core", mockFile0.getParent());
      assertEquals(0L, mockFile0.length());
      assertEquals(0L, mockFile0.getTotalSpace());
      assertEquals("Capabilities.props", mockFile0.getName());
      assertFalse(mockFile0.canRead());
      assertEquals("weka/core/Capabilities.props", mockFile0.toString());
      assertFalse(mockFile0.canWrite());
      assertFalse(mockFile0.isDirectory());
      assertEquals(0L, mockFile0.getUsableSpace());
      assertEquals(0L, mockFile0.lastModified());
      assertFalse(mockFile0.isHidden());
      assertFalse(mockFile0.exists());
      assertTrue(mockFile0.isFile());
      assertFalse(mockFile0.canExecute());
  }

  /**
  //Test case number: 9
  /*Coverage entropy=1.945910149055313
  */
  @Test(timeout = 4000)
  public void test09()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertNotNull(naiveBayesMultinomialText0);
      
      String string0 = naiveBayesMultinomialText0.globalInfo();
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", string0);
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertNotNull(string0);
      
      NullStemmer nullStemmer0 = new NullStemmer();
      assertEquals("A dummy stemmer that performs no stemming at all.", nullStemmer0.globalInfo());
      assertNotNull(nullStemmer0);
      
      naiveBayesMultinomialText0.m_stemmer = (Stemmer) nullStemmer0;
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("A dummy stemmer that performs no stemming at all.", nullStemmer0.globalInfo());
      
      Classifier[] classifierArray0 = AbstractClassifier.makeCopies(naiveBayesMultinomialText0, 0);
      assertEquals(0, classifierArray0.length);
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertNotNull(classifierArray0);
      
      naiveBayesMultinomialText0.setUseWordFrequencies(true);
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertTrue(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      
      String string1 = nullStemmer0.toString();
      assertEquals("weka.core.stemmers.NullStemmer", string1);
      assertFalse(string1.equals((Object)string0));
      assertEquals("A dummy stemmer that performs no stemming at all.", nullStemmer0.globalInfo());
      assertNotNull(string1);
      
      Capabilities capabilities0 = naiveBayesMultinomialText0.getCapabilities();
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertTrue(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      assertFalse(capabilities0.hasDependencies());
      assertNotNull(capabilities0);
      
      Enumeration<Option> enumeration0 = naiveBayesMultinomialText0.listOptions();
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertTrue(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertNotNull(enumeration0);
      
      String string2 = naiveBayesMultinomialText0.LNormTipText();
      assertEquals("The LNorm to use for document length normalization.", string2);
      assertFalse(string2.equals((Object)string0));
      assertFalse(string2.equals((Object)string1));
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertTrue(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertNotNull(string2);
      
      String string3 = naiveBayesMultinomialText0.tokenizerTipText();
      assertEquals("The tokenizing algorithm to use on the strings.", string3);
      assertFalse(string3.equals((Object)string1));
      assertFalse(string3.equals((Object)string0));
      assertFalse(string3.equals((Object)string2));
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertTrue(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertNotNull(string3);
  }

  /**
  //Test case number: 10
  /*Coverage entropy=2.1972245773362196
  */
  @Test(timeout = 4000)
  public void test10()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertNotNull(naiveBayesMultinomialText0);
      
      String[] stringArray0 = new String[3];
      stringArray0[0] = "";
      naiveBayesMultinomialText0.setNormalizeDocLength(true);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertTrue(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      
      stringArray0[1] = "|a+*lacMDUI";
      stringArray0[2] = "stoplist";
      AbstractClassifier.runClassifier(naiveBayesMultinomialText0, stringArray0);
      assertEquals(3, stringArray0.length);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertTrue(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      
      naiveBayesMultinomialText0.pruneDictionary();
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertTrue(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      
      naiveBayesMultinomialText0.setUseStopList(false);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertTrue(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      
      String string0 = naiveBayesMultinomialText0.stopwordsTipText();
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", string0);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertTrue(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertNotNull(string0);
      
      Capabilities capabilities0 = naiveBayesMultinomialText0.getCapabilities();
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertTrue(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      assertFalse(capabilities0.hasDependencies());
      assertNotNull(capabilities0);
      
      String string1 = naiveBayesMultinomialText0.useStopListTipText();
      assertEquals("If true, ignores all words that are on the stoplist.", string1);
      assertFalse(string1.equals((Object)string0));
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertTrue(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertNotNull(string1);
      
      String string2 = naiveBayesMultinomialText0.normalizeDocLengthTipText();
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", string2);
      assertFalse(string2.equals((Object)string1));
      assertFalse(string2.equals((Object)string0));
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertTrue(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertNotNull(string2);
  }

  /**
  //Test case number: 11
  /*Coverage entropy=1.8937882323911377
  */
  @Test(timeout = 4000)
  public void test11()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertNotNull(naiveBayesMultinomialText0);
      
      LinkedHashMap<Integer, LinkedHashMap<String, NaiveBayesMultinomialText.Count>> linkedHashMap0 = new LinkedHashMap<Integer, LinkedHashMap<String, NaiveBayesMultinomialText.Count>>();
      assertTrue(linkedHashMap0.isEmpty());
      assertEquals(0, linkedHashMap0.size());
      assertNotNull(linkedHashMap0);
      
      naiveBayesMultinomialText0.m_probOfWordGivenClass = (Map<Integer, LinkedHashMap<String, NaiveBayesMultinomialText.Count>>) linkedHashMap0;
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertTrue(linkedHashMap0.isEmpty());
      assertEquals(0, linkedHashMap0.size());
      assertTrue(naiveBayesMultinomialText0.m_probOfWordGivenClass.isEmpty());
      assertEquals(0, naiveBayesMultinomialText0.m_probOfWordGivenClass.size());
      
      naiveBayesMultinomialText0.setMinWordFrequency(0.0);
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(0.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      
      naiveBayesMultinomialText0.m_leplace = 0.0;
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(0.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      
      naiveBayesMultinomialText0.setNorm(0.0);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(0.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals(0.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      
      Integer integer0 = new Integer((-3702));
      assertEquals((-3702), (int)integer0);
      assertNotNull(integer0);
      
      naiveBayesMultinomialText0.setNorm(0.0);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(0.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals(0.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      
      Integer integer1 = Integer.getInteger("");
      assertNull(integer1);
      
      LinkedHashMap<String, NaiveBayesMultinomialText.Count> linkedHashMap1 = naiveBayesMultinomialText0.m_inputVector;
      assertNull(linkedHashMap1);
      
      LinkedHashMap<String, NaiveBayesMultinomialText.Count> linkedHashMap2 = linkedHashMap0.put(integer0, (LinkedHashMap<String, NaiveBayesMultinomialText.Count>) null);
      assertFalse(linkedHashMap0.isEmpty());
      assertEquals(1, linkedHashMap0.size());
      assertNull(linkedHashMap2);
      
      String string0 = naiveBayesMultinomialText0.getRevision();
      assertEquals("9122", string0);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(0.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals(0.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertNotNull(string0);
      
      naiveBayesMultinomialText0.setMinWordFrequency((-347.231942331));
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals(0.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals((-347.231942331), naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      
      String string1 = naiveBayesMultinomialText0.tokenizerTipText();
      assertEquals("The tokenizing algorithm to use on the strings.", string1);
      assertFalse(string1.equals((Object)string0));
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals(0.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals((-347.231942331), naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertNotNull(string1);
      
      Capabilities capabilities0 = naiveBayesMultinomialText0.getCapabilities();
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals(0.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals((-347.231942331), naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      assertFalse(capabilities0.hasDependencies());
      assertNotNull(capabilities0);
      
      double[] doubleArray0 = new double[3];
      doubleArray0[0] = 0.0;
      doubleArray0[1] = 0.0;
      doubleArray0[2] = (-4684.241846445425);
      int[] intArray0 = new int[6];
      intArray0[0] = (-3702);
      intArray0[1] = (-3702);
      intArray0[2] = (-3702);
      intArray0[3] = (-3702);
      intArray0[4] = (-3702);
      intArray0[5] = (-3702);
      SparseInstance sparseInstance0 = new SparseInstance((-4684.241846445425), doubleArray0, intArray0, (-1781));
      assertArrayEquals(new int[] {(-3702), (-3702), (-3702), (-3702), (-3702), (-3702)}, intArray0);
      assertArrayEquals(new double[] {0.0, 0.0, (-4684.241846445425)}, doubleArray0, 0.01);
      assertEquals(6, intArray0.length);
      assertEquals(3, doubleArray0.length);
      assertEquals(1, sparseInstance0.numValues());
      assertEquals((-4684.241846445425), sparseInstance0.weight(), 0.01);
      assertEquals((-1781), sparseInstance0.numAttributes());
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertNotNull(sparseInstance0);
      
      try { 
        naiveBayesMultinomialText0.distributionForInstance(sparseInstance0);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("weka.classifiers.bayes.NaiveBayesMultinomialText", e);
      }
  }

  /**
  //Test case number: 12
  /*Coverage entropy=3.0910424533583143
  */
  @Test(timeout = 4000)
  public void test12()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertNotNull(naiveBayesMultinomialText0);
      
      IteratedLovinsStemmer iteratedLovinsStemmer0 = new IteratedLovinsStemmer();
      assertNotNull(iteratedLovinsStemmer0);
      
      naiveBayesMultinomialText0.setStemmer(iteratedLovinsStemmer0);
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      
      NaiveBayesMultinomialText naiveBayesMultinomialText1 = new NaiveBayesMultinomialText();
      assertFalse(naiveBayesMultinomialText1.equals((Object)naiveBayesMultinomialText0));
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText1.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText1.stemmerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText1.minWordFrequencyTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText1.lowercaseTokensTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText1.normTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText1.globalInfo());
      assertFalse(naiveBayesMultinomialText1.getUseWordFrequencies());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText1.normalizeDocLengthTipText());
      assertEquals(2.0, naiveBayesMultinomialText1.getLNorm(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText1.stopwordsTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText1.useWordFrequenciesTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText1.useStopListTipText());
      assertEquals(0, naiveBayesMultinomialText1.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText1.getLowercaseTokens());
      assertEquals(1.0, naiveBayesMultinomialText1.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText1.getUseStopList());
      assertFalse(naiveBayesMultinomialText1.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText1.getDebug());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText1.tokenizerTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText1.periodicPruningTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText1.LNormTipText());
      assertEquals(3.0, naiveBayesMultinomialText1.getMinWordFrequency(), 0.01);
      assertNotNull(naiveBayesMultinomialText1);
      
      naiveBayesMultinomialText1.setNormalizeDocLength(true);
      assertNotSame(naiveBayesMultinomialText1, naiveBayesMultinomialText0);
      assertFalse(naiveBayesMultinomialText1.equals((Object)naiveBayesMultinomialText0));
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText1.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText1.stemmerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText1.minWordFrequencyTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText1.lowercaseTokensTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText1.normTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText1.globalInfo());
      assertFalse(naiveBayesMultinomialText1.getUseWordFrequencies());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText1.normalizeDocLengthTipText());
      assertEquals(2.0, naiveBayesMultinomialText1.getLNorm(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText1.stopwordsTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText1.useWordFrequenciesTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText1.useStopListTipText());
      assertEquals(0, naiveBayesMultinomialText1.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText1.getLowercaseTokens());
      assertEquals(1.0, naiveBayesMultinomialText1.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText1.getUseStopList());
      assertFalse(naiveBayesMultinomialText1.getDebug());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText1.tokenizerTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText1.periodicPruningTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText1.LNormTipText());
      assertTrue(naiveBayesMultinomialText1.getNormalizeDocLength());
      assertEquals(3.0, naiveBayesMultinomialText1.getMinWordFrequency(), 0.01);
      
      VotedPerceptron votedPerceptron0 = new VotedPerceptron();
      assertEquals("The maximum number of alterations to the perceptron.", votedPerceptron0.maxKTipText());
      assertEquals("Seed for the random number generator.", votedPerceptron0.seedTipText());
      assertEquals("Number of iterations to be performed.", votedPerceptron0.numIterationsTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", votedPerceptron0.debugTipText());
      assertEquals(1, votedPerceptron0.getSeed());
      assertFalse(votedPerceptron0.getDebug());
      assertEquals(10000, votedPerceptron0.getMaxK());
      assertEquals(1, votedPerceptron0.getNumIterations());
      assertEquals("Exponent for the polynomial kernel.", votedPerceptron0.exponentTipText());
      assertEquals(1.0, votedPerceptron0.getExponent(), 0.01);
      assertNotNull(votedPerceptron0);
      
      String[] stringArray0 = new String[5];
      stringArray0[0] = "acc";
      stringArray0[1] = "|a+*lacMDUI";
      stringArray0[2] = "|a+*lacMDUI";
      stringArray0[3] = "";
      stringArray0[4] = "|a+*lacMDUI";
      AbstractClassifier.runClassifier(votedPerceptron0, stringArray0);
      assertEquals(5, stringArray0.length);
      assertEquals("The maximum number of alterations to the perceptron.", votedPerceptron0.maxKTipText());
      assertEquals("Seed for the random number generator.", votedPerceptron0.seedTipText());
      assertEquals("Number of iterations to be performed.", votedPerceptron0.numIterationsTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", votedPerceptron0.debugTipText());
      assertEquals(1, votedPerceptron0.getSeed());
      assertFalse(votedPerceptron0.getDebug());
      assertEquals(10000, votedPerceptron0.getMaxK());
      assertEquals(1, votedPerceptron0.getNumIterations());
      assertEquals("Exponent for the polynomial kernel.", votedPerceptron0.exponentTipText());
      assertEquals(1.0, votedPerceptron0.getExponent(), 0.01);
      
      naiveBayesMultinomialText1.pruneDictionary();
      assertNotSame(naiveBayesMultinomialText1, naiveBayesMultinomialText0);
      assertFalse(naiveBayesMultinomialText1.equals((Object)naiveBayesMultinomialText0));
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText1.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText1.stemmerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText1.minWordFrequencyTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText1.lowercaseTokensTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText1.normTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText1.globalInfo());
      assertFalse(naiveBayesMultinomialText1.getUseWordFrequencies());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText1.normalizeDocLengthTipText());
      assertEquals(2.0, naiveBayesMultinomialText1.getLNorm(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText1.stopwordsTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText1.useWordFrequenciesTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText1.useStopListTipText());
      assertEquals(0, naiveBayesMultinomialText1.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText1.getLowercaseTokens());
      assertEquals(1.0, naiveBayesMultinomialText1.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText1.getUseStopList());
      assertFalse(naiveBayesMultinomialText1.getDebug());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText1.tokenizerTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText1.periodicPruningTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText1.LNormTipText());
      assertTrue(naiveBayesMultinomialText1.getNormalizeDocLength());
      assertEquals(3.0, naiveBayesMultinomialText1.getMinWordFrequency(), 0.01);
      
      naiveBayesMultinomialText1.setUseStopList(true);
      assertNotSame(naiveBayesMultinomialText1, naiveBayesMultinomialText0);
      assertFalse(naiveBayesMultinomialText1.equals((Object)naiveBayesMultinomialText0));
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText1.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText1.stemmerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText1.minWordFrequencyTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText1.lowercaseTokensTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText1.normTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText1.globalInfo());
      assertFalse(naiveBayesMultinomialText1.getUseWordFrequencies());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText1.normalizeDocLengthTipText());
      assertTrue(naiveBayesMultinomialText1.getUseStopList());
      assertEquals(2.0, naiveBayesMultinomialText1.getLNorm(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText1.stopwordsTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText1.useWordFrequenciesTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText1.useStopListTipText());
      assertEquals(0, naiveBayesMultinomialText1.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText1.getLowercaseTokens());
      assertEquals(1.0, naiveBayesMultinomialText1.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText1.getDebug());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText1.tokenizerTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText1.periodicPruningTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText1.LNormTipText());
      assertTrue(naiveBayesMultinomialText1.getNormalizeDocLength());
      assertEquals(3.0, naiveBayesMultinomialText1.getMinWordFrequency(), 0.01);
      
      String string0 = naiveBayesMultinomialText1.stopwordsTipText();
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", string0);
      assertNotSame(naiveBayesMultinomialText1, naiveBayesMultinomialText0);
      assertFalse(naiveBayesMultinomialText1.equals((Object)naiveBayesMultinomialText0));
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText1.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText1.stemmerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText1.minWordFrequencyTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText1.lowercaseTokensTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText1.normTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText1.globalInfo());
      assertFalse(naiveBayesMultinomialText1.getUseWordFrequencies());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText1.normalizeDocLengthTipText());
      assertTrue(naiveBayesMultinomialText1.getUseStopList());
      assertEquals(2.0, naiveBayesMultinomialText1.getLNorm(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText1.stopwordsTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText1.useWordFrequenciesTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText1.useStopListTipText());
      assertEquals(0, naiveBayesMultinomialText1.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText1.getLowercaseTokens());
      assertEquals(1.0, naiveBayesMultinomialText1.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText1.getDebug());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText1.tokenizerTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText1.periodicPruningTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText1.LNormTipText());
      assertTrue(naiveBayesMultinomialText1.getNormalizeDocLength());
      assertEquals(3.0, naiveBayesMultinomialText1.getMinWordFrequency(), 0.01);
      assertNotNull(string0);
      
      Capabilities capabilities0 = naiveBayesMultinomialText1.getCapabilities();
      assertNotSame(naiveBayesMultinomialText1, naiveBayesMultinomialText0);
      assertFalse(naiveBayesMultinomialText1.equals((Object)naiveBayesMultinomialText0));
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      assertFalse(capabilities0.hasDependencies());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText1.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText1.stemmerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText1.minWordFrequencyTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText1.lowercaseTokensTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText1.normTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText1.globalInfo());
      assertFalse(naiveBayesMultinomialText1.getUseWordFrequencies());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText1.normalizeDocLengthTipText());
      assertTrue(naiveBayesMultinomialText1.getUseStopList());
      assertEquals(2.0, naiveBayesMultinomialText1.getLNorm(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText1.stopwordsTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText1.useWordFrequenciesTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText1.useStopListTipText());
      assertEquals(0, naiveBayesMultinomialText1.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText1.getLowercaseTokens());
      assertEquals(1.0, naiveBayesMultinomialText1.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText1.getDebug());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText1.tokenizerTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText1.periodicPruningTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText1.LNormTipText());
      assertTrue(naiveBayesMultinomialText1.getNormalizeDocLength());
      assertEquals(3.0, naiveBayesMultinomialText1.getMinWordFrequency(), 0.01);
      assertNotNull(capabilities0);
      
      String string1 = naiveBayesMultinomialText1.useStopListTipText();
      assertEquals("If true, ignores all words that are on the stoplist.", string1);
      assertNotSame(naiveBayesMultinomialText1, naiveBayesMultinomialText0);
      assertFalse(string1.equals((Object)string0));
      assertFalse(naiveBayesMultinomialText1.equals((Object)naiveBayesMultinomialText0));
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText1.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText1.stemmerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText1.minWordFrequencyTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText1.lowercaseTokensTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText1.normTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText1.globalInfo());
      assertFalse(naiveBayesMultinomialText1.getUseWordFrequencies());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText1.normalizeDocLengthTipText());
      assertTrue(naiveBayesMultinomialText1.getUseStopList());
      assertEquals(2.0, naiveBayesMultinomialText1.getLNorm(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText1.stopwordsTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText1.useWordFrequenciesTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText1.useStopListTipText());
      assertEquals(0, naiveBayesMultinomialText1.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText1.getLowercaseTokens());
      assertEquals(1.0, naiveBayesMultinomialText1.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText1.getDebug());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText1.tokenizerTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText1.periodicPruningTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText1.LNormTipText());
      assertTrue(naiveBayesMultinomialText1.getNormalizeDocLength());
      assertEquals(3.0, naiveBayesMultinomialText1.getMinWordFrequency(), 0.01);
      assertNotNull(string1);
      
      String string2 = naiveBayesMultinomialText1.normalizeDocLengthTipText();
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", string2);
      assertNotSame(naiveBayesMultinomialText1, naiveBayesMultinomialText0);
      assertFalse(string2.equals((Object)string1));
      assertFalse(string2.equals((Object)string0));
      assertFalse(naiveBayesMultinomialText1.equals((Object)naiveBayesMultinomialText0));
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText1.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText1.stemmerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText1.minWordFrequencyTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText1.lowercaseTokensTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText1.normTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText1.globalInfo());
      assertFalse(naiveBayesMultinomialText1.getUseWordFrequencies());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText1.normalizeDocLengthTipText());
      assertTrue(naiveBayesMultinomialText1.getUseStopList());
      assertEquals(2.0, naiveBayesMultinomialText1.getLNorm(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText1.stopwordsTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText1.useWordFrequenciesTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText1.useStopListTipText());
      assertEquals(0, naiveBayesMultinomialText1.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText1.getLowercaseTokens());
      assertEquals(1.0, naiveBayesMultinomialText1.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText1.getDebug());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText1.tokenizerTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText1.periodicPruningTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText1.LNormTipText());
      assertTrue(naiveBayesMultinomialText1.getNormalizeDocLength());
      assertEquals(3.0, naiveBayesMultinomialText1.getMinWordFrequency(), 0.01);
      assertNotNull(string2);
      
      String[] stringArray1 = naiveBayesMultinomialText0.getOptions();
      assertEquals(12, stringArray1.length);
      assertNotSame(naiveBayesMultinomialText0, naiveBayesMultinomialText1);
      assertNotSame(stringArray1, stringArray0);
      assertFalse(naiveBayesMultinomialText0.equals((Object)naiveBayesMultinomialText1));
      assertFalse(stringArray1.equals((Object)stringArray0));
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertNotNull(stringArray1);
      
      naiveBayesMultinomialText1.setLowercaseTokens(true);
      assertNotSame(naiveBayesMultinomialText1, naiveBayesMultinomialText0);
      assertFalse(naiveBayesMultinomialText1.equals((Object)naiveBayesMultinomialText0));
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText1.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText1.stemmerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText1.minWordFrequencyTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText1.lowercaseTokensTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText1.normTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText1.globalInfo());
      assertFalse(naiveBayesMultinomialText1.getUseWordFrequencies());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText1.normalizeDocLengthTipText());
      assertTrue(naiveBayesMultinomialText1.getUseStopList());
      assertEquals(2.0, naiveBayesMultinomialText1.getLNorm(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText1.stopwordsTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText1.useWordFrequenciesTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText1.useStopListTipText());
      assertEquals(0, naiveBayesMultinomialText1.getPeriodicPruning());
      assertEquals(1.0, naiveBayesMultinomialText1.getNorm(), 0.01);
      assertTrue(naiveBayesMultinomialText1.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText1.getDebug());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText1.tokenizerTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText1.periodicPruningTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText1.LNormTipText());
      assertTrue(naiveBayesMultinomialText1.getNormalizeDocLength());
      assertEquals(3.0, naiveBayesMultinomialText1.getMinWordFrequency(), 0.01);
  }

  /**
  //Test case number: 13
  /*Coverage entropy=3.0280290733074104
  */
  @Test(timeout = 4000)
  public void test13()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertNotNull(naiveBayesMultinomialText0);
      
      IteratedLovinsStemmer iteratedLovinsStemmer0 = new IteratedLovinsStemmer();
      assertNotNull(iteratedLovinsStemmer0);
      
      naiveBayesMultinomialText0.setStemmer(iteratedLovinsStemmer0);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      
      naiveBayesMultinomialText0.setNormalizeDocLength(true);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertTrue(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      
      WordTokenizer wordTokenizer0 = (WordTokenizer)naiveBayesMultinomialText0.getTokenizer();
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertTrue(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals(" \r\n\t.,;:'\"()?!", wordTokenizer0.getDelimiters());
      assertEquals("A simple tokenizer that is using the java.util.StringTokenizer class to tokenize the strings.", wordTokenizer0.globalInfo());
      assertEquals("Set of delimiter characters to use in tokenizing (\\r, \\n and \\t can be used for carriage-return, line-feed and tab)", wordTokenizer0.delimitersTipText());
      assertNotNull(wordTokenizer0);
      
      String[] stringArray0 = new String[5];
      stringArray0[0] = ":_8]Ab_";
      stringArray0[1] = "";
      stringArray0[2] = "";
      stringArray0[3] = "|x{Ao80G|@z";
      stringArray0[4] = "#~PiA}g#WQ ";
      NaiveBayesMultinomialText.main(stringArray0);
      assertEquals(5, stringArray0.length);
      
      naiveBayesMultinomialText0.setNormalizeDocLength(true);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertTrue(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      
      naiveBayesMultinomialText0.setLNorm(3.0);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertTrue(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      
      naiveBayesMultinomialText0.setNorm(5721.639);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertTrue(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(5721.639, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      
      IteratedLovinsStemmer iteratedLovinsStemmer1 = (IteratedLovinsStemmer)naiveBayesMultinomialText0.getStemmer();
      assertSame(iteratedLovinsStemmer1, iteratedLovinsStemmer0);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertTrue(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(5721.639, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertNotNull(iteratedLovinsStemmer1);
      
      MockFile mockFile0 = (MockFile)naiveBayesMultinomialText0.getStopwords();
      assertEquals(1392409281320L, mockFile0.lastModified());
      assertEquals(0L, mockFile0.length());
      assertEquals(0L, mockFile0.getFreeSpace());
      assertTrue(mockFile0.isDirectory());
      assertEquals(0L, mockFile0.getTotalSpace());
      assertTrue(mockFile0.exists());
      assertEquals("107_weka", mockFile0.getName());
      assertEquals("/home/ubuntu/evosuite_readability_gen/projects", mockFile0.getParent());
      assertTrue(mockFile0.canExecute());
      assertEquals(0L, mockFile0.getUsableSpace());
      assertFalse(mockFile0.isHidden());
      assertTrue(mockFile0.canWrite());
      assertTrue(mockFile0.isAbsolute());
      assertTrue(mockFile0.canRead());
      assertEquals("/home/ubuntu/evosuite_readability_gen/projects/107_weka", mockFile0.toString());
      assertFalse(mockFile0.isFile());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertTrue(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(5721.639, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertNotNull(mockFile0);
      
      naiveBayesMultinomialText0.setUseWordFrequencies(false);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertTrue(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(5721.639, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      
      naiveBayesMultinomialText0.setStemmer(iteratedLovinsStemmer0);
      assertSame(iteratedLovinsStemmer0, iteratedLovinsStemmer1);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertTrue(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(5721.639, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      
      String[] stringArray1 = naiveBayesMultinomialText0.getOptions();
      assertEquals(13, stringArray1.length);
      assertNotSame(stringArray1, stringArray0);
      assertFalse(stringArray1.equals((Object)stringArray0));
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertTrue(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(5721.639, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertNotNull(stringArray1);
      
      boolean boolean0 = naiveBayesMultinomialText0.getUseStopList();
      assertFalse(boolean0);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertTrue(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(5721.639, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      
      naiveBayesMultinomialText0.setNormalizeDocLength(true);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertTrue(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(5721.639, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      
      String string0 = naiveBayesMultinomialText0.getRevision();
      assertEquals("9122", string0);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertTrue(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(5721.639, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertNotNull(string0);
      
      int int0 = naiveBayesMultinomialText0.getPeriodicPruning();
      assertEquals(0, int0);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertTrue(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(5721.639, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      
      naiveBayesMultinomialText0.setNormalizeDocLength(true);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertTrue(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(5721.639, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
  }

  /**
  //Test case number: 14
  /*Coverage entropy=3.2536462828828747
  */
  @Test(timeout = 4000)
  public void test14()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertNotNull(naiveBayesMultinomialText0);
      
      naiveBayesMultinomialText0.m_wordsPerClass = null;
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      
      NGramTokenizer nGramTokenizer0 = new NGramTokenizer();
      assertEquals(3, nGramTokenizer0.getNGramMaxSize());
      assertEquals(" \r\n\t.,;:'\"()?!", nGramTokenizer0.getDelimiters());
      assertEquals("The max N of the NGram.", nGramTokenizer0.NGramMaxSizeTipText());
      assertEquals("Set of delimiter characters to use in tokenizing (\\r, \\n and \\t can be used for carriage-return, line-feed and tab)", nGramTokenizer0.delimitersTipText());
      assertEquals("Splits a string into an n-gram with min and max grams.", nGramTokenizer0.globalInfo());
      assertFalse(nGramTokenizer0.hasMoreElements());
      assertEquals(1, nGramTokenizer0.getNGramMinSize());
      assertEquals("The min N of the NGram.", nGramTokenizer0.NGramMinSizeTipText());
      assertNotNull(nGramTokenizer0);
      
      naiveBayesMultinomialText0.setTokenizer(nGramTokenizer0);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(3, nGramTokenizer0.getNGramMaxSize());
      assertEquals(" \r\n\t.,;:'\"()?!", nGramTokenizer0.getDelimiters());
      assertEquals("The max N of the NGram.", nGramTokenizer0.NGramMaxSizeTipText());
      assertEquals("Set of delimiter characters to use in tokenizing (\\r, \\n and \\t can be used for carriage-return, line-feed and tab)", nGramTokenizer0.delimitersTipText());
      assertEquals("Splits a string into an n-gram with min and max grams.", nGramTokenizer0.globalInfo());
      assertFalse(nGramTokenizer0.hasMoreElements());
      assertEquals(1, nGramTokenizer0.getNGramMinSize());
      assertEquals("The min N of the NGram.", nGramTokenizer0.NGramMinSizeTipText());
      
      double[] doubleArray0 = new double[4];
      String[] stringArray0 = naiveBayesMultinomialText0.getOptions();
      assertEquals(12, stringArray0.length);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertNotNull(stringArray0);
      
      doubleArray0[2] = 2470.19;
      String string0 = naiveBayesMultinomialText0.getRevision();
      assertEquals("9122", string0);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertNotNull(string0);
      
      naiveBayesMultinomialText0.m_norm = (-1264.0);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals((-1264.0), naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      
      doubleArray0[1] = 1.0E10;
      naiveBayesMultinomialText0.setUseStopList(true);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals((-1264.0), naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertTrue(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      
      naiveBayesMultinomialText0.setOptions(stringArray0);
      assertEquals(12, stringArray0.length);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
  }

  /**
  //Test case number: 15
  /*Coverage entropy=2.821563807860364
  */
  @Test(timeout = 4000)
  public void test15()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertNotNull(naiveBayesMultinomialText0);
      
      PrecomputedKernelMatrixKernel precomputedKernelMatrixKernel0 = new PrecomputedKernelMatrixKernel();
      assertEquals("Turns time-consuming checks off - use with caution.", precomputedKernelMatrixKernel0.checksTurnedOffTipText());
      assertEquals(0, precomputedKernelMatrixKernel0.numCacheHits());
      assertEquals("This kernel is based on a static kernel matrix that is read from a file. Instances must have a single nominal attribute (excluding the class). This attribute must be the first attribute in the file and its values are used to reference rows/columns in the kernel matrix. The second attribute must be the class attribute.", precomputedKernelMatrixKernel0.globalInfo());
      assertFalse(precomputedKernelMatrixKernel0.getChecksTurnedOff());
      assertEquals(0, precomputedKernelMatrixKernel0.numEvals());
      assertEquals("Turns on the output of debugging information.", precomputedKernelMatrixKernel0.debugTipText());
      assertFalse(precomputedKernelMatrixKernel0.getDebug());
      assertEquals("The file holding the kernel matrix.", precomputedKernelMatrixKernel0.kernelMatrixFileTipText());
      assertNotNull(precomputedKernelMatrixKernel0);
      
      precomputedKernelMatrixKernel0.setChecksTurnedOff(false);
      assertEquals("Turns time-consuming checks off - use with caution.", precomputedKernelMatrixKernel0.checksTurnedOffTipText());
      assertEquals(0, precomputedKernelMatrixKernel0.numCacheHits());
      assertEquals("This kernel is based on a static kernel matrix that is read from a file. Instances must have a single nominal attribute (excluding the class). This attribute must be the first attribute in the file and its values are used to reference rows/columns in the kernel matrix. The second attribute must be the class attribute.", precomputedKernelMatrixKernel0.globalInfo());
      assertFalse(precomputedKernelMatrixKernel0.getChecksTurnedOff());
      assertEquals(0, precomputedKernelMatrixKernel0.numEvals());
      assertEquals("Turns on the output of debugging information.", precomputedKernelMatrixKernel0.debugTipText());
      assertFalse(precomputedKernelMatrixKernel0.getDebug());
      assertEquals("The file holding the kernel matrix.", precomputedKernelMatrixKernel0.kernelMatrixFileTipText());
      
      IteratedLovinsStemmer iteratedLovinsStemmer0 = new IteratedLovinsStemmer();
      assertNotNull(iteratedLovinsStemmer0);
      
      naiveBayesMultinomialText0.setStemmer(iteratedLovinsStemmer0);
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      
      precomputedKernelMatrixKernel0.setDebug(true);
      assertEquals("Turns time-consuming checks off - use with caution.", precomputedKernelMatrixKernel0.checksTurnedOffTipText());
      assertTrue(precomputedKernelMatrixKernel0.getDebug());
      assertEquals(0, precomputedKernelMatrixKernel0.numCacheHits());
      assertEquals("This kernel is based on a static kernel matrix that is read from a file. Instances must have a single nominal attribute (excluding the class). This attribute must be the first attribute in the file and its values are used to reference rows/columns in the kernel matrix. The second attribute must be the class attribute.", precomputedKernelMatrixKernel0.globalInfo());
      assertFalse(precomputedKernelMatrixKernel0.getChecksTurnedOff());
      assertEquals(0, precomputedKernelMatrixKernel0.numEvals());
      assertEquals("Turns on the output of debugging information.", precomputedKernelMatrixKernel0.debugTipText());
      assertEquals("The file holding the kernel matrix.", precomputedKernelMatrixKernel0.kernelMatrixFileTipText());
      
      File file0 = precomputedKernelMatrixKernel0.getKernelMatrixFile();
      assertEquals("Turns time-consuming checks off - use with caution.", precomputedKernelMatrixKernel0.checksTurnedOffTipText());
      assertTrue(precomputedKernelMatrixKernel0.getDebug());
      assertEquals(0, precomputedKernelMatrixKernel0.numCacheHits());
      assertEquals("This kernel is based on a static kernel matrix that is read from a file. Instances must have a single nominal attribute (excluding the class). This attribute must be the first attribute in the file and its values are used to reference rows/columns in the kernel matrix. The second attribute must be the class attribute.", precomputedKernelMatrixKernel0.globalInfo());
      assertFalse(precomputedKernelMatrixKernel0.getChecksTurnedOff());
      assertEquals(0, precomputedKernelMatrixKernel0.numEvals());
      assertEquals("Turns on the output of debugging information.", precomputedKernelMatrixKernel0.debugTipText());
      assertEquals("The file holding the kernel matrix.", precomputedKernelMatrixKernel0.kernelMatrixFileTipText());
      assertFalse(file0.canWrite());
      assertTrue(file0.isFile());
      assertFalse(file0.canRead());
      assertEquals(0L, file0.lastModified());
      assertFalse(file0.isAbsolute());
      assertEquals(0L, file0.getUsableSpace());
      assertEquals(0L, file0.getFreeSpace());
      assertEquals("kernelMatrix.matrix", file0.getName());
      assertFalse(file0.isHidden());
      assertNull(file0.getParent());
      assertFalse(file0.exists());
      assertFalse(file0.isDirectory());
      assertEquals("kernelMatrix.matrix", file0.toString());
      assertEquals(0L, file0.length());
      assertFalse(file0.canExecute());
      assertEquals(0L, file0.getTotalSpace());
      assertNotNull(file0);
      
      naiveBayesMultinomialText0.setStopwords(file0);
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Turns time-consuming checks off - use with caution.", precomputedKernelMatrixKernel0.checksTurnedOffTipText());
      assertTrue(precomputedKernelMatrixKernel0.getDebug());
      assertEquals(0, precomputedKernelMatrixKernel0.numCacheHits());
      assertEquals("This kernel is based on a static kernel matrix that is read from a file. Instances must have a single nominal attribute (excluding the class). This attribute must be the first attribute in the file and its values are used to reference rows/columns in the kernel matrix. The second attribute must be the class attribute.", precomputedKernelMatrixKernel0.globalInfo());
      assertFalse(precomputedKernelMatrixKernel0.getChecksTurnedOff());
      assertEquals(0, precomputedKernelMatrixKernel0.numEvals());
      assertEquals("Turns on the output of debugging information.", precomputedKernelMatrixKernel0.debugTipText());
      assertEquals("The file holding the kernel matrix.", precomputedKernelMatrixKernel0.kernelMatrixFileTipText());
      assertFalse(file0.canWrite());
      assertTrue(file0.isFile());
      assertFalse(file0.canRead());
      assertEquals(0L, file0.lastModified());
      assertFalse(file0.isAbsolute());
      assertEquals(0L, file0.getUsableSpace());
      assertEquals(0L, file0.getFreeSpace());
      assertEquals("kernelMatrix.matrix", file0.getName());
      assertFalse(file0.isHidden());
      assertNull(file0.getParent());
      assertFalse(file0.exists());
      assertFalse(file0.isDirectory());
      assertEquals("kernelMatrix.matrix", file0.toString());
      assertEquals(0L, file0.length());
      assertFalse(file0.canExecute());
      assertEquals(0L, file0.getTotalSpace());
      
      naiveBayesMultinomialText0.m_normalize = true;
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertTrue(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      
      naiveBayesMultinomialText0.m_lowercaseTokens = true;
      assertTrue(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertTrue(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      
      Capabilities capabilities0 = naiveBayesMultinomialText0.getCapabilities();
      assertTrue(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertTrue(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(capabilities0.hasDependencies());
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      assertNotNull(capabilities0);
      
      naiveBayesMultinomialText0.setMinWordFrequency(4.94E-321);
      assertTrue(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertTrue(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(4.94E-321, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      
      naiveBayesMultinomialText0.pruneDictionary();
      assertTrue(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertTrue(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(4.94E-321, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      
      String[] stringArray0 = naiveBayesMultinomialText0.getOptions();
      assertEquals(16, stringArray0.length);
      assertTrue(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertTrue(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(4.94E-321, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertNotNull(stringArray0);
  }

  /**
  //Test case number: 16
  /*Coverage entropy=1.2424533248940002
  */
  @Test(timeout = 4000)
  public void test16()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertNotNull(naiveBayesMultinomialText0);
      
      double[] doubleArray0 = new double[5];
      doubleArray0[0] = 1953.31;
      doubleArray0[1] = 1953.31;
      doubleArray0[2] = 1953.31;
      doubleArray0[3] = 1953.31;
      doubleArray0[4] = (-2175.59888);
      int[] intArray0 = new int[7];
      intArray0[0] = 1181;
      intArray0[1] = (-649);
      intArray0[2] = 0;
      intArray0[3] = 1;
      intArray0[4] = (-2315);
      intArray0[5] = 22;
      intArray0[6] = 117;
      SparseInstance sparseInstance0 = new SparseInstance(1953.31, doubleArray0, intArray0, (-803));
      assertArrayEquals(new double[] {1953.31, 1953.31, 1953.31, 1953.31, (-2175.59888)}, doubleArray0, 0.01);
      assertArrayEquals(new int[] {1181, (-649), 0, 1, (-2315), 22, 117}, intArray0);
      assertEquals(5, doubleArray0.length);
      assertEquals(7, intArray0.length);
      assertEquals(7, sparseInstance0.numValues());
      assertEquals((-803), sparseInstance0.numAttributes());
      assertEquals(1953.31, sparseInstance0.weight(), 0.01);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertNotNull(sparseInstance0);
      
      File file0 = MockFile.createTempFile("t;[", "$!p$&{)");
      assertEquals(1392409281320L, file0.lastModified());
      assertEquals(0L, file0.length());
      assertEquals(0L, file0.getFreeSpace());
      assertEquals("/tmp/t;[0$!p$&{)", file0.toString());
      assertTrue(file0.exists());
      assertEquals(0L, file0.getTotalSpace());
      assertTrue(file0.isAbsolute());
      assertTrue(file0.canExecute());
      assertFalse(file0.isDirectory());
      assertEquals(0L, file0.getUsableSpace());
      assertEquals("t;[0$!p$&{)", file0.getName());
      assertTrue(file0.canWrite());
      assertFalse(file0.isHidden());
      assertTrue(file0.canRead());
      assertEquals("/tmp", file0.getParent());
      assertTrue(file0.isFile());
      assertNotNull(file0);
      
      naiveBayesMultinomialText0.setStopwords(file0);
      assertEquals(1392409281320L, file0.lastModified());
      assertEquals(0L, file0.length());
      assertEquals(0L, file0.getFreeSpace());
      assertEquals("/tmp/t;[0$!p$&{)", file0.toString());
      assertTrue(file0.exists());
      assertEquals(0L, file0.getTotalSpace());
      assertTrue(file0.isAbsolute());
      assertTrue(file0.canExecute());
      assertFalse(file0.isDirectory());
      assertEquals(0L, file0.getUsableSpace());
      assertEquals("t;[0$!p$&{)", file0.getName());
      assertTrue(file0.canWrite());
      assertFalse(file0.isHidden());
      assertTrue(file0.canRead());
      assertEquals("/tmp", file0.getParent());
      assertTrue(file0.isFile());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertTrue(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      
      NaiveBayesMultinomialText naiveBayesMultinomialText1 = new NaiveBayesMultinomialText();
      assertFalse(naiveBayesMultinomialText1.equals((Object)naiveBayesMultinomialText0));
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText1.tokenizerTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText1.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText1.getNormalizeDocLength());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText1.lowercaseTokensTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText1.useWordFrequenciesTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText1.useStopListTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText1.stemmerTipText());
      assertFalse(naiveBayesMultinomialText1.getLowercaseTokens());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText1.minWordFrequencyTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText1.LNormTipText());
      assertEquals(3.0, naiveBayesMultinomialText1.getMinWordFrequency(), 0.01);
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText1.globalInfo());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText1.periodicPruningTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText1.normTipText());
      assertFalse(naiveBayesMultinomialText1.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText1.getDebug());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText1.debugTipText());
      assertEquals(0, naiveBayesMultinomialText1.getPeriodicPruning());
      assertEquals(1.0, naiveBayesMultinomialText1.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText1.getUseStopList());
      assertEquals(2.0, naiveBayesMultinomialText1.getLNorm(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText1.stopwordsTipText());
      assertNotNull(naiveBayesMultinomialText1);
      
      Capabilities capabilities0 = naiveBayesMultinomialText1.getCapabilities();
      assertNotSame(naiveBayesMultinomialText1, naiveBayesMultinomialText0);
      assertFalse(naiveBayesMultinomialText1.equals((Object)naiveBayesMultinomialText0));
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText1.tokenizerTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText1.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText1.getNormalizeDocLength());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText1.lowercaseTokensTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText1.useWordFrequenciesTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText1.useStopListTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText1.stemmerTipText());
      assertFalse(naiveBayesMultinomialText1.getLowercaseTokens());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText1.minWordFrequencyTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText1.LNormTipText());
      assertEquals(3.0, naiveBayesMultinomialText1.getMinWordFrequency(), 0.01);
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText1.globalInfo());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText1.periodicPruningTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText1.normTipText());
      assertFalse(naiveBayesMultinomialText1.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText1.getDebug());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText1.debugTipText());
      assertEquals(0, naiveBayesMultinomialText1.getPeriodicPruning());
      assertEquals(1.0, naiveBayesMultinomialText1.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText1.getUseStopList());
      assertEquals(2.0, naiveBayesMultinomialText1.getLNorm(), 0.01);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText1.stopwordsTipText());
      assertEquals(0, capabilities0.getMinimumNumberInstances());
      assertFalse(capabilities0.hasDependencies());
      assertNotNull(capabilities0);
  }

  /**
  //Test case number: 17
  /*Coverage entropy=3.0910424533583143
  */
  @Test(timeout = 4000)
  public void test17()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertNotNull(naiveBayesMultinomialText0);
      
      Enumeration<Option> enumeration0 = naiveBayesMultinomialText0.listOptions();
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertNotNull(enumeration0);
      
      String string0 = naiveBayesMultinomialText0.toString();
      assertEquals("NaiveBayesMultinomialText: No model built yet.\n", string0);
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertNotNull(string0);
      
      naiveBayesMultinomialText0.m_probOfWordGivenClass = null;
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      
      naiveBayesMultinomialText0.m_probOfWordGivenClass = null;
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      
      SparseInstance sparseInstance0 = new SparseInstance(1270);
      assertEquals(1270, sparseInstance0.numValues());
      assertEquals(1270, sparseInstance0.numAttributes());
      assertEquals(1.0, sparseInstance0.weight(), 0.01);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertNotNull(sparseInstance0);
      
      IBk iBk0 = new IBk(1270);
      assertFalse(iBk0.getDebug());
      assertEquals("Whether hold-one-out cross-validation will be used to select the best k value.", iBk0.crossValidateTipText());
      assertEquals(0, iBk0.getWindowSize());
      assertEquals("Whether the mean squared error is used rather than mean absolute error when doing cross-validation for regression problems.", iBk0.meanSquaredTipText());
      assertEquals(1270, iBk0.getKNN());
      assertEquals("The nearest neighbour search algorithm to use (Default: weka.core.neighboursearch.LinearNNSearch).", iBk0.nearestNeighbourSearchAlgorithmTipText());
      assertEquals("Gets the maximum number of instances allowed in the training pool. The addition of new instances above this value will result in old instances being removed. A value of 0 signifies no limit to the number of training instances.", iBk0.windowSizeTipText());
      assertEquals("The number of neighbours to use.", iBk0.KNNTipText());
      assertFalse(iBk0.getMeanSquared());
      assertEquals("Gets the distance weighting method used.", iBk0.distanceWeightingTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", iBk0.debugTipText());
      assertFalse(iBk0.getCrossValidate());
      assertEquals(2, IBk.WEIGHT_INVERSE);
      assertEquals(1, IBk.WEIGHT_NONE);
      assertEquals(4, IBk.WEIGHT_SIMILARITY);
      assertNotNull(iBk0);
      
      TestInstances testInstances0 = new TestInstances();
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getSeed());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(0, testInstances0.getNumString());
      assertFalse(testInstances0.getNoClass());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertNotNull(testInstances0);
      
      Instances instances0 = testInstances0.generate();
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getSeed());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(0, testInstances0.getNumString());
      assertFalse(testInstances0.getNoClass());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(2, instances0.numAttributes());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(20, instances0.size());
      assertEquals(1, instances0.classIndex());
      assertEquals(20, instances0.numInstances());
      assertEquals(2, instances0.numClasses());
      assertEquals("Testdata", instances0.relationName());
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertNotNull(instances0);
      
      double[] doubleArray0 = new double[9];
      doubleArray0[0] = (double) 1270;
      doubleArray0[1] = (double) 1;
      doubleArray0[2] = (double) 1;
      doubleArray0[3] = (double) 1;
      doubleArray0[4] = (double) (-1);
      doubleArray0[5] = (double) (-302);
      boolean boolean0 = instances0.add((Instance) sparseInstance0);
      assertTrue(boolean0);
      assertEquals(1270, sparseInstance0.numValues());
      assertEquals(1270, sparseInstance0.numAttributes());
      assertEquals(1.0, sparseInstance0.weight(), 0.01);
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getSeed());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(0, testInstances0.getNumString());
      assertFalse(testInstances0.getNoClass());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(21.0, instances0.sumOfWeights(), 0.01);
      assertEquals(2, instances0.numAttributes());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(21, instances0.size());
      assertEquals(21, instances0.numInstances());
      assertEquals(1, instances0.classIndex());
      assertEquals(2, instances0.numClasses());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      
      doubleArray0[6] = (double) 4;
      doubleArray0[7] = (double) 1;
      doubleArray0[8] = (double) (-2);
      Instances instances1 = iBk0.pruneToK(instances0, doubleArray0, (-1));
      assertArrayEquals(new double[] {1270.0, 1.0, 1.0, 1.0, (-1.0), (-302.0), 4.0, 1.0, (-2.0)}, doubleArray0, 0.01);
      assertEquals(9, doubleArray0.length);
      assertNotSame(instances1, instances0);
      assertNotSame(instances0, instances1);
      assertFalse(instances1.equals((Object)instances0));
      assertEquals(1.0, instances1.sumOfWeights(), 0.01);
      assertEquals("Testdata", instances1.relationName());
      assertEquals(2, instances1.numAttributes());
      assertEquals(1, instances1.size());
      assertEquals(1, instances1.numInstances());
      assertEquals(2, instances1.numClasses());
      assertFalse(instances1.checkForStringAttributes());
      assertEquals(1, instances1.classIndex());
      assertFalse(iBk0.getDebug());
      assertEquals("Whether hold-one-out cross-validation will be used to select the best k value.", iBk0.crossValidateTipText());
      assertEquals(0, iBk0.getWindowSize());
      assertEquals("Whether the mean squared error is used rather than mean absolute error when doing cross-validation for regression problems.", iBk0.meanSquaredTipText());
      assertEquals(1270, iBk0.getKNN());
      assertEquals("The nearest neighbour search algorithm to use (Default: weka.core.neighboursearch.LinearNNSearch).", iBk0.nearestNeighbourSearchAlgorithmTipText());
      assertEquals("Gets the maximum number of instances allowed in the training pool. The addition of new instances above this value will result in old instances being removed. A value of 0 signifies no limit to the number of training instances.", iBk0.windowSizeTipText());
      assertEquals("The number of neighbours to use.", iBk0.KNNTipText());
      assertFalse(iBk0.getMeanSquared());
      assertEquals("Gets the distance weighting method used.", iBk0.distanceWeightingTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", iBk0.debugTipText());
      assertFalse(iBk0.getCrossValidate());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getSeed());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(0, testInstances0.getNumString());
      assertFalse(testInstances0.getNoClass());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(21.0, instances0.sumOfWeights(), 0.01);
      assertEquals(2, instances0.numAttributes());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(21, instances0.size());
      assertEquals(21, instances0.numInstances());
      assertEquals(1, instances0.classIndex());
      assertEquals(2, instances0.numClasses());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(2, IBk.WEIGHT_INVERSE);
      assertEquals(1, IBk.WEIGHT_NONE);
      assertEquals(4, IBk.WEIGHT_SIMILARITY);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertNotNull(instances1);
      
      naiveBayesMultinomialText0.buildClassifier(instances0);
      assertNotSame(instances0, instances1);
      assertFalse(instances0.equals((Object)instances1));
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getSeed());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(0, testInstances0.getNumString());
      assertFalse(testInstances0.getNoClass());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(21.0, instances0.sumOfWeights(), 0.01);
      assertEquals(2, instances0.numAttributes());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(21, instances0.size());
      assertEquals(21, instances0.numInstances());
      assertEquals(1, instances0.classIndex());
      assertEquals(2, instances0.numClasses());
      assertEquals("Testdata", instances0.relationName());
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      
      String[] stringArray0 = naiveBayesMultinomialText0.getOptions();
      assertEquals(12, stringArray0.length);
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertNotNull(stringArray0);
  }

  /**
  //Test case number: 18
  /*Coverage entropy=2.6797988296177926
  */
  @Test(timeout = 4000)
  public void test18()  throws Throwable  {
      FileSystemHandling fileSystemHandling0 = new FileSystemHandling();
      assertNotNull(fileSystemHandling0);
      
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertNotNull(naiveBayesMultinomialText0);
      
      naiveBayesMultinomialText0.m_wordFrequencies = true;
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertTrue(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      
      String[] stringArray0 = naiveBayesMultinomialText0.getOptions();
      assertEquals(13, stringArray0.length);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertTrue(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertNotNull(stringArray0);
      
      naiveBayesMultinomialText0.m_lowercaseTokens = false;
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertTrue(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      
      naiveBayesMultinomialText0.pruneDictionary();
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertTrue(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      
      System.setCurrentTimeMillis(1L);
      naiveBayesMultinomialText0.setUseStopList(true);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertTrue(naiveBayesMultinomialText0.getUseStopList());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertTrue(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      
      naiveBayesMultinomialText0.setOptions(stringArray0);
      assertEquals(13, stringArray0.length);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertTrue(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      
      naiveBayesMultinomialText0.setOptions(stringArray0);
      assertEquals(13, stringArray0.length);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      
      System.setCurrentTimeMillis(1L);
      naiveBayesMultinomialText0.pruneDictionary();
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
  }

  /**
  //Test case number: 19
  /*Coverage entropy=2.7184734013300127
  */
  @Test(timeout = 4000)
  public void test19()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertNotNull(naiveBayesMultinomialText0);
      
      Enumeration<Option> enumeration0 = naiveBayesMultinomialText0.listOptions();
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertNotNull(enumeration0);
      
      String string0 = naiveBayesMultinomialText0.toString();
      assertEquals("NaiveBayesMultinomialText: No model built yet.\n", string0);
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertNotNull(string0);
      
      naiveBayesMultinomialText0.m_probOfWordGivenClass = null;
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      
      naiveBayesMultinomialText0.m_probOfWordGivenClass = null;
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      
      SparseInstance sparseInstance0 = new SparseInstance(1270);
      assertEquals(1.0, sparseInstance0.weight(), 0.01);
      assertEquals(1270, sparseInstance0.numValues());
      assertEquals(1270, sparseInstance0.numAttributes());
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertNotNull(sparseInstance0);
      
      IBk iBk0 = new IBk(1270);
      assertEquals("Whether hold-one-out cross-validation will be used to select the best k value.", iBk0.crossValidateTipText());
      assertFalse(iBk0.getDebug());
      assertEquals(0, iBk0.getWindowSize());
      assertEquals("If set to true, classifier may output additional info to the console.", iBk0.debugTipText());
      assertFalse(iBk0.getCrossValidate());
      assertEquals("Gets the distance weighting method used.", iBk0.distanceWeightingTipText());
      assertFalse(iBk0.getMeanSquared());
      assertEquals(1270, iBk0.getKNN());
      assertEquals("Gets the maximum number of instances allowed in the training pool. The addition of new instances above this value will result in old instances being removed. A value of 0 signifies no limit to the number of training instances.", iBk0.windowSizeTipText());
      assertEquals("The nearest neighbour search algorithm to use (Default: weka.core.neighboursearch.LinearNNSearch).", iBk0.nearestNeighbourSearchAlgorithmTipText());
      assertEquals("The number of neighbours to use.", iBk0.KNNTipText());
      assertEquals("Whether the mean squared error is used rather than mean absolute error when doing cross-validation for regression problems.", iBk0.meanSquaredTipText());
      assertEquals(2, IBk.WEIGHT_INVERSE);
      assertEquals(1, IBk.WEIGHT_NONE);
      assertEquals(4, IBk.WEIGHT_SIMILARITY);
      assertNotNull(iBk0);
      
      TestInstances testInstances0 = new TestInstances();
      assertFalse(testInstances0.getNoClass());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(1, testInstances0.getSeed());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(1, testInstances0.getClassType());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertNotNull(testInstances0);
      
      Instances instances0 = testInstances0.generate();
      assertFalse(testInstances0.getNoClass());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(1, testInstances0.getSeed());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(1, testInstances0.getClassType());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(2, instances0.numAttributes());
      assertEquals(2, instances0.numClasses());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(1, instances0.classIndex());
      assertEquals(20, instances0.size());
      assertEquals(20, instances0.numInstances());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertNotNull(instances0);
      
      naiveBayesMultinomialText0.setUseStopList(true);
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertTrue(naiveBayesMultinomialText0.getUseStopList());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      
      Capabilities capabilities0 = Capabilities.forInstances(instances0);
      assertFalse(testInstances0.getNoClass());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(1, testInstances0.getSeed());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(1, testInstances0.getClassType());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(2, instances0.numAttributes());
      assertEquals(2, instances0.numClasses());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(1, instances0.classIndex());
      assertEquals(20, instances0.size());
      assertEquals(20, instances0.numInstances());
      assertFalse(instances0.checkForStringAttributes());
      assertFalse(capabilities0.hasDependencies());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertNotNull(capabilities0);
      
      naiveBayesMultinomialText0.pruneDictionary();
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertTrue(naiveBayesMultinomialText0.getUseStopList());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      
      String string1 = naiveBayesMultinomialText0.toString();
      assertEquals("NaiveBayesMultinomialText: No model built yet.\n", string1);
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertTrue(naiveBayesMultinomialText0.getUseStopList());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertNotNull(string1);
      
      String[] stringArray0 = naiveBayesMultinomialText0.getOptions();
      assertEquals(13, stringArray0.length);
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertTrue(naiveBayesMultinomialText0.getUseStopList());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertNotNull(stringArray0);
  }

  /**
  //Test case number: 20
  /*Coverage entropy=2.274240930731769
  */
  @Test(timeout = 4000)
  public void test20()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertNotNull(naiveBayesMultinomialText0);
      
      Enumeration<Option> enumeration0 = naiveBayesMultinomialText0.listOptions();
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertNotNull(enumeration0);
      
      String string0 = naiveBayesMultinomialText0.toString();
      assertEquals("NaiveBayesMultinomialText: No model built yet.\n", string0);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertNotNull(string0);
      
      naiveBayesMultinomialText0.m_probOfWordGivenClass = null;
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      
      SparseInstance sparseInstance0 = new SparseInstance(1270);
      assertEquals(1.0, sparseInstance0.weight(), 0.01);
      assertEquals(1270, sparseInstance0.numValues());
      assertEquals(1270, sparseInstance0.numAttributes());
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertNotNull(sparseInstance0);
      
      IBk iBk0 = new IBk(1270);
      assertEquals("Whether hold-one-out cross-validation will be used to select the best k value.", iBk0.crossValidateTipText());
      assertFalse(iBk0.getDebug());
      assertEquals(0, iBk0.getWindowSize());
      assertEquals("If set to true, classifier may output additional info to the console.", iBk0.debugTipText());
      assertEquals("The nearest neighbour search algorithm to use (Default: weka.core.neighboursearch.LinearNNSearch).", iBk0.nearestNeighbourSearchAlgorithmTipText());
      assertEquals("Whether the mean squared error is used rather than mean absolute error when doing cross-validation for regression problems.", iBk0.meanSquaredTipText());
      assertFalse(iBk0.getMeanSquared());
      assertEquals("The number of neighbours to use.", iBk0.KNNTipText());
      assertEquals(1270, iBk0.getKNN());
      assertEquals("Gets the maximum number of instances allowed in the training pool. The addition of new instances above this value will result in old instances being removed. A value of 0 signifies no limit to the number of training instances.", iBk0.windowSizeTipText());
      assertFalse(iBk0.getCrossValidate());
      assertEquals("Gets the distance weighting method used.", iBk0.distanceWeightingTipText());
      assertEquals(1, IBk.WEIGHT_NONE);
      assertEquals(2, IBk.WEIGHT_INVERSE);
      assertEquals(4, IBk.WEIGHT_SIMILARITY);
      assertNotNull(iBk0);
      
      TestInstances testInstances0 = new TestInstances();
      assertEquals(0, testInstances0.getNumString());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals((-1), testInstances0.getClassIndex());
      assertFalse(testInstances0.getNoClass());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(1, testInstances0.getSeed());
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertNotNull(testInstances0);
      
      Instances instances0 = testInstances0.generate();
      assertEquals(0, testInstances0.getNumString());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals((-1), testInstances0.getClassIndex());
      assertFalse(testInstances0.getNoClass());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(2, instances0.numAttributes());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(20, instances0.size());
      assertEquals(20, instances0.numInstances());
      assertEquals(2, instances0.numClasses());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(1, instances0.classIndex());
      assertEquals("Testdata", instances0.relationName());
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertNotNull(instances0);
      
      naiveBayesMultinomialText0.setUseStopList(true);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertTrue(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      
      Capabilities capabilities0 = Capabilities.forInstances(instances0);
      assertEquals(0, testInstances0.getNumString());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals((-1), testInstances0.getClassIndex());
      assertFalse(testInstances0.getNoClass());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(2, instances0.numAttributes());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(20, instances0.size());
      assertEquals(20, instances0.numInstances());
      assertEquals(2, instances0.numClasses());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(1, instances0.classIndex());
      assertEquals("Testdata", instances0.relationName());
      assertFalse(capabilities0.hasDependencies());
      assertEquals(1, capabilities0.getMinimumNumberInstances());
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertNotNull(capabilities0);
      
      Consumer<Object> consumer0 = (Consumer<Object>) mock(Consumer.class, new ViolatedAssumptionAnswer());
      instances0.forEach(consumer0);
      assertEquals(0, testInstances0.getNumString());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals((-1), testInstances0.getClassIndex());
      assertFalse(testInstances0.getNoClass());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(2, instances0.numAttributes());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(20, instances0.size());
      assertEquals(20, instances0.numInstances());
      assertEquals(2, instances0.numClasses());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(1, instances0.classIndex());
      assertEquals("Testdata", instances0.relationName());
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      
      naiveBayesMultinomialText0.buildClassifier(instances0);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertTrue(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals((-1), testInstances0.getClassIndex());
      assertFalse(testInstances0.getNoClass());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(2, instances0.numAttributes());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(20, instances0.size());
      assertEquals(20, instances0.numInstances());
      assertEquals(2, instances0.numClasses());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(1, instances0.classIndex());
      assertEquals("Testdata", instances0.relationName());
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      
      double[] doubleArray0 = naiveBayesMultinomialText0.m_probOfClass;
      assertArrayEquals(new double[] {12.0, 10.0}, doubleArray0, 0.01);
      assertEquals(2, doubleArray0.length);
      assertNotNull(doubleArray0);
  }

  /**
  //Test case number: 21
  /*Coverage entropy=3.46118129388929
  */
  @Test(timeout = 4000)
  public void test21()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertNotNull(naiveBayesMultinomialText0);
      
      TestInstances testInstances0 = new TestInstances();
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(1, testInstances0.getClassType());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(0, testInstances0.getNumDate());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(20, testInstances0.getNumInstances());
      assertFalse(testInstances0.getNoClass());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertNotNull(testInstances0);
      
      Instances instances0 = testInstances0.generate();
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(1, testInstances0.getClassType());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(0, testInstances0.getNumDate());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(20, testInstances0.getNumInstances());
      assertFalse(testInstances0.getNoClass());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(2, instances0.numAttributes());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20, instances0.numInstances());
      assertEquals(20, instances0.size());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(1, instances0.classIndex());
      assertEquals(2, instances0.numClasses());
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertNotNull(instances0);
      
      int[] intArray0 = new int[6];
      intArray0[0] = (-2);
      intArray0[1] = (-1);
      intArray0[2] = (-2);
      intArray0[3] = (-2);
      intArray0[4] = (-2);
      intArray0[5] = (-1);
      BinarySparseInstance binarySparseInstance0 = new BinarySparseInstance(426.3, intArray0, (-2));
      assertArrayEquals(new int[] {(-2), (-1), (-2), (-2), (-2), (-1)}, intArray0);
      assertEquals(6, intArray0.length);
      assertEquals(426.3, binarySparseInstance0.weight(), 0.01);
      assertEquals(6, binarySparseInstance0.numValues());
      assertEquals((-2), binarySparseInstance0.numAttributes());
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertNotNull(binarySparseInstance0);
      
      boolean boolean0 = instances0.add((Instance) binarySparseInstance0);
      assertTrue(boolean0);
      assertArrayEquals(new int[] {(-2), (-1), (-2), (-2), (-2), (-1)}, intArray0);
      assertEquals(6, intArray0.length);
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(1, testInstances0.getClassType());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(0, testInstances0.getNumDate());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(20, testInstances0.getNumInstances());
      assertFalse(testInstances0.getNoClass());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(2, instances0.numAttributes());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(446.3, instances0.sumOfWeights(), 0.01);
      assertEquals(21, instances0.numInstances());
      assertEquals(21, instances0.size());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(1, instances0.classIndex());
      assertEquals(2, instances0.numClasses());
      assertEquals(426.3, binarySparseInstance0.weight(), 0.01);
      assertEquals(6, binarySparseInstance0.numValues());
      assertEquals((-2), binarySparseInstance0.numAttributes());
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      
      IBk iBk0 = new IBk((-2));
      assertEquals("The number of neighbours to use.", iBk0.KNNTipText());
      assertEquals("Gets the distance weighting method used.", iBk0.distanceWeightingTipText());
      assertEquals("Whether the mean squared error is used rather than mean absolute error when doing cross-validation for regression problems.", iBk0.meanSquaredTipText());
      assertEquals((-2), iBk0.getKNN());
      assertEquals("If set to true, classifier may output additional info to the console.", iBk0.debugTipText());
      assertFalse(iBk0.getCrossValidate());
      assertEquals("The nearest neighbour search algorithm to use (Default: weka.core.neighboursearch.LinearNNSearch).", iBk0.nearestNeighbourSearchAlgorithmTipText());
      assertEquals("Whether hold-one-out cross-validation will be used to select the best k value.", iBk0.crossValidateTipText());
      assertFalse(iBk0.getMeanSquared());
      assertEquals(0, iBk0.getWindowSize());
      assertFalse(iBk0.getDebug());
      assertEquals("Gets the maximum number of instances allowed in the training pool. The addition of new instances above this value will result in old instances being removed. A value of 0 signifies no limit to the number of training instances.", iBk0.windowSizeTipText());
      assertEquals(1, IBk.WEIGHT_NONE);
      assertEquals(4, IBk.WEIGHT_SIMILARITY);
      assertEquals(2, IBk.WEIGHT_INVERSE);
      assertNotNull(iBk0);
      
      double[] doubleArray0 = new double[8];
      doubleArray0[0] = (double) (-2);
      doubleArray0[1] = (double) 2;
      doubleArray0[2] = 0.0;
      doubleArray0[3] = (double) (-2);
      doubleArray0[4] = (double) (-1);
      doubleArray0[5] = (double) 4;
      doubleArray0[6] = (double) (-2);
      doubleArray0[7] = 0.0;
      Instances instances1 = iBk0.pruneToK(instances0, doubleArray0, (-2));
      assertArrayEquals(new double[] {(-2.0), 2.0, 0.0, (-2.0), (-1.0), 4.0, (-2.0), 0.0}, doubleArray0, 0.01);
      assertEquals(8, doubleArray0.length);
      assertNotSame(instances0, instances1);
      assertNotSame(instances1, instances0);
      assertFalse(instances1.equals((Object)instances0));
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(1, testInstances0.getClassType());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(0, testInstances0.getNumDate());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(20, testInstances0.getNumInstances());
      assertFalse(testInstances0.getNoClass());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(2, instances0.numAttributes());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(446.3, instances0.sumOfWeights(), 0.01);
      assertEquals(21, instances0.numInstances());
      assertEquals(21, instances0.size());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(1, instances0.classIndex());
      assertEquals(2, instances0.numClasses());
      assertEquals(2, instances1.numAttributes());
      assertEquals(2, instances1.numClasses());
      assertEquals(1, instances1.size());
      assertEquals(1.0, instances1.sumOfWeights(), 0.01);
      assertEquals("Testdata", instances1.relationName());
      assertEquals(1, instances1.numInstances());
      assertEquals(1, instances1.classIndex());
      assertFalse(instances1.checkForStringAttributes());
      assertEquals("The number of neighbours to use.", iBk0.KNNTipText());
      assertEquals("Gets the distance weighting method used.", iBk0.distanceWeightingTipText());
      assertEquals("Whether the mean squared error is used rather than mean absolute error when doing cross-validation for regression problems.", iBk0.meanSquaredTipText());
      assertEquals((-2), iBk0.getKNN());
      assertEquals("If set to true, classifier may output additional info to the console.", iBk0.debugTipText());
      assertFalse(iBk0.getCrossValidate());
      assertEquals("The nearest neighbour search algorithm to use (Default: weka.core.neighboursearch.LinearNNSearch).", iBk0.nearestNeighbourSearchAlgorithmTipText());
      assertEquals("Whether hold-one-out cross-validation will be used to select the best k value.", iBk0.crossValidateTipText());
      assertFalse(iBk0.getMeanSquared());
      assertEquals(0, iBk0.getWindowSize());
      assertFalse(iBk0.getDebug());
      assertEquals("Gets the maximum number of instances allowed in the training pool. The addition of new instances above this value will result in old instances being removed. A value of 0 signifies no limit to the number of training instances.", iBk0.windowSizeTipText());
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals(1, IBk.WEIGHT_NONE);
      assertEquals(4, IBk.WEIGHT_SIMILARITY);
      assertEquals(2, IBk.WEIGHT_INVERSE);
      assertNotNull(instances1);
      
      naiveBayesMultinomialText0.buildClassifier(instances0);
      assertNotSame(instances0, instances1);
      assertFalse(instances0.equals((Object)instances1));
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(1, testInstances0.getClassType());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(0, testInstances0.getNumDate());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(20, testInstances0.getNumInstances());
      assertFalse(testInstances0.getNoClass());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(2, instances0.numAttributes());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(446.3, instances0.sumOfWeights(), 0.01);
      assertEquals(21, instances0.numInstances());
      assertEquals(21, instances0.size());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(1, instances0.classIndex());
      assertEquals(2, instances0.numClasses());
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      
      String[] stringArray0 = naiveBayesMultinomialText0.getOptions();
      assertEquals(12, stringArray0.length);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertNotNull(stringArray0);
      
      naiveBayesMultinomialText0.setOptions(stringArray0);
      assertEquals(12, stringArray0.length);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      
      try { 
        naiveBayesMultinomialText0.distributionForInstance(binarySparseInstance0);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("weka.classifiers.bayes.NaiveBayesMultinomialText", e);
      }
  }

  /**
  //Test case number: 22
  /*Coverage entropy=2.70805020110221
  */
  @Test(timeout = 4000)
  public void test22()  throws Throwable  {
      FileSystemHandling fileSystemHandling0 = new FileSystemHandling();
      assertNotNull(fileSystemHandling0);
      
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertNotNull(naiveBayesMultinomialText0);
      
      boolean boolean0 = true;
      naiveBayesMultinomialText0.m_wordFrequencies = true;
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertTrue(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      
      naiveBayesMultinomialText0.m_lowercaseTokens = true;
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertTrue(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertTrue(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      
      naiveBayesMultinomialText0.setPeriodicPruning(1);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertTrue(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals(1, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertTrue(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      
      String[] stringArray0 = naiveBayesMultinomialText0.getOptions();
      assertEquals(14, stringArray0.length);
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertTrue(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals(1, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertTrue(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertNotNull(stringArray0);
      
      naiveBayesMultinomialText0.m_lowercaseTokens = false;
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertTrue(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals(1, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      
      // Undeclared exception!
      try { 
        naiveBayesMultinomialText0.pruneDictionary();
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("weka.classifiers.bayes.NaiveBayesMultinomialText", e);
      }
  }

  /**
  //Test case number: 23
  /*Coverage entropy=3.2065536680867055
  */
  @Test(timeout = 4000)
  public void test23()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertNotNull(naiveBayesMultinomialText0);
      
      TestInstances testInstances0 = new TestInstances();
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(0, testInstances0.getNumString());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(20, testInstances0.getNumInstances());
      assertFalse(testInstances0.getNoClass());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertNotNull(testInstances0);
      
      Instances instances0 = testInstances0.generate();
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(0, testInstances0.getNumString());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(20, testInstances0.getNumInstances());
      assertFalse(testInstances0.getNoClass());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(2, instances0.numAttributes());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(2, instances0.numClasses());
      assertEquals(20, instances0.size());
      assertEquals(1, instances0.classIndex());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20, instances0.numInstances());
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertNotNull(instances0);
      
      int[] intArray0 = new int[6];
      intArray0[0] = (-2);
      intArray0[1] = (-1);
      naiveBayesMultinomialText0.m_normalize = true;
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertTrue(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      
      intArray0[2] = (-2);
      intArray0[3] = (-2);
      intArray0[0] = (-2);
      intArray0[5] = (-1);
      BinarySparseInstance binarySparseInstance0 = new BinarySparseInstance(426.3, intArray0, (-2));
      assertArrayEquals(new int[] {(-2), (-1), (-2), (-2), 0, (-1)}, intArray0);
      assertEquals(6, intArray0.length);
      assertEquals(6, binarySparseInstance0.numValues());
      assertEquals((-2), binarySparseInstance0.numAttributes());
      assertEquals(426.3, binarySparseInstance0.weight(), 0.01);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertNotNull(binarySparseInstance0);
      
      boolean boolean0 = instances0.add((Instance) binarySparseInstance0);
      assertTrue(boolean0);
      assertArrayEquals(new int[] {(-2), (-1), (-2), (-2), 0, (-1)}, intArray0);
      assertEquals(6, intArray0.length);
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(0, testInstances0.getNumString());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(20, testInstances0.getNumInstances());
      assertFalse(testInstances0.getNoClass());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(2, instances0.numAttributes());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(2, instances0.numClasses());
      assertEquals(1, instances0.classIndex());
      assertEquals(446.3, instances0.sumOfWeights(), 0.01);
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(21, instances0.numInstances());
      assertEquals(21, instances0.size());
      assertEquals(6, binarySparseInstance0.numValues());
      assertEquals((-2), binarySparseInstance0.numAttributes());
      assertEquals(426.3, binarySparseInstance0.weight(), 0.01);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      
      IBk iBk0 = new IBk((-2));
      assertEquals((-2), iBk0.getKNN());
      assertEquals(0, iBk0.getWindowSize());
      assertEquals("Whether the mean squared error is used rather than mean absolute error when doing cross-validation for regression problems.", iBk0.meanSquaredTipText());
      assertEquals("The number of neighbours to use.", iBk0.KNNTipText());
      assertEquals("Gets the maximum number of instances allowed in the training pool. The addition of new instances above this value will result in old instances being removed. A value of 0 signifies no limit to the number of training instances.", iBk0.windowSizeTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", iBk0.debugTipText());
      assertFalse(iBk0.getDebug());
      assertEquals("The nearest neighbour search algorithm to use (Default: weka.core.neighboursearch.LinearNNSearch).", iBk0.nearestNeighbourSearchAlgorithmTipText());
      assertFalse(iBk0.getMeanSquared());
      assertEquals("Whether hold-one-out cross-validation will be used to select the best k value.", iBk0.crossValidateTipText());
      assertEquals("Gets the distance weighting method used.", iBk0.distanceWeightingTipText());
      assertFalse(iBk0.getCrossValidate());
      assertEquals(1, IBk.WEIGHT_NONE);
      assertEquals(4, IBk.WEIGHT_SIMILARITY);
      assertEquals(2, IBk.WEIGHT_INVERSE);
      assertNotNull(iBk0);
      
      double[] doubleArray0 = new double[8];
      doubleArray0[0] = (double) (-2);
      doubleArray0[1] = (double) 2;
      doubleArray0[3] = (double) (-2);
      doubleArray0[4] = (double) (-1);
      doubleArray0[5] = (double) 4;
      doubleArray0[6] = (double) (-2);
      doubleArray0[7] = 0.0;
      Instances instances1 = iBk0.pruneToK(instances0, doubleArray0, (-2));
      assertArrayEquals(new double[] {(-2.0), 2.0, 0.0, (-2.0), (-1.0), 4.0, (-2.0), 0.0}, doubleArray0, 0.01);
      assertEquals(8, doubleArray0.length);
      assertNotSame(instances0, instances1);
      assertNotSame(instances1, instances0);
      assertFalse(instances1.equals((Object)instances0));
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(0, testInstances0.getNumString());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(20, testInstances0.getNumInstances());
      assertFalse(testInstances0.getNoClass());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(2, instances0.numAttributes());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(2, instances0.numClasses());
      assertEquals(1, instances0.classIndex());
      assertEquals(446.3, instances0.sumOfWeights(), 0.01);
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(21, instances0.numInstances());
      assertEquals(21, instances0.size());
      assertEquals(1, instances1.size());
      assertEquals(1, instances1.classIndex());
      assertEquals("Testdata", instances1.relationName());
      assertEquals(2, instances1.numClasses());
      assertFalse(instances1.checkForStringAttributes());
      assertEquals(2, instances1.numAttributes());
      assertEquals(1, instances1.numInstances());
      assertEquals(1.0, instances1.sumOfWeights(), 0.01);
      assertEquals((-2), iBk0.getKNN());
      assertEquals(0, iBk0.getWindowSize());
      assertEquals("Whether the mean squared error is used rather than mean absolute error when doing cross-validation for regression problems.", iBk0.meanSquaredTipText());
      assertEquals("The number of neighbours to use.", iBk0.KNNTipText());
      assertEquals("Gets the maximum number of instances allowed in the training pool. The addition of new instances above this value will result in old instances being removed. A value of 0 signifies no limit to the number of training instances.", iBk0.windowSizeTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", iBk0.debugTipText());
      assertFalse(iBk0.getDebug());
      assertEquals("The nearest neighbour search algorithm to use (Default: weka.core.neighboursearch.LinearNNSearch).", iBk0.nearestNeighbourSearchAlgorithmTipText());
      assertFalse(iBk0.getMeanSquared());
      assertEquals("Whether hold-one-out cross-validation will be used to select the best k value.", iBk0.crossValidateTipText());
      assertEquals("Gets the distance weighting method used.", iBk0.distanceWeightingTipText());
      assertFalse(iBk0.getCrossValidate());
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals(1, IBk.WEIGHT_NONE);
      assertEquals(4, IBk.WEIGHT_SIMILARITY);
      assertEquals(2, IBk.WEIGHT_INVERSE);
      assertNotNull(instances1);
      
      naiveBayesMultinomialText0.buildClassifier(instances0);
      assertNotSame(instances0, instances1);
      assertFalse(instances0.equals((Object)instances1));
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertTrue(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(0, testInstances0.getNumString());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(20, testInstances0.getNumInstances());
      assertFalse(testInstances0.getNoClass());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(2, instances0.numAttributes());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(2, instances0.numClasses());
      assertEquals(1, instances0.classIndex());
      assertEquals(446.3, instances0.sumOfWeights(), 0.01);
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(21, instances0.numInstances());
      assertEquals(21, instances0.size());
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      
      String[] stringArray0 = naiveBayesMultinomialText0.getOptions();
      assertEquals(13, stringArray0.length);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertTrue(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertNotNull(stringArray0);
      
      naiveBayesMultinomialText0.setOptions(stringArray0);
      assertEquals(13, stringArray0.length);
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertTrue(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      
      try { 
        naiveBayesMultinomialText0.distributionForInstance(binarySparseInstance0);
        fail("Expecting exception: NullPointerException");
      
      } catch(NullPointerException e) {
         //
         // no message in exception (getMessage() returned null)
         //
         verifyException("weka.classifiers.bayes.NaiveBayesMultinomialText", e);
      }
  }

  /**
  //Test case number: 24
  /*Coverage entropy=3.2451263407580138
  */
  @Test(timeout = 4000)
  public void test24()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertNotNull(naiveBayesMultinomialText0);
      
      naiveBayesMultinomialText0.m_stemmer = null;
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      
      SparseInstance sparseInstance0 = new SparseInstance(1270);
      assertEquals(1270, sparseInstance0.numAttributes());
      assertEquals(1.0, sparseInstance0.weight(), 0.01);
      assertEquals(1270, sparseInstance0.numValues());
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertNotNull(sparseInstance0);
      
      IBk iBk0 = new IBk(1270);
      assertEquals("Gets the maximum number of instances allowed in the training pool. The addition of new instances above this value will result in old instances being removed. A value of 0 signifies no limit to the number of training instances.", iBk0.windowSizeTipText());
      assertEquals("Whether the mean squared error is used rather than mean absolute error when doing cross-validation for regression problems.", iBk0.meanSquaredTipText());
      assertEquals("The number of neighbours to use.", iBk0.KNNTipText());
      assertFalse(iBk0.getMeanSquared());
      assertEquals("Whether hold-one-out cross-validation will be used to select the best k value.", iBk0.crossValidateTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", iBk0.debugTipText());
      assertEquals("Gets the distance weighting method used.", iBk0.distanceWeightingTipText());
      assertFalse(iBk0.getCrossValidate());
      assertEquals(1270, iBk0.getKNN());
      assertEquals(0, iBk0.getWindowSize());
      assertFalse(iBk0.getDebug());
      assertEquals("The nearest neighbour search algorithm to use (Default: weka.core.neighboursearch.LinearNNSearch).", iBk0.nearestNeighbourSearchAlgorithmTipText());
      assertEquals(4, IBk.WEIGHT_SIMILARITY);
      assertEquals(1, IBk.WEIGHT_NONE);
      assertEquals(2, IBk.WEIGHT_INVERSE);
      assertNotNull(iBk0);
      
      TestInstances testInstances0 = new TestInstances();
      assertEquals(0, testInstances0.getNumString());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(20, testInstances0.getNumInstances());
      assertFalse(testInstances0.getNoClass());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(1, testInstances0.getSeed());
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertNotNull(testInstances0);
      
      naiveBayesMultinomialText0.setPeriodicPruning(20);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(20, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      
      String[] stringArray0 = naiveBayesMultinomialText0.getOptions();
      assertEquals(10, stringArray0.length);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(20, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertNotNull(stringArray0);
      
      naiveBayesMultinomialText0.setUseStopList(false);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(20, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      
      naiveBayesMultinomialText0.setOptions(stringArray0);
      assertEquals(10, stringArray0.length);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(20, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      
      naiveBayesMultinomialText0.setOptions(testInstances0.DEFAULT_WORDS);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(20, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(20, testInstances0.getNumInstances());
      assertFalse(testInstances0.getNoClass());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(1, testInstances0.getSeed());
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      
      naiveBayesMultinomialText0.pruneDictionary();
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals(20, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      
      System.setCurrentTimeMillis((-1));
  }

  /**
  //Test case number: 25
  /*Coverage entropy=1.6839972770367555
  */
  @Test(timeout = 4000)
  public void test25()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertNotNull(naiveBayesMultinomialText0);
      
      TestInstances testInstances0 = new TestInstances();
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getSeed());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(20, testInstances0.getNumInstances());
      assertFalse(testInstances0.getNoClass());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertNotNull(testInstances0);
      
      Instances instances0 = testInstances0.generate();
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getSeed());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(20, testInstances0.getNumInstances());
      assertFalse(testInstances0.getNoClass());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(1, instances0.classIndex());
      assertEquals(20, instances0.numInstances());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(20, instances0.size());
      assertEquals(2, instances0.numAttributes());
      assertEquals(2, instances0.numClasses());
      assertEquals("Testdata", instances0.relationName());
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertNotNull(instances0);
      
      int[] intArray0 = new int[6];
      intArray0[0] = (-2);
      intArray0[1] = (-1);
      naiveBayesMultinomialText0.m_normalize = true;
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertTrue(naiveBayesMultinomialText0.getNormalizeDocLength());
      
      intArray0[2] = (-2);
      intArray0[3] = (-2);
      intArray0[0] = (-2);
      intArray0[5] = (-1);
      BinarySparseInstance binarySparseInstance0 = new BinarySparseInstance(426.3, intArray0, (-2));
      assertArrayEquals(new int[] {(-2), (-1), (-2), (-2), 0, (-1)}, intArray0);
      assertEquals(6, intArray0.length);
      assertEquals(426.3, binarySparseInstance0.weight(), 0.01);
      assertEquals((-2), binarySparseInstance0.numAttributes());
      assertEquals(6, binarySparseInstance0.numValues());
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertNotNull(binarySparseInstance0);
      
      boolean boolean0 = instances0.add((Instance) binarySparseInstance0);
      assertTrue(boolean0);
      assertArrayEquals(new int[] {(-2), (-1), (-2), (-2), 0, (-1)}, intArray0);
      assertEquals(6, intArray0.length);
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getSeed());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(20, testInstances0.getNumInstances());
      assertFalse(testInstances0.getNoClass());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(21, instances0.numInstances());
      assertEquals(1, instances0.classIndex());
      assertEquals(446.3, instances0.sumOfWeights(), 0.01);
      assertEquals(21, instances0.size());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(2, instances0.numAttributes());
      assertEquals(2, instances0.numClasses());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(426.3, binarySparseInstance0.weight(), 0.01);
      assertEquals((-2), binarySparseInstance0.numAttributes());
      assertEquals(6, binarySparseInstance0.numValues());
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      
      IBk iBk0 = new IBk((-2));
      assertEquals("If set to true, classifier may output additional info to the console.", iBk0.debugTipText());
      assertEquals((-2), iBk0.getKNN());
      assertEquals(0, iBk0.getWindowSize());
      assertEquals("Whether hold-one-out cross-validation will be used to select the best k value.", iBk0.crossValidateTipText());
      assertEquals("Whether the mean squared error is used rather than mean absolute error when doing cross-validation for regression problems.", iBk0.meanSquaredTipText());
      assertEquals("Gets the distance weighting method used.", iBk0.distanceWeightingTipText());
      assertEquals("The number of neighbours to use.", iBk0.KNNTipText());
      assertFalse(iBk0.getCrossValidate());
      assertEquals("Gets the maximum number of instances allowed in the training pool. The addition of new instances above this value will result in old instances being removed. A value of 0 signifies no limit to the number of training instances.", iBk0.windowSizeTipText());
      assertEquals("The nearest neighbour search algorithm to use (Default: weka.core.neighboursearch.LinearNNSearch).", iBk0.nearestNeighbourSearchAlgorithmTipText());
      assertFalse(iBk0.getDebug());
      assertFalse(iBk0.getMeanSquared());
      assertEquals(4, IBk.WEIGHT_SIMILARITY);
      assertEquals(2, IBk.WEIGHT_INVERSE);
      assertEquals(1, IBk.WEIGHT_NONE);
      assertNotNull(iBk0);
      
      double[] doubleArray0 = new double[8];
      String string0 = instances0.toSummaryString();
      assertEquals("Relation Name:  Testdata\nNum Instances:  21\nNum Attributes: 2\n\n     Name                      Type  Nom  Int Real     Missing      Unique  Dist\n   1 Nominal1                   Nom 100%   0%   0%     0 /  0%     0 /  0%     2 \n   2 Class                      Nom 100%   0%   0%     0 /  0%     0 /  0%     2 \n", string0);
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getSeed());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(20, testInstances0.getNumInstances());
      assertFalse(testInstances0.getNoClass());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(21, instances0.numInstances());
      assertEquals(1, instances0.classIndex());
      assertEquals(446.3, instances0.sumOfWeights(), 0.01);
      assertEquals(21, instances0.size());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(2, instances0.numAttributes());
      assertEquals(2, instances0.numClasses());
      assertEquals("Testdata", instances0.relationName());
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertNotNull(string0);
      
      doubleArray0[0] = (double) (-2);
      doubleArray0[1] = (double) 2;
      doubleArray0[3] = (double) (-2);
      doubleArray0[4] = (double) (-1);
      doubleArray0[5] = (double) 4;
      doubleArray0[6] = (double) (-2);
      doubleArray0[7] = 0.0;
      Instances instances1 = iBk0.pruneToK(instances0, doubleArray0, (-2));
      assertArrayEquals(new double[] {(-2.0), 2.0, 0.0, (-2.0), (-1.0), 4.0, (-2.0), 0.0}, doubleArray0, 0.01);
      assertEquals(8, doubleArray0.length);
      assertNotSame(instances0, instances1);
      assertNotSame(instances1, instances0);
      assertFalse(instances1.equals((Object)instances0));
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getSeed());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(20, testInstances0.getNumInstances());
      assertFalse(testInstances0.getNoClass());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(21, instances0.numInstances());
      assertEquals(1, instances0.classIndex());
      assertEquals(446.3, instances0.sumOfWeights(), 0.01);
      assertEquals(21, instances0.size());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(2, instances0.numAttributes());
      assertEquals(2, instances0.numClasses());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(2, instances1.numClasses());
      assertEquals(1, instances1.classIndex());
      assertEquals(1.0, instances1.sumOfWeights(), 0.01);
      assertEquals("Testdata", instances1.relationName());
      assertEquals(1, instances1.size());
      assertEquals(2, instances1.numAttributes());
      assertFalse(instances1.checkForStringAttributes());
      assertEquals(1, instances1.numInstances());
      assertEquals("If set to true, classifier may output additional info to the console.", iBk0.debugTipText());
      assertEquals((-2), iBk0.getKNN());
      assertEquals(0, iBk0.getWindowSize());
      assertEquals("Whether hold-one-out cross-validation will be used to select the best k value.", iBk0.crossValidateTipText());
      assertEquals("Whether the mean squared error is used rather than mean absolute error when doing cross-validation for regression problems.", iBk0.meanSquaredTipText());
      assertEquals("Gets the distance weighting method used.", iBk0.distanceWeightingTipText());
      assertEquals("The number of neighbours to use.", iBk0.KNNTipText());
      assertFalse(iBk0.getCrossValidate());
      assertEquals("Gets the maximum number of instances allowed in the training pool. The addition of new instances above this value will result in old instances being removed. A value of 0 signifies no limit to the number of training instances.", iBk0.windowSizeTipText());
      assertEquals("The nearest neighbour search algorithm to use (Default: weka.core.neighboursearch.LinearNNSearch).", iBk0.nearestNeighbourSearchAlgorithmTipText());
      assertFalse(iBk0.getDebug());
      assertFalse(iBk0.getMeanSquared());
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals(4, IBk.WEIGHT_SIMILARITY);
      assertEquals(2, IBk.WEIGHT_INVERSE);
      assertEquals(1, IBk.WEIGHT_NONE);
      assertNotNull(instances1);
      
      naiveBayesMultinomialText0.buildClassifier(instances0);
      assertNotSame(instances0, instances1);
      assertFalse(instances0.equals((Object)instances1));
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertTrue(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getSeed());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(0, testInstances0.getNumString());
      assertEquals(0, testInstances0.getNumDate());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertEquals(20, testInstances0.getNumInstances());
      assertFalse(testInstances0.getNoClass());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(21, instances0.numInstances());
      assertEquals(1, instances0.classIndex());
      assertEquals(446.3, instances0.sumOfWeights(), 0.01);
      assertEquals(21, instances0.size());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(2, instances0.numAttributes());
      assertEquals(2, instances0.numClasses());
      assertEquals("Testdata", instances0.relationName());
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      
      NaiveBayesMultinomialText naiveBayesMultinomialText1 = new NaiveBayesMultinomialText();
      assertFalse(naiveBayesMultinomialText1.equals((Object)naiveBayesMultinomialText0));
      assertFalse(naiveBayesMultinomialText1.getUseWordFrequencies());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText1.globalInfo());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText1.normTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText1.debugTipText());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText1.stopwordsTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText1.stemmerTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText1.minWordFrequencyTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText1.lowercaseTokensTipText());
      assertFalse(naiveBayesMultinomialText1.getDebug());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText1.tokenizerTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText1.periodicPruningTipText());
      assertFalse(naiveBayesMultinomialText1.getNormalizeDocLength());
      assertEquals(2.0, naiveBayesMultinomialText1.getLNorm(), 0.01);
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText1.normalizeDocLengthTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText1.useStopListTipText());
      assertEquals(3.0, naiveBayesMultinomialText1.getMinWordFrequency(), 0.01);
      assertEquals(1.0, naiveBayesMultinomialText1.getNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText1.getLowercaseTokens());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText1.LNormTipText());
      assertFalse(naiveBayesMultinomialText1.getUseStopList());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText1.useWordFrequenciesTipText());
      assertEquals(0, naiveBayesMultinomialText1.getPeriodicPruning());
      assertNotNull(naiveBayesMultinomialText1);
      
      String string1 = naiveBayesMultinomialText0.toString();
      assertEquals("The independent probability of a class\n--------------------------------------\nclass1\t438.3\nclass2\t10.0\n\nThe probability of a word given the class\n-----------------------------------------\n\tclass1\tclass2\t\n", string1);
      assertNotSame(naiveBayesMultinomialText0, naiveBayesMultinomialText1);
      assertFalse(naiveBayesMultinomialText0.equals((Object)naiveBayesMultinomialText1));
      assertFalse(string1.equals((Object)string0));
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertTrue(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertNotNull(string1);
  }

  /**
  //Test case number: 26
  /*Coverage entropy=2.7709738809556868
  */
  @Test(timeout = 4000)
  public void test26()  throws Throwable  {
      NaiveBayesMultinomialText naiveBayesMultinomialText0 = new NaiveBayesMultinomialText();
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertNotNull(naiveBayesMultinomialText0);
      
      TestInstances testInstances0 = new TestInstances();
      assertEquals(0, testInstances0.getNumString());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(0, testInstances0.getNumDate());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertFalse(testInstances0.getNoClass());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertNotNull(testInstances0);
      
      Instances instances0 = testInstances0.generate();
      assertEquals(0, testInstances0.getNumString());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(0, testInstances0.getNumDate());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertFalse(testInstances0.getNoClass());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(20, instances0.size());
      assertEquals(1, instances0.classIndex());
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(20, instances0.numInstances());
      assertEquals(20.0, instances0.sumOfWeights(), 0.01);
      assertEquals(2, instances0.numClasses());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(2, instances0.numAttributes());
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertNotNull(instances0);
      
      int[] intArray0 = new int[6];
      intArray0[0] = (-2);
      intArray0[1] = (-1);
      intArray0[3] = (-2);
      intArray0[0] = (-2);
      intArray0[5] = (-1);
      BinarySparseInstance binarySparseInstance0 = new BinarySparseInstance(426.3, intArray0, 0);
      assertArrayEquals(new int[] {(-2), (-1), 0, (-2), 0, (-1)}, intArray0);
      assertEquals(6, intArray0.length);
      assertEquals(426.3, binarySparseInstance0.weight(), 0.01);
      assertEquals(0, binarySparseInstance0.numAttributes());
      assertEquals(6, binarySparseInstance0.numValues());
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertNotNull(binarySparseInstance0);
      
      boolean boolean0 = instances0.add((Instance) binarySparseInstance0);
      assertTrue(boolean0);
      assertArrayEquals(new int[] {(-2), (-1), 0, (-2), 0, (-1)}, intArray0);
      assertEquals(6, intArray0.length);
      assertEquals(0, testInstances0.getNumString());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(0, testInstances0.getNumDate());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertFalse(testInstances0.getNoClass());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, instances0.classIndex());
      assertEquals(446.3, instances0.sumOfWeights(), 0.01);
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(21, instances0.size());
      assertEquals(21, instances0.numInstances());
      assertEquals(2, instances0.numClasses());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(2, instances0.numAttributes());
      assertEquals(426.3, binarySparseInstance0.weight(), 0.01);
      assertEquals(0, binarySparseInstance0.numAttributes());
      assertEquals(6, binarySparseInstance0.numValues());
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      
      IBk iBk0 = new IBk((-2));
      assertFalse(iBk0.getCrossValidate());
      assertEquals(0, iBk0.getWindowSize());
      assertFalse(iBk0.getDebug());
      assertEquals("The nearest neighbour search algorithm to use (Default: weka.core.neighboursearch.LinearNNSearch).", iBk0.nearestNeighbourSearchAlgorithmTipText());
      assertEquals("Gets the maximum number of instances allowed in the training pool. The addition of new instances above this value will result in old instances being removed. A value of 0 signifies no limit to the number of training instances.", iBk0.windowSizeTipText());
      assertEquals("Whether the mean squared error is used rather than mean absolute error when doing cross-validation for regression problems.", iBk0.meanSquaredTipText());
      assertEquals("The number of neighbours to use.", iBk0.KNNTipText());
      assertFalse(iBk0.getMeanSquared());
      assertEquals("Whether hold-one-out cross-validation will be used to select the best k value.", iBk0.crossValidateTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", iBk0.debugTipText());
      assertEquals((-2), iBk0.getKNN());
      assertEquals("Gets the distance weighting method used.", iBk0.distanceWeightingTipText());
      assertEquals(2, IBk.WEIGHT_INVERSE);
      assertEquals(4, IBk.WEIGHT_SIMILARITY);
      assertEquals(1, IBk.WEIGHT_NONE);
      assertNotNull(iBk0);
      
      double[] doubleArray0 = new double[8];
      iBk0.setKNN((-2));
      assertFalse(iBk0.getCrossValidate());
      assertEquals(0, iBk0.getWindowSize());
      assertFalse(iBk0.getDebug());
      assertEquals("The nearest neighbour search algorithm to use (Default: weka.core.neighboursearch.LinearNNSearch).", iBk0.nearestNeighbourSearchAlgorithmTipText());
      assertEquals("Gets the maximum number of instances allowed in the training pool. The addition of new instances above this value will result in old instances being removed. A value of 0 signifies no limit to the number of training instances.", iBk0.windowSizeTipText());
      assertEquals("Whether the mean squared error is used rather than mean absolute error when doing cross-validation for regression problems.", iBk0.meanSquaredTipText());
      assertEquals("The number of neighbours to use.", iBk0.KNNTipText());
      assertFalse(iBk0.getMeanSquared());
      assertEquals("Whether hold-one-out cross-validation will be used to select the best k value.", iBk0.crossValidateTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", iBk0.debugTipText());
      assertEquals((-2), iBk0.getKNN());
      assertEquals("Gets the distance weighting method used.", iBk0.distanceWeightingTipText());
      assertEquals(2, IBk.WEIGHT_INVERSE);
      assertEquals(4, IBk.WEIGHT_SIMILARITY);
      assertEquals(1, IBk.WEIGHT_NONE);
      
      doubleArray0[0] = (double) (-2);
      boolean boolean1 = instances0.checkInstance(binarySparseInstance0);
      assertFalse(boolean1);
      assertArrayEquals(new int[] {(-2), (-1), 0, (-2), 0, (-1)}, intArray0);
      assertEquals(6, intArray0.length);
      assertFalse(boolean1 == boolean0);
      assertEquals(0, testInstances0.getNumString());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(0, testInstances0.getNumDate());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertFalse(testInstances0.getNoClass());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, instances0.classIndex());
      assertEquals(446.3, instances0.sumOfWeights(), 0.01);
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(21, instances0.size());
      assertEquals(21, instances0.numInstances());
      assertEquals(2, instances0.numClasses());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(2, instances0.numAttributes());
      assertEquals(426.3, binarySparseInstance0.weight(), 0.01);
      assertEquals(0, binarySparseInstance0.numAttributes());
      assertEquals(6, binarySparseInstance0.numValues());
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      
      doubleArray0[1] = (double) 2;
      doubleArray0[2] = (double) (-1);
      doubleArray0[3] = (double) (-2);
      doubleArray0[4] = (double) (-1);
      doubleArray0[5] = (double) 4;
      doubleArray0[6] = (double) (-2);
      doubleArray0[7] = 0.0;
      Instances instances1 = iBk0.pruneToK(instances0, doubleArray0, (-2));
      assertArrayEquals(new double[] {(-2.0), 2.0, (-1.0), (-2.0), (-1.0), 4.0, (-2.0), 0.0}, doubleArray0, 0.01);
      assertEquals(8, doubleArray0.length);
      assertNotSame(instances0, instances1);
      assertNotSame(instances1, instances0);
      assertFalse(instances1.equals((Object)instances0));
      assertEquals(0, testInstances0.getNumString());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(0, testInstances0.getNumDate());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertFalse(testInstances0.getNoClass());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, instances0.classIndex());
      assertEquals(446.3, instances0.sumOfWeights(), 0.01);
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(21, instances0.size());
      assertEquals(21, instances0.numInstances());
      assertEquals(2, instances0.numClasses());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(2, instances0.numAttributes());
      assertEquals(1, instances1.numInstances());
      assertEquals(2, instances1.numClasses());
      assertFalse(instances1.checkForStringAttributes());
      assertEquals(1, instances1.classIndex());
      assertEquals(1.0, instances1.sumOfWeights(), 0.01);
      assertEquals("Testdata", instances1.relationName());
      assertEquals(1, instances1.size());
      assertEquals(2, instances1.numAttributes());
      assertFalse(iBk0.getCrossValidate());
      assertEquals(0, iBk0.getWindowSize());
      assertFalse(iBk0.getDebug());
      assertEquals("The nearest neighbour search algorithm to use (Default: weka.core.neighboursearch.LinearNNSearch).", iBk0.nearestNeighbourSearchAlgorithmTipText());
      assertEquals("Gets the maximum number of instances allowed in the training pool. The addition of new instances above this value will result in old instances being removed. A value of 0 signifies no limit to the number of training instances.", iBk0.windowSizeTipText());
      assertEquals("Whether the mean squared error is used rather than mean absolute error when doing cross-validation for regression problems.", iBk0.meanSquaredTipText());
      assertEquals("The number of neighbours to use.", iBk0.KNNTipText());
      assertFalse(iBk0.getMeanSquared());
      assertEquals("Whether hold-one-out cross-validation will be used to select the best k value.", iBk0.crossValidateTipText());
      assertEquals("If set to true, classifier may output additional info to the console.", iBk0.debugTipText());
      assertEquals((-2), iBk0.getKNN());
      assertEquals("Gets the distance weighting method used.", iBk0.distanceWeightingTipText());
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      assertEquals(2, IBk.WEIGHT_INVERSE);
      assertEquals(4, IBk.WEIGHT_SIMILARITY);
      assertEquals(1, IBk.WEIGHT_NONE);
      assertNotNull(instances1);
      
      naiveBayesMultinomialText0.buildClassifier(instances0);
      assertNotSame(instances0, instances1);
      assertFalse(instances0.equals((Object)instances1));
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals(0, testInstances0.getNumString());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(0, testInstances0.getNumDate());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertFalse(testInstances0.getNoClass());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals(1, instances0.classIndex());
      assertEquals(446.3, instances0.sumOfWeights(), 0.01);
      assertFalse(instances0.checkForStringAttributes());
      assertEquals(21, instances0.size());
      assertEquals(21, instances0.numInstances());
      assertEquals(2, instances0.numClasses());
      assertEquals("Testdata", instances0.relationName());
      assertEquals(2, instances0.numAttributes());
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
      
      double[] doubleArray1 = naiveBayesMultinomialText0.distributionForInstance(binarySparseInstance0);
      assertArrayEquals(new int[] {(-2), (-1), 0, (-2), 0, (-1)}, intArray0);
      assertArrayEquals(new double[] {0.977693508811064, 0.02230649118893598}, doubleArray1, 0.01);
      assertEquals(6, intArray0.length);
      assertEquals(2, doubleArray1.length);
      assertNotSame(doubleArray1, doubleArray0);
      assertFalse(doubleArray1.equals((Object)doubleArray0));
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals(426.3, binarySparseInstance0.weight(), 0.01);
      assertEquals(0, binarySparseInstance0.numAttributes());
      assertEquals(6, binarySparseInstance0.numValues());
      assertEquals(6, AbstractInstance.s_numericAfterDecimalPoint);
      assertNotNull(doubleArray1);
      
      System.setCurrentTimeMillis(120L);
      naiveBayesMultinomialText0.setOptions(testInstances0.DEFAULT_WORDS);
      assertEquals("Multinomial naive bayes for text data. Operates directly (and only) on String attributes. Other types of input attributes are accepted but ignored during training and classification", naiveBayesMultinomialText0.globalInfo());
      assertFalse(naiveBayesMultinomialText0.getUseWordFrequencies());
      assertEquals("The file containing the stopwords (if this is a directory then the default ones are used).", naiveBayesMultinomialText0.stopwordsTipText());
      assertEquals("The norm of the instances after normalization.", naiveBayesMultinomialText0.normTipText());
      assertEquals(1.0, naiveBayesMultinomialText0.getNorm(), 0.01);
      assertEquals("If set to true, classifier may output additional info to the console.", naiveBayesMultinomialText0.debugTipText());
      assertEquals(0, naiveBayesMultinomialText0.getPeriodicPruning());
      assertEquals(2.0, naiveBayesMultinomialText0.getLNorm(), 0.01);
      assertFalse(naiveBayesMultinomialText0.getUseStopList());
      assertEquals("The tokenizing algorithm to use on the strings.", naiveBayesMultinomialText0.tokenizerTipText());
      assertFalse(naiveBayesMultinomialText0.getNormalizeDocLength());
      assertEquals("The LNorm to use for document length normalization.", naiveBayesMultinomialText0.LNormTipText());
      assertEquals("Whether to convert all tokens to lowercase", naiveBayesMultinomialText0.lowercaseTokensTipText());
      assertEquals("Use word frequencies rather than binary bag of words representation", naiveBayesMultinomialText0.useWordFrequenciesTipText());
      assertEquals("If true then document length is normalized according to the settings for norm and lnorm", naiveBayesMultinomialText0.normalizeDocLengthTipText());
      assertEquals("Ignore any words that don't occur at least min frequency times in the training data. If periodic pruning is turned on, then the dictionary is pruned according to this value", naiveBayesMultinomialText0.minWordFrequencyTipText());
      assertFalse(naiveBayesMultinomialText0.getDebug());
      assertEquals("If true, ignores all words that are on the stoplist.", naiveBayesMultinomialText0.useStopListTipText());
      assertEquals("The stemming algorithm to use on the words.", naiveBayesMultinomialText0.stemmerTipText());
      assertFalse(naiveBayesMultinomialText0.getLowercaseTokens());
      assertEquals("How often (number of instances) to prune the dictionary of low frequency terms. 0 means don't prune. Setting a positive integer n means prune after every n instances", naiveBayesMultinomialText0.periodicPruningTipText());
      assertEquals(3.0, naiveBayesMultinomialText0.getMinWordFrequency(), 0.01);
      assertEquals(0, testInstances0.getNumString());
      assertEquals(0, testInstances0.getNumNumeric());
      assertEquals((-1), testInstances0.getClassIndex());
      assertEquals(1, testInstances0.getClassType());
      assertEquals(1, testInstances0.getNumNominal());
      assertEquals(0, testInstances0.getNumRelationalDate());
      assertEquals(20, testInstances0.getNumInstances());
      assertEquals(" ", testInstances0.getWordSeparators());
      assertEquals("Testdata", testInstances0.getRelation());
      assertEquals(0, testInstances0.getNumRelational());
      assertEquals(2, testInstances0.getNumClasses());
      assertEquals(2, testInstances0.getNumAttributes());
      assertEquals(0, testInstances0.getNumDate());
      assertFalse(testInstances0.getMultiInstance());
      assertEquals(2, testInstances0.getNumNominalValues());
      assertFalse(testInstances0.getNoClass());
      assertEquals(2, testInstances0.getNumRelationalNominalValues());
      assertEquals(10, testInstances0.getNumInstancesRelational());
      assertEquals(1, testInstances0.getSeed());
      assertEquals(1, testInstances0.getNumRelationalNominal());
      assertEquals(0, testInstances0.getNumRelationalNumeric());
      assertEquals(0, testInstances0.getNumRelationalString());
      assertEquals((-1), TestInstances.CLASS_IS_LAST);
      assertEquals((-2), TestInstances.NO_CLASS);
  }
}
